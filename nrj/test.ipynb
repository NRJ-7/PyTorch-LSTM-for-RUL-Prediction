{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e60c12ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from load_data import FD001_dataset\n",
    "from visualize_modified import visualize\n",
    "from RUL_model import RUL_Model, RUL_Model_LearnableStates\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Optional, Tuple, List\n",
    "from tqdm.auto import tqdm\n",
    "from train_model import train_model, evaluate_data\n",
    "\n",
    "# Check if MPS is available and built\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    # Fallback for systems with NVIDIA GPUs (though less relevant on a Mac)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS or CUDA not available, using CPU\")\n",
    "\n",
    "DATA_DIR_PATH = '../CMAPSSData/'\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "# pio.templates.default = \"none\"\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "RUL_UPPER_BOUND = 130  # UPPER BOUND OF RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b45916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Define a padding index for labels ***\n",
    "# This value MUST be ignored by your loss function.\n",
    "# For nn.CrossEntropyLoss, use its 'ignore_index' parameter (default -100 is common).\n",
    "# Choose a value that doesn't conflict with valid label indices/values.\n",
    "LABEL_PADDING_IDX = -1000000\n",
    "\n",
    "def pad_collate_fn_time_step_level(batch):\n",
    "    sequences = [item[0] for item in batch]\n",
    "    # Labels are also sequences now\n",
    "    labels = [item[1] for item in batch] # e.g., list of tensors of shape [Li] or [Li, C]\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long) # Lengths based on input sequences\n",
    "\n",
    "    # Pad features (input sequences)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # *** Pad labels (label sequences) ***\n",
    "    # Use the dedicated label padding index!\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=LABEL_PADDING_IDX)\n",
    "\n",
    "    return padded_sequences, lengths, padded_labels\n",
    "\n",
    "# Usage:\n",
    "# train_loader = DataLoader(..., collate_fn=pad_collate_fn_time_step_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec659cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rul_vs_time(unit_nr, model, dataloader):\n",
    "    \"\"\"\n",
    "    Plot the RUL over time for a given unit number.\n",
    "    \"\"\"\n",
    "    X, true_rul = dataloader.dataset[unit_nr - 1]\n",
    "    \n",
    "    model.eval()\n",
    "    predicted_rul = model(X).detach().numpy().squeeze(0)\n",
    "\n",
    "    fig = px.line(\n",
    "        x=np.arange(1, len(true_rul) + 1),\n",
    "        y=true_rul.reshape(-1),\n",
    "        title=f'RUL Over Time: Unit {unit_nr}',\n",
    "        labels={'x': 'Time', 'y': 'RUL'},\n",
    "        markers=False\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        x=np.arange(1, len(predicted_rul) + 1),\n",
    "        y=predicted_rul.reshape(-1),\n",
    "        mode='lines',\n",
    "        name='Predicted RUL'\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        legend=dict(yanchor=\"bottom\",\n",
    "                    y=0.1,\n",
    "                    xanchor=\"left\",\n",
    "                    x=0.1)\n",
    "    )\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910f1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_hparam_string(hparams):\n",
    "  \"\"\"\n",
    "  Generates a concise file string representation from a hyperparameter dictionary.\n",
    "\n",
    "  Args:\n",
    "    hparams (dict): A dictionary containing hyperparameters.\n",
    "\n",
    "  Returns:\n",
    "    str: A formatted string summarizing the hyperparameters, suitable for filenames.\n",
    "         Returns an empty string if the input is not a dictionary or is empty.\n",
    "  \"\"\"\n",
    "  if not isinstance(hparams, dict) or not hparams:\n",
    "    print(\"Warning: Input must be a non-empty dictionary.\")\n",
    "    return \"\"\n",
    "\n",
    "  parts = []\n",
    "\n",
    "  # Define abbreviations for keys to keep the string short\n",
    "  abbreviations = {\n",
    "      'LearningRate': 'LR',\n",
    "      'BatchSize': 'BS',\n",
    "      'HiddenSizes': 'HS',\n",
    "      'LayerSizes': 'LS',\n",
    "      'DropoutRate': 'DR',\n",
    "      'LossFn': 'LF',\n",
    "      'Optimizer': 'Opt',\n",
    "      'LearnableStates': 'LrnS'\n",
    "      # Add more abbreviations as needed\n",
    "  }\n",
    "\n",
    "  # Define the order in which parameters should appear in the string\n",
    "  param_order = [\n",
    "      'LearningRate', 'BatchSize', 'HiddenSizes', 'LayerSizes', 'DropoutRate',\n",
    "      'LossFn', 'Optimizer', 'LearnableStates'\n",
    "  ]\n",
    "\n",
    "  for key in param_order:\n",
    "      if key in hparams:\n",
    "          value = hparams[key]\n",
    "          abbr = abbreviations.get(key, key) # Use abbreviation if available, else full key\n",
    "\n",
    "          # Format value based on its type\n",
    "          if isinstance(value, list):\n",
    "              # Join list elements with a hyphen\n",
    "              value_str = '-'.join(map(str, value))\n",
    "          elif isinstance(value, bool):\n",
    "              # Use T/F for boolean values\n",
    "              value_str = 'T' if value else 'F'\n",
    "          elif isinstance(value, float):\n",
    "              # Format float to avoid excessive decimal places and replace '.' with 'p'\n",
    "              value_str = f\"{value:.1e}\".replace('.', 'p').replace('+','') # e.g., 0.001 -> 1e-03 -> 1e-03\n",
    "          else:\n",
    "              # Use string representation for other types\n",
    "              value_str = str(value)\n",
    "\n",
    "          # Sanitize value string for filename compatibility (remove potentially problematic characters)\n",
    "          # Allows alphanumeric, hyphen, underscore, period\n",
    "          # value_str = re.sub(r'[^\\w\\-._]+', '', value_str)\n",
    "\n",
    "          parts.append(f\"{abbr}_{value_str}\")\n",
    "\n",
    "  # Add any parameters from hparams that were not in param_order\n",
    "  for key, value in hparams.items():\n",
    "      if key not in param_order:\n",
    "          abbr = abbreviations.get(key, key)\n",
    "          if isinstance(value, bool):\n",
    "              value_str = 'T' if value else 'F'\n",
    "          elif isinstance(value, float):\n",
    "              value_str = f\"{value:.1e}\".replace('.', 'p').replace('+','')\n",
    "          else:\n",
    "              value_str = str(value)\n",
    "          # value_str = re.sub(r'[^\\w\\-._]+', '', value_str) # Sanitize\n",
    "          parts.append(f\"{abbr}_{value_str}\")\n",
    "\n",
    "\n",
    "  return '__'.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7a65491",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FD001_dataset(data_type='train', rul_ub=RUL_UPPER_BOUND, norm_type='zscore')\n",
    "test_dataset = FD001_dataset(data_type='test', rul_ub=RUL_UPPER_BOUND, norm_type='zscore')\n",
    "\n",
    "train_dataLoader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate_fn_time_step_level)\n",
    "test_dataLoader = DataLoader(test_dataset, batch_size=100, shuffle=False, collate_fn=pad_collate_fn_time_step_level) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d18634",
   "metadata": {},
   "source": [
    "<h2>Model Initialization and Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb12404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams = {\n",
    "#     'LR': 0.001,\n",
    "#     'BatchSize': 20,\n",
    "#     'HiddenSizes': [32, 64],\n",
    "#     'LayerSizes': [1, 1],\n",
    "#     'DropoutRate': 0.3,\n",
    "#     'LossFn': 'MSELoss',\n",
    "#     'Optimizer': 'RMSprop',\n",
    "#     'LearnableStates': True,\n",
    "#     'device': device\n",
    "# }\n",
    "# hparams_str = create_hparam_string(hparams)\n",
    "# model = RUL_Model_LearnableStates(input_size=train_dataLoader.dataset[0][0].shape[1], \n",
    "#                                   lstm_hidden_sizes=hparams['HiddenSizes'], \n",
    "#                                   lstm_layer_sizes=hparams['LayerSizes'], \n",
    "#                                   lstm_dropout_rate=hparams['DropoutRate'], \n",
    "#                                   output_dropout_rate=hparams['DropoutRate'], state_init=True)\n",
    "# model.to(device)\n",
    "# loss_fn = torch.nn.MSELoss(reduction='none')  # mean-squared error for regression\n",
    "# # optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=LR)\n",
    "\n",
    "# # Just to use the original visualisation module\n",
    "# y_test = pd.DataFrame({\"RUL\": [test_dataset[i][1][-1].item() for i in range(100)]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bca55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# writer = SummaryWriter(log_dir=f'runs/{hparams_str}')\n",
    "# writer.add_graph(model, train_dataset[0][0].to(device))\n",
    "\n",
    "# final_train_loss, best_val_loss, last_epoch = train_model(\n",
    "#     model=model,\n",
    "#     train_dataLoader=train_dataLoader, # Your training DataLoader\n",
    "#     test_dataLoader=test_dataLoader,   # Your validation DataLoader\n",
    "#     loss_fn=loss_fn,\n",
    "#     optimizer=optimizer,\n",
    "#     device=device,\n",
    "#     writer=writer, # Pass your writer if using TensorBoard\n",
    "#     num_epochs=1,\n",
    "#     patience=10,\n",
    "#     best_model_path=f'./models/best_model_{hparams_str}.pth', # Choose save path\n",
    "#     y_test=y_test, # Optional: for visualization\n",
    "# )\n",
    "\n",
    "# # writer.add_hparams(hparam_dict=hparams, metric_dict={'Train Loss': train_loss,\n",
    "# #                                                      'Validation Loss': val_loss,\n",
    "# #                                                      'Num Epochs Run': epochs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f670d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rul_seqlen_err = list() # np.zeros((100, 3))\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i, (x, y) in enumerate(test_dataLoader):\n",
    "#         y_pred = model(x)[:, -1, :].item()\n",
    "#         true_rul = y[:, -1, :].item()\n",
    "#         rul_err = true_rul - y_pred\n",
    "#         rul_seqlen_err.append([true_rul, x.shape[1], rul_err])\n",
    "\n",
    "# rul_seqlen_err = np.array(rul_seqlen_err).T\n",
    "# rul_seqlen_err[2] = np.abs(rul_seqlen_err[2])\n",
    "\n",
    "# px.scatter(\n",
    "#     x=rul_seqlen_err[0],\n",
    "#     y=rul_seqlen_err[1],\n",
    "#     color=rul_seqlen_err[2],\n",
    "#     title='True RUL vs Sequence Length',\n",
    "#     labels={'x': 'True RUL', 'y': 'Sequence Length', 'color': 'RUL Error'},\n",
    "#     color_continuous_scale=px.colors.sequential.Viridis\n",
    "# ).show()\n",
    "# px.scatter(\n",
    "#     x=rul_seqlen_err[0],\n",
    "#     y=rul_seqlen_err[2],\n",
    "#     color=rul_seqlen_err[1],\n",
    "#     title='True RUL vs RUL Error',\n",
    "#     labels={'x': 'True RUL', 'y': 'RUL Error', 'color': 'Sequence Length'},\n",
    "#     color_continuous_scale=px.colors.sequential.Viridis\n",
    "# ).show()\n",
    "# px.scatter(\n",
    "#     x=rul_seqlen_err[1],\n",
    "#     y=rul_seqlen_err[2],\n",
    "#     color=rul_seqlen_err[0],\n",
    "#     title='Sequence Length vs RUL Error',\n",
    "#     labels={'x': 'Sequence Length', 'y': 'RUL Error', 'color': 'True RUL'},\n",
    "#     color_continuous_scale=px.colors.sequential.Viridis\n",
    "# ).show()\n",
    "\n",
    "# # px.line(x=np.arange(1, len(true_rul) + 1), y=((true_rul - predicted_rul)).reshape(-1), title='RUL Error Over Time', labels={'x': 'Time', 'y': 'RUL Error'}, markers=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb4d30",
   "metadata": {},
   "source": [
    "<h2>Cross Validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1177930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, random_split, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bf59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams = {\n",
    "    'LearningRate': 0.002,\n",
    "    'BatchSize': 32,\n",
    "    'HiddenSizes': [32, 64],\n",
    "    'LayerSizes': [1, 1],\n",
    "    'DropoutRate': 0.3,\n",
    "    'LossFn': 'MSELoss',\n",
    "    'Optimizer': 'RMSprop',\n",
    "    'LearnableStates': True,\n",
    "    'device': device,\n",
    "    'CrossVal': 10\n",
    "}\n",
    "hparams_str = create_hparam_string(hparams)\n",
    "\n",
    "# # Just to use the original visualisation module\n",
    "# y_test = pd.DataFrame({\"RUL\": [test_dataset[i][1][-1].item() for i in range(100)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2cadd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9865.1556 | Val Loss: 96.7258\n",
      "Validation RMSE improved (inf --> 96.7258). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9786.1129 | Val Loss: 95.8922\n",
      "Validation RMSE improved (96.7258 --> 95.8922). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 9579.0675 | Val Loss: 94.5183\n",
      "Validation RMSE improved (95.8922 --> 94.5183). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 9299.3126 | Val Loss: 92.9350\n",
      "Validation RMSE improved (94.5183 --> 92.9350). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 8987.4851 | Val Loss: 91.1485\n",
      "Validation RMSE improved (92.9350 --> 91.1485). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 8635.2422 | Val Loss: 89.2339\n",
      "Validation RMSE improved (91.1485 --> 89.2339). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 8290.7616 | Val Loss: 87.2137\n",
      "Validation RMSE improved (89.2339 --> 87.2137). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 7912.9638 | Val Loss: 85.0707\n",
      "Validation RMSE improved (87.2137 --> 85.0707). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 7539.3965 | Val Loss: 82.7998\n",
      "Validation RMSE improved (85.0707 --> 82.7998). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 7156.6983 | Val Loss: 80.4830\n",
      "Validation RMSE improved (82.7998 --> 80.4830). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 6779.1773 | Val Loss: 78.1185\n",
      "Validation RMSE improved (80.4830 --> 78.1185). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 6416.9383 | Val Loss: 75.6888\n",
      "Validation RMSE improved (78.1185 --> 75.6888). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 6050.1835 | Val Loss: 73.2864\n",
      "Validation RMSE improved (75.6888 --> 73.2864). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 5708.3742 | Val Loss: 70.9148\n",
      "Validation RMSE improved (73.2864 --> 70.9148). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 5374.4791 | Val Loss: 68.5761\n",
      "Validation RMSE improved (70.9148 --> 68.5761). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 5056.5858 | Val Loss: 66.2715\n",
      "Validation RMSE improved (68.5761 --> 66.2715). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 4765.4024 | Val Loss: 63.9065\n",
      "Validation RMSE improved (66.2715 --> 63.9065). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 4460.0574 | Val Loss: 61.2391\n",
      "Validation RMSE improved (63.9065 --> 61.2391). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 4142.6407 | Val Loss: 58.5864\n",
      "Validation RMSE improved (61.2391 --> 58.5864). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 3824.1780 | Val Loss: 56.1948\n",
      "Validation RMSE improved (58.5864 --> 56.1948). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 3593.9792 | Val Loss: 53.8387\n",
      "Validation RMSE improved (56.1948 --> 53.8387). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 3372.1552 | Val Loss: 51.5430\n",
      "Validation RMSE improved (53.8387 --> 51.5430). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 3151.1052 | Val Loss: 49.3249\n",
      "Validation RMSE improved (51.5430 --> 49.3249). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 2973.8158 | Val Loss: 47.1848\n",
      "Validation RMSE improved (49.3249 --> 47.1848). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 2800.3395 | Val Loss: 45.0950\n",
      "Validation RMSE improved (47.1848 --> 45.0950). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 2627.0746 | Val Loss: 43.0687\n",
      "Validation RMSE improved (45.0950 --> 43.0687). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 2500.6500 | Val Loss: 41.1905\n",
      "Validation RMSE improved (43.0687 --> 41.1905). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 2331.2461 | Val Loss: 39.7719\n",
      "Validation RMSE improved (41.1905 --> 39.7719). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 2237.7868 | Val Loss: 37.5741\n",
      "Validation RMSE improved (39.7719 --> 37.5741). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 2099.0829 | Val Loss: 36.0097\n",
      "Validation RMSE improved (37.5741 --> 36.0097). Saving model...\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 2031.7157 | Val Loss: 34.5354\n",
      "Validation RMSE improved (36.0097 --> 34.5354). Saving model...\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 1961.3265 | Val Loss: 33.1861\n",
      "Validation RMSE improved (34.5354 --> 33.1861). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 1869.1140 | Val Loss: 31.9908\n",
      "Validation RMSE improved (33.1861 --> 31.9908). Saving model...\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 1864.8396 | Val Loss: 30.4966\n",
      "Validation RMSE improved (31.9908 --> 30.4966). Saving model...\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 1762.7383 | Val Loss: 29.3029\n",
      "Validation RMSE improved (30.4966 --> 29.3029). Saving model...\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 1737.6002 | Val Loss: 28.8992\n",
      "Validation RMSE improved (29.3029 --> 28.8992). Saving model...\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 1694.6203 | Val Loss: 27.3919\n",
      "Validation RMSE improved (28.8992 --> 27.3919). Saving model...\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1654.5796 | Val Loss: 26.5574\n",
      "Validation RMSE improved (27.3919 --> 26.5574). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1616.7187 | Val Loss: 25.5398\n",
      "Validation RMSE improved (26.5574 --> 25.5398). Saving model...\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1621.5473 | Val Loss: 25.2522\n",
      "Validation RMSE improved (25.5398 --> 25.2522). Saving model...\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1575.5318 | Val Loss: 24.0767\n",
      "Validation RMSE improved (25.2522 --> 24.0767). Saving model...\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1526.1629 | Val Loss: 23.3110\n",
      "Validation RMSE improved (24.0767 --> 23.3110). Saving model...\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1512.8120 | Val Loss: 23.5088\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 23.3110\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1542.5134 | Val Loss: 22.5330\n",
      "Validation RMSE improved (23.3110 --> 22.5330). Saving model...\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1512.6786 | Val Loss: 21.9151\n",
      "Validation RMSE improved (22.5330 --> 21.9151). Saving model...\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1497.6417 | Val Loss: 21.3394\n",
      "Validation RMSE improved (21.9151 --> 21.3394). Saving model...\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1489.6834 | Val Loss: 20.5991\n",
      "Validation RMSE improved (21.3394 --> 20.5991). Saving model...\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1501.6535 | Val Loss: 22.4761\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.5991\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1496.3011 | Val Loss: 20.0264\n",
      "Validation RMSE improved (20.5991 --> 20.0264). Saving model...\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1479.5132 | Val Loss: 21.0893\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.0264\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1480.9778 | Val Loss: 20.4962\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.0264\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1421.6640 | Val Loss: 19.3940\n",
      "Validation RMSE improved (20.0264 --> 19.3940). Saving model...\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1455.9382 | Val Loss: 20.3290\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.3940\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1432.2127 | Val Loss: 20.1416\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.3940\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1462.3142 | Val Loss: 18.9196\n",
      "Validation RMSE improved (19.3940 --> 18.9196). Saving model...\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1456.6668 | Val Loss: 19.0435\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.9196\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1403.0933 | Val Loss: 18.9572\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 18.9196\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1404.8995 | Val Loss: 17.7210\n",
      "Validation RMSE improved (18.9196 --> 17.7210). Saving model...\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1416.3599 | Val Loss: 19.1946\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.7210\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 1426.5509 | Val Loss: 19.4789\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 17.7210\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 1408.8448 | Val Loss: 18.8255\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 17.7210\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 1428.4497 | Val Loss: 18.1502\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 17.7210\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 1410.0808 | Val Loss: 18.9138\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 17.7210\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 1469.0664 | Val Loss: 17.0356\n",
      "Validation RMSE improved (17.7210 --> 17.0356). Saving model...\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 1433.1235 | Val Loss: 18.6187\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.0356\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 1412.7988 | Val Loss: 18.3263\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 17.0356\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 1400.8917 | Val Loss: 19.1575\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 17.0356\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1407.3084 | Val Loss: 17.5208\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 17.0356\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 1499.4664 | Val Loss: 19.0689\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 17.0356\n",
      "Epoch  70/250 | Training... 96 | Train Loss (avg/element): 1460.4711 | Val Loss: 18.6155\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 17.0356\n",
      "Epoch  71/250 | Training... 96 | Train Loss (avg/element): 1416.2220 | Val Loss: 18.3657\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 17.0356\n",
      "Epoch  72/250 | Training... 96 | Train Loss (avg/element): 1419.8825 | Val Loss: 18.2381\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 17.0356\n",
      "Epoch  73/250 | Training... 96 | Train Loss (avg/element): 1425.3282 | Val Loss: 18.1384\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 17.0356\n",
      "Epoch  74/250 | Training... 96 | Train Loss (avg/element): 1423.7605 | Val Loss: 17.6878\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 17.0356\n",
      "\n",
      "--- Early stopping triggered after 74 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 74. Best Validation RMSE: 17.0356. Validation RMSE (Last Step): 3.6067.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9575.4808 | Val Loss: 102.9020\n",
      "Validation RMSE improved (inf --> 102.9020). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9258.4324 | Val Loss: 100.0213\n",
      "Validation RMSE improved (102.9020 --> 100.0213). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 8672.7486 | Val Loss: 96.6120\n",
      "Validation RMSE improved (100.0213 --> 96.6120). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 8052.9748 | Val Loss: 93.0314\n",
      "Validation RMSE improved (96.6120 --> 93.0314). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 7450.9707 | Val Loss: 89.2698\n",
      "Validation RMSE improved (93.0314 --> 89.2698). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 6833.7400 | Val Loss: 85.3393\n",
      "Validation RMSE improved (89.2698 --> 85.3393). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 6256.8147 | Val Loss: 81.3693\n",
      "Validation RMSE improved (85.3393 --> 81.3693). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 5671.6661 | Val Loss: 77.3752\n",
      "Validation RMSE improved (81.3693 --> 77.3752). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 5176.1474 | Val Loss: 73.4863\n",
      "Validation RMSE improved (77.3752 --> 73.4863). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 4704.2561 | Val Loss: 69.7055\n",
      "Validation RMSE improved (73.4863 --> 69.7055). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 4300.0356 | Val Loss: 66.0631\n",
      "Validation RMSE improved (69.7055 --> 66.0631). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 3854.3433 | Val Loss: 62.2600\n",
      "Validation RMSE improved (66.0631 --> 62.2600). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 3534.8928 | Val Loss: 58.5772\n",
      "Validation RMSE improved (62.2600 --> 58.5772). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 3138.3843 | Val Loss: 54.9889\n",
      "Validation RMSE improved (58.5772 --> 54.9889). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 2796.1801 | Val Loss: 51.4430\n",
      "Validation RMSE improved (54.9889 --> 51.4430). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 2505.3069 | Val Loss: 48.2597\n",
      "Validation RMSE improved (51.4430 --> 48.2597). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 2291.9544 | Val Loss: 45.2369\n",
      "Validation RMSE improved (48.2597 --> 45.2369). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 2096.5881 | Val Loss: 42.4490\n",
      "Validation RMSE improved (45.2369 --> 42.4490). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 1944.3144 | Val Loss: 39.8345\n",
      "Validation RMSE improved (42.4490 --> 39.8345). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 1813.5585 | Val Loss: 38.0386\n",
      "Validation RMSE improved (39.8345 --> 38.0386). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 1702.3168 | Val Loss: 35.2217\n",
      "Validation RMSE improved (38.0386 --> 35.2217). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 1626.4683 | Val Loss: 33.9653\n",
      "Validation RMSE improved (35.2217 --> 33.9653). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 1547.6283 | Val Loss: 32.6447\n",
      "Validation RMSE improved (33.9653 --> 32.6447). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 1461.2069 | Val Loss: 30.6187\n",
      "Validation RMSE improved (32.6447 --> 30.6187). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 1402.7316 | Val Loss: 29.5655\n",
      "Validation RMSE improved (30.6187 --> 29.5655). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 1424.3657 | Val Loss: 28.5639\n",
      "Validation RMSE improved (29.5655 --> 28.5639). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 1369.3346 | Val Loss: 27.3571\n",
      "Validation RMSE improved (28.5639 --> 27.3571). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 1301.4493 | Val Loss: 26.9911\n",
      "Validation RMSE improved (27.3571 --> 26.9911). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 1291.0642 | Val Loss: 25.9038\n",
      "Validation RMSE improved (26.9911 --> 25.9038). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 1270.9057 | Val Loss: 26.3274\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 25.9038\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 1266.2622 | Val Loss: 27.7274\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 25.9038\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 1263.6789 | Val Loss: 25.3140\n",
      "Validation RMSE improved (25.9038 --> 25.3140). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 1231.6723 | Val Loss: 25.5640\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 25.3140\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 1193.7712 | Val Loss: 22.3086\n",
      "Validation RMSE improved (25.3140 --> 22.3086). Saving model...\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 1205.4029 | Val Loss: 23.0536\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 22.3086\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 1180.8465 | Val Loss: 21.8436\n",
      "Validation RMSE improved (22.3086 --> 21.8436). Saving model...\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 1223.8949 | Val Loss: 24.0094\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.8436\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1175.2899 | Val Loss: 25.9812\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.8436\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1160.6053 | Val Loss: 24.7125\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 21.8436\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1153.4820 | Val Loss: 22.4113\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 21.8436\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1159.4932 | Val Loss: 24.0903\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 21.8436\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1137.1541 | Val Loss: 24.7596\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 21.8436\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1135.1199 | Val Loss: 21.6600\n",
      "Validation RMSE improved (21.8436 --> 21.6600). Saving model...\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1123.4797 | Val Loss: 22.1015\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.6600\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1120.8253 | Val Loss: 24.8988\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.6600\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1130.7494 | Val Loss: 24.2975\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 21.6600\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1109.2944 | Val Loss: 26.7332\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 21.6600\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1126.7395 | Val Loss: 22.5033\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 21.6600\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1107.4131 | Val Loss: 21.9266\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 21.6600\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1124.5720 | Val Loss: 20.8837\n",
      "Validation RMSE improved (21.6600 --> 20.8837). Saving model...\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1102.3807 | Val Loss: 25.2351\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.8837\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1103.2906 | Val Loss: 24.1253\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.8837\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1116.4975 | Val Loss: 22.3986\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 20.8837\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1126.2226 | Val Loss: 25.7933\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 20.8837\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1072.2762 | Val Loss: 22.0963\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 20.8837\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1110.1932 | Val Loss: 23.8666\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 20.8837\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1064.6393 | Val Loss: 24.9458\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 20.8837\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1069.3785 | Val Loss: 25.1100\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 20.8837\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1083.3300 | Val Loss: 19.9380\n",
      "Validation RMSE improved (20.8837 --> 19.9380). Saving model...\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 1084.8615 | Val Loss: 21.1709\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.9380\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 1083.0536 | Val Loss: 24.3906\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.9380\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 1092.1997 | Val Loss: 27.3228\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.9380\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 1213.7734 | Val Loss: 22.6965\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 19.9380\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 1110.0547 | Val Loss: 23.8050\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 19.9380\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 1116.6413 | Val Loss: 21.7175\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 19.9380\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 1098.9179 | Val Loss: 23.3333\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 19.9380\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 1089.9546 | Val Loss: 24.7839\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 19.9380\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1099.2157 | Val Loss: 23.9787\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 19.9380\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 1103.4618 | Val Loss: 22.0673\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 19.9380\n",
      "\n",
      "--- Early stopping triggered after 69 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 69. Best Validation RMSE: 19.9380. Validation RMSE (Last Step): 4.4251.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9744.8266 | Val Loss: 95.5150\n",
      "Validation RMSE improved (inf --> 95.5150). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9589.3706 | Val Loss: 93.9181\n",
      "Validation RMSE improved (95.5150 --> 93.9181). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 9211.5462 | Val Loss: 91.5134\n",
      "Validation RMSE improved (93.9181 --> 91.5134). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 8718.7917 | Val Loss: 88.5645\n",
      "Validation RMSE improved (91.5134 --> 88.5645). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 8166.0815 | Val Loss: 85.2728\n",
      "Validation RMSE improved (88.5645 --> 85.2728). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 7577.8689 | Val Loss: 81.7676\n",
      "Validation RMSE improved (85.2728 --> 81.7676). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 6970.6803 | Val Loss: 78.0724\n",
      "Validation RMSE improved (81.7676 --> 78.0724). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 6386.9391 | Val Loss: 74.3331\n",
      "Validation RMSE improved (78.0724 --> 74.3331). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 5813.7092 | Val Loss: 70.6009\n",
      "Validation RMSE improved (74.3331 --> 70.6009). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 5311.6873 | Val Loss: 66.9847\n",
      "Validation RMSE improved (70.6009 --> 66.9847). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 4856.7361 | Val Loss: 63.4560\n",
      "Validation RMSE improved (66.9847 --> 63.4560). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 4413.9109 | Val Loss: 59.5336\n",
      "Validation RMSE improved (63.4560 --> 59.5336). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 3936.1594 | Val Loss: 55.8661\n",
      "Validation RMSE improved (59.5336 --> 55.8661). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 3552.2015 | Val Loss: 52.2332\n",
      "Validation RMSE improved (55.8661 --> 52.2332). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 3199.9113 | Val Loss: 49.7141\n",
      "Validation RMSE improved (52.2332 --> 49.7141). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 2969.0320 | Val Loss: 45.9470\n",
      "Validation RMSE improved (49.7141 --> 45.9470). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 2605.2297 | Val Loss: 42.9054\n",
      "Validation RMSE improved (45.9470 --> 42.9054). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 2382.9735 | Val Loss: 40.3485\n",
      "Validation RMSE improved (42.9054 --> 40.3485). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 2286.0176 | Val Loss: 37.7817\n",
      "Validation RMSE improved (40.3485 --> 37.7817). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 2113.7962 | Val Loss: 35.4692\n",
      "Validation RMSE improved (37.7817 --> 35.4692). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 1948.3334 | Val Loss: 33.3977\n",
      "Validation RMSE improved (35.4692 --> 33.3977). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 1879.2294 | Val Loss: 31.5661\n",
      "Validation RMSE improved (33.3977 --> 31.5661). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 1754.3601 | Val Loss: 30.5746\n",
      "Validation RMSE improved (31.5661 --> 30.5746). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 1704.8690 | Val Loss: 29.2856\n",
      "Validation RMSE improved (30.5746 --> 29.2856). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 1697.6315 | Val Loss: 28.2914\n",
      "Validation RMSE improved (29.2856 --> 28.2914). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 1627.5131 | Val Loss: 27.2060\n",
      "Validation RMSE improved (28.2914 --> 27.2060). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 1602.6946 | Val Loss: 26.2313\n",
      "Validation RMSE improved (27.2060 --> 26.2313). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 1596.5849 | Val Loss: 25.7841\n",
      "Validation RMSE improved (26.2313 --> 25.7841). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 1588.2016 | Val Loss: 24.9213\n",
      "Validation RMSE improved (25.7841 --> 24.9213). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 1571.1014 | Val Loss: 24.0865\n",
      "Validation RMSE improved (24.9213 --> 24.0865). Saving model...\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 1513.9166 | Val Loss: 26.6475\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 24.0865\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 1514.6235 | Val Loss: 23.2289\n",
      "Validation RMSE improved (24.0865 --> 23.2289). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 1503.0043 | Val Loss: 22.6464\n",
      "Validation RMSE improved (23.2289 --> 22.6464). Saving model...\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 1498.2553 | Val Loss: 22.2643\n",
      "Validation RMSE improved (22.6464 --> 22.2643). Saving model...\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 1492.0272 | Val Loss: 22.2805\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 22.2643\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 1494.4274 | Val Loss: 22.0954\n",
      "Validation RMSE improved (22.2643 --> 22.0954). Saving model...\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 1477.4881 | Val Loss: 22.7476\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 22.0954\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1463.4182 | Val Loss: 21.3886\n",
      "Validation RMSE improved (22.0954 --> 21.3886). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1454.2328 | Val Loss: 25.1391\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.3886\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1497.2114 | Val Loss: 22.1486\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.3886\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1442.4860 | Val Loss: 21.8342\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 21.3886\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1482.2293 | Val Loss: 24.3783\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 21.3886\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1450.1962 | Val Loss: 20.8587\n",
      "Validation RMSE improved (21.3886 --> 20.8587). Saving model...\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1438.1822 | Val Loss: 21.9421\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.8587\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1438.2821 | Val Loss: 20.5174\n",
      "Validation RMSE improved (20.8587 --> 20.5174). Saving model...\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1445.5095 | Val Loss: 21.7277\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.5174\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1453.3695 | Val Loss: 20.9938\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.5174\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1429.7214 | Val Loss: 19.6642\n",
      "Validation RMSE improved (20.5174 --> 19.6642). Saving model...\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1458.0397 | Val Loss: 22.9729\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.6642\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1412.8556 | Val Loss: 18.7180\n",
      "Validation RMSE improved (19.6642 --> 18.7180). Saving model...\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1432.4366 | Val Loss: 20.5812\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.7180\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1410.4184 | Val Loss: 19.8410\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 18.7180\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1398.7566 | Val Loss: 18.2071\n",
      "Validation RMSE improved (18.7180 --> 18.2071). Saving model...\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1424.1590 | Val Loss: 17.8554\n",
      "Validation RMSE improved (18.2071 --> 17.8554). Saving model...\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1441.3514 | Val Loss: 17.8071\n",
      "Validation RMSE improved (17.8554 --> 17.8071). Saving model...\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1369.3569 | Val Loss: 21.0036\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.8071\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1396.0876 | Val Loss: 19.5915\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 17.8071\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1391.3473 | Val Loss: 18.1225\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 17.8071\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1411.4322 | Val Loss: 17.6087\n",
      "Validation RMSE improved (17.8071 --> 17.6087). Saving model...\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 1465.0210 | Val Loss: 18.0179\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.6087\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 1397.7472 | Val Loss: 18.5774\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 17.6087\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 1406.8370 | Val Loss: 19.0775\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 17.6087\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 1401.0643 | Val Loss: 20.6064\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 17.6087\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 1393.3643 | Val Loss: 18.5330\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 17.6087\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 1399.8427 | Val Loss: 17.9317\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 17.6087\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 1385.1415 | Val Loss: 17.8328\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 17.6087\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 1343.2146 | Val Loss: 17.1974\n",
      "Validation RMSE improved (17.6087 --> 17.1974). Saving model...\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1326.0969 | Val Loss: 17.0093\n",
      "Validation RMSE improved (17.1974 --> 17.0093). Saving model...\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 1326.8727 | Val Loss: 17.4779\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.0093\n",
      "Epoch  70/250 | Training... 96 | Train Loss (avg/element): 1338.7912 | Val Loss: 16.8185\n",
      "Validation RMSE improved (17.0093 --> 16.8185). Saving model...\n",
      "Epoch  71/250 | Training... 96 | Train Loss (avg/element): 1352.3752 | Val Loss: 17.2101\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 16.8185\n",
      "Epoch  72/250 | Training... 96 | Train Loss (avg/element): 1327.3800 | Val Loss: 17.4127\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 16.8185\n",
      "Epoch  73/250 | Training... 96 | Train Loss (avg/element): 1386.0240 | Val Loss: 18.8492\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 16.8185\n",
      "Epoch  74/250 | Training... 96 | Train Loss (avg/element): 1440.4632 | Val Loss: 19.1427\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 16.8185\n",
      "Epoch  75/250 | Training... 96 | Train Loss (avg/element): 1366.2745 | Val Loss: 17.8291\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 16.8185\n",
      "Epoch  76/250 | Training... 96 | Train Loss (avg/element): 1374.7745 | Val Loss: 19.4933\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 16.8185\n",
      "Epoch  77/250 | Training... 96 | Train Loss (avg/element): 1350.1028 | Val Loss: 16.7270\n",
      "Validation RMSE improved (16.8185 --> 16.7270). Saving model...\n",
      "Epoch  78/250 | Training... 96 | Train Loss (avg/element): 1332.7340 | Val Loss: 15.6988\n",
      "Validation RMSE improved (16.7270 --> 15.6988). Saving model...\n",
      "Epoch  79/250 | Training... 96 | Train Loss (avg/element): 1431.3004 | Val Loss: 18.7298\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 15.6988\n",
      "Epoch  80/250 | Training... 96 | Train Loss (avg/element): 1410.9958 | Val Loss: 17.9776\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 15.6988\n",
      "Epoch  81/250 | Training... 96 | Train Loss (avg/element): 1365.8992 | Val Loss: 17.3434\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 15.6988\n",
      "Epoch  82/250 | Training... 96 | Train Loss (avg/element): 1367.4987 | Val Loss: 18.7336\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 15.6988\n",
      "Epoch  83/250 | Training... 96 | Train Loss (avg/element): 1380.9010 | Val Loss: 18.5779\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 15.6988\n",
      "Epoch  84/250 | Training... 96 | Train Loss (avg/element): 1359.6632 | Val Loss: 18.4708\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 15.6988\n",
      "Epoch  85/250 | Training... 96 | Train Loss (avg/element): 1379.8227 | Val Loss: 18.2213\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 15.6988\n",
      "Epoch  86/250 | Training... 96 | Train Loss (avg/element): 1354.7730 | Val Loss: 17.8251\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 15.6988\n",
      "Epoch  87/250 | Training... 96 | Train Loss (avg/element): 1359.3293 | Val Loss: 17.2048\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 15.6988\n",
      "Epoch  88/250 | Training... 96 | Train Loss (avg/element): 1371.0519 | Val Loss: 17.1574\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 15.6988\n",
      "\n",
      "--- Early stopping triggered after 88 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 88. Best Validation RMSE: 15.6988. Validation RMSE (Last Step): 4.5268.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9814.4899 | Val Loss: 95.4202\n",
      "Validation RMSE improved (inf --> 95.4202). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9644.0376 | Val Loss: 94.3333\n",
      "Validation RMSE improved (95.4202 --> 94.3333). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 9427.7185 | Val Loss: 93.2192\n",
      "Validation RMSE improved (94.3333 --> 93.2192). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 9211.3527 | Val Loss: 92.0591\n",
      "Validation RMSE improved (93.2192 --> 92.0591). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 8986.4346 | Val Loss: 90.8550\n",
      "Validation RMSE improved (92.0591 --> 90.8550). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 8754.6722 | Val Loss: 89.5928\n",
      "Validation RMSE improved (90.8550 --> 89.5928). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 8520.9801 | Val Loss: 88.2703\n",
      "Validation RMSE improved (89.5928 --> 88.2703). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 8272.9200 | Val Loss: 86.8754\n",
      "Validation RMSE improved (88.2703 --> 86.8754). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 8013.1224 | Val Loss: 85.4028\n",
      "Validation RMSE improved (86.8754 --> 85.4028). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 7763.7621 | Val Loss: 83.8541\n",
      "Validation RMSE improved (85.4028 --> 83.8541). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 7477.3455 | Val Loss: 82.2288\n",
      "Validation RMSE improved (83.8541 --> 82.2288). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 7197.3028 | Val Loss: 80.5058\n",
      "Validation RMSE improved (82.2288 --> 80.5058). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 6906.2309 | Val Loss: 78.7307\n",
      "Validation RMSE improved (80.5058 --> 78.7307). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 6617.2374 | Val Loss: 76.9454\n",
      "Validation RMSE improved (78.7307 --> 76.9454). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 6335.7081 | Val Loss: 75.1376\n",
      "Validation RMSE improved (76.9454 --> 75.1376). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 6069.7005 | Val Loss: 73.3258\n",
      "Validation RMSE improved (75.1376 --> 73.3258). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 5782.3656 | Val Loss: 71.5051\n",
      "Validation RMSE improved (73.3258 --> 71.5051). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 5536.2077 | Val Loss: 69.6931\n",
      "Validation RMSE improved (71.5051 --> 69.6931). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 5291.3210 | Val Loss: 67.8984\n",
      "Validation RMSE improved (69.6931 --> 67.8984). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 5072.8196 | Val Loss: 66.0650\n",
      "Validation RMSE improved (67.8984 --> 66.0650). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 4787.6489 | Val Loss: 63.8244\n",
      "Validation RMSE improved (66.0650 --> 63.8244). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 4539.8193 | Val Loss: 61.8648\n",
      "Validation RMSE improved (63.8244 --> 61.8648). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 4278.9391 | Val Loss: 59.9150\n",
      "Validation RMSE improved (61.8648 --> 59.9150). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 4047.5728 | Val Loss: 58.0024\n",
      "Validation RMSE improved (59.9150 --> 58.0024). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 3815.1439 | Val Loss: 56.1452\n",
      "Validation RMSE improved (58.0024 --> 56.1452). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 3614.5974 | Val Loss: 54.2704\n",
      "Validation RMSE improved (56.1452 --> 54.2704). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 3408.9402 | Val Loss: 52.4317\n",
      "Validation RMSE improved (54.2704 --> 52.4317). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 3239.2653 | Val Loss: 50.6525\n",
      "Validation RMSE improved (52.4317 --> 50.6525). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 3064.8687 | Val Loss: 48.8510\n",
      "Validation RMSE improved (50.6525 --> 48.8510). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 2921.6772 | Val Loss: 47.0820\n",
      "Validation RMSE improved (48.8510 --> 47.0820). Saving model...\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 2761.1867 | Val Loss: 45.4988\n",
      "Validation RMSE improved (47.0820 --> 45.4988). Saving model...\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 2589.3293 | Val Loss: 43.7747\n",
      "Validation RMSE improved (45.4988 --> 43.7747). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 2459.5938 | Val Loss: 42.1687\n",
      "Validation RMSE improved (43.7747 --> 42.1687). Saving model...\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 2334.2305 | Val Loss: 40.7587\n",
      "Validation RMSE improved (42.1687 --> 40.7587). Saving model...\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 2205.0081 | Val Loss: 39.2061\n",
      "Validation RMSE improved (40.7587 --> 39.2061). Saving model...\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 2104.4253 | Val Loss: 37.9478\n",
      "Validation RMSE improved (39.2061 --> 37.9478). Saving model...\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 2019.4579 | Val Loss: 36.5893\n",
      "Validation RMSE improved (37.9478 --> 36.5893). Saving model...\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1934.3082 | Val Loss: 35.4121\n",
      "Validation RMSE improved (36.5893 --> 35.4121). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1882.6986 | Val Loss: 33.9236\n",
      "Validation RMSE improved (35.4121 --> 33.9236). Saving model...\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1783.0932 | Val Loss: 33.0356\n",
      "Validation RMSE improved (33.9236 --> 33.0356). Saving model...\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1731.0753 | Val Loss: 31.8189\n",
      "Validation RMSE improved (33.0356 --> 31.8189). Saving model...\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1683.8662 | Val Loss: 30.7509\n",
      "Validation RMSE improved (31.8189 --> 30.7509). Saving model...\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1598.8559 | Val Loss: 29.5497\n",
      "Validation RMSE improved (30.7509 --> 29.5497). Saving model...\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1528.4646 | Val Loss: 27.8756\n",
      "Validation RMSE improved (29.5497 --> 27.8756). Saving model...\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1454.1913 | Val Loss: 26.8370\n",
      "Validation RMSE improved (27.8756 --> 26.8370). Saving model...\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1439.3808 | Val Loss: 25.1358\n",
      "Validation RMSE improved (26.8370 --> 25.1358). Saving model...\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1373.4206 | Val Loss: 24.0914\n",
      "Validation RMSE improved (25.1358 --> 24.0914). Saving model...\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1362.4768 | Val Loss: 22.9242\n",
      "Validation RMSE improved (24.0914 --> 22.9242). Saving model...\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1334.3898 | Val Loss: 22.5030\n",
      "Validation RMSE improved (22.9242 --> 22.5030). Saving model...\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1373.2307 | Val Loss: 22.1755\n",
      "Validation RMSE improved (22.5030 --> 22.1755). Saving model...\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1282.1641 | Val Loss: 21.4149\n",
      "Validation RMSE improved (22.1755 --> 21.4149). Saving model...\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1271.9563 | Val Loss: 21.4888\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.4149\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1254.7473 | Val Loss: 20.6514\n",
      "Validation RMSE improved (21.4149 --> 20.6514). Saving model...\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1267.3664 | Val Loss: 20.3510\n",
      "Validation RMSE improved (20.6514 --> 20.3510). Saving model...\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1239.9061 | Val Loss: 21.3352\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.3510\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1286.9426 | Val Loss: 20.5903\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.3510\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1257.5571 | Val Loss: 20.7562\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 20.3510\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1238.2789 | Val Loss: 21.5405\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 20.3510\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1257.9798 | Val Loss: 20.8666\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 20.3510\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 1238.2144 | Val Loss: 21.5569\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 20.3510\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 1194.6293 | Val Loss: 20.1479\n",
      "Validation RMSE improved (20.3510 --> 20.1479). Saving model...\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 1219.3356 | Val Loss: 19.7182\n",
      "Validation RMSE improved (20.1479 --> 19.7182). Saving model...\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 1247.3725 | Val Loss: 21.9420\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.7182\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 1223.8991 | Val Loss: 21.6760\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.7182\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 1217.9639 | Val Loss: 21.2937\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.7182\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 1221.3031 | Val Loss: 21.7772\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 19.7182\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 1205.8333 | Val Loss: 21.3196\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 19.7182\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1208.2310 | Val Loss: 20.5079\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 19.7182\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 1218.9574 | Val Loss: 20.9595\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 19.7182\n",
      "Epoch  70/250 | Training... 96 | Train Loss (avg/element): 1217.2836 | Val Loss: 20.2507\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 19.7182\n",
      "Epoch  71/250 | Training... 96 | Train Loss (avg/element): 1205.1068 | Val Loss: 19.1164\n",
      "Validation RMSE improved (19.7182 --> 19.1164). Saving model...\n",
      "Epoch  72/250 | Training... 96 | Train Loss (avg/element): 1180.5028 | Val Loss: 20.0971\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.1164\n",
      "Epoch  73/250 | Training... 96 | Train Loss (avg/element): 1171.5337 | Val Loss: 19.9427\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.1164\n",
      "Epoch  74/250 | Training... 96 | Train Loss (avg/element): 1176.6876 | Val Loss: 19.3392\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.1164\n",
      "Epoch  75/250 | Training... 96 | Train Loss (avg/element): 1181.7616 | Val Loss: 19.6587\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 19.1164\n",
      "Epoch  76/250 | Training... 96 | Train Loss (avg/element): 1175.9745 | Val Loss: 18.5551\n",
      "Validation RMSE improved (19.1164 --> 18.5551). Saving model...\n",
      "Epoch  77/250 | Training... 96 | Train Loss (avg/element): 1240.1430 | Val Loss: 19.3439\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.5551\n",
      "Epoch  78/250 | Training... 96 | Train Loss (avg/element): 1176.9589 | Val Loss: 17.8001\n",
      "Validation RMSE improved (18.5551 --> 17.8001). Saving model...\n",
      "Epoch  79/250 | Training... 96 | Train Loss (avg/element): 1173.7968 | Val Loss: 18.9811\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.8001\n",
      "Epoch  80/250 | Training... 96 | Train Loss (avg/element): 1152.2380 | Val Loss: 18.2261\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 17.8001\n",
      "Epoch  81/250 | Training... 96 | Train Loss (avg/element): 1124.2036 | Val Loss: 19.4497\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 17.8001\n",
      "Epoch  82/250 | Training... 96 | Train Loss (avg/element): 1153.5349 | Val Loss: 18.4339\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 17.8001\n",
      "Epoch  83/250 | Training... 96 | Train Loss (avg/element): 1129.0840 | Val Loss: 18.4743\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 17.8001\n",
      "Epoch  84/250 | Training... 96 | Train Loss (avg/element): 1137.6174 | Val Loss: 19.0191\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 17.8001\n",
      "Epoch  85/250 | Training... 96 | Train Loss (avg/element): 1145.6722 | Val Loss: 18.9908\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 17.8001\n",
      "Epoch  86/250 | Training... 96 | Train Loss (avg/element): 1130.3610 | Val Loss: 19.1962\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 17.8001\n",
      "Epoch  87/250 | Training... 96 | Train Loss (avg/element): 1200.2865 | Val Loss: 19.4360\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 17.8001\n",
      "Epoch  88/250 | Training... 96 | Train Loss (avg/element): 1142.6683 | Val Loss: 18.2059\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 17.8001\n",
      "\n",
      "--- Early stopping triggered after 88 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 88. Best Validation RMSE: 17.8001. Validation RMSE (Last Step): 5.9560.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9732.5251 | Val Loss: 97.6999\n",
      "Validation RMSE improved (inf --> 97.6999). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9219.1360 | Val Loss: 93.8261\n",
      "Validation RMSE improved (97.6999 --> 93.8261). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 8458.4617 | Val Loss: 89.1602\n",
      "Validation RMSE improved (93.8261 --> 89.1602). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 7609.6382 | Val Loss: 84.1400\n",
      "Validation RMSE improved (89.1602 --> 84.1400). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 6761.4510 | Val Loss: 78.7427\n",
      "Validation RMSE improved (84.1400 --> 78.7427). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 5974.4727 | Val Loss: 73.3501\n",
      "Validation RMSE improved (78.7427 --> 73.3501). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 5215.3764 | Val Loss: 68.0837\n",
      "Validation RMSE improved (73.3501 --> 68.0837). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 4559.1720 | Val Loss: 63.0814\n",
      "Validation RMSE improved (68.0837 --> 63.0814). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 3993.9786 | Val Loss: 57.4813\n",
      "Validation RMSE improved (63.0814 --> 57.4813). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 3380.8410 | Val Loss: 52.5081\n",
      "Validation RMSE improved (57.4813 --> 52.5081). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 2971.0492 | Val Loss: 47.7621\n",
      "Validation RMSE improved (52.5081 --> 47.7621). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 2567.8208 | Val Loss: 43.7653\n",
      "Validation RMSE improved (47.7621 --> 43.7653). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 2256.0563 | Val Loss: 39.8034\n",
      "Validation RMSE improved (43.7653 --> 39.8034). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 2005.0544 | Val Loss: 36.3699\n",
      "Validation RMSE improved (39.8034 --> 36.3699). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 1904.1378 | Val Loss: 33.1380\n",
      "Validation RMSE improved (36.3699 --> 33.1380). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 1703.9846 | Val Loss: 30.8041\n",
      "Validation RMSE improved (33.1380 --> 30.8041). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 1591.8987 | Val Loss: 29.6330\n",
      "Validation RMSE improved (30.8041 --> 29.6330). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 1548.1778 | Val Loss: 27.0177\n",
      "Validation RMSE improved (29.6330 --> 27.0177). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 1465.1160 | Val Loss: 25.3961\n",
      "Validation RMSE improved (27.0177 --> 25.3961). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 1413.4827 | Val Loss: 24.3395\n",
      "Validation RMSE improved (25.3961 --> 24.3395). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 1366.3656 | Val Loss: 23.2308\n",
      "Validation RMSE improved (24.3395 --> 23.2308). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 1345.8711 | Val Loss: 22.4459\n",
      "Validation RMSE improved (23.2308 --> 22.4459). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 1363.9338 | Val Loss: 21.6879\n",
      "Validation RMSE improved (22.4459 --> 21.6879). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 1364.5571 | Val Loss: 21.1879\n",
      "Validation RMSE improved (21.6879 --> 21.1879). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 1329.8275 | Val Loss: 20.9728\n",
      "Validation RMSE improved (21.1879 --> 20.9728). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 1317.9020 | Val Loss: 21.3743\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.9728\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 1313.3053 | Val Loss: 19.5896\n",
      "Validation RMSE improved (20.9728 --> 19.5896). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 1277.3412 | Val Loss: 18.9380\n",
      "Validation RMSE improved (19.5896 --> 18.9380). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 1263.3417 | Val Loss: 19.1218\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.9380\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 1286.6575 | Val Loss: 19.5859\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 18.9380\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 1344.3904 | Val Loss: 19.0326\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 18.9380\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 1288.3264 | Val Loss: 18.2089\n",
      "Validation RMSE improved (18.9380 --> 18.2089). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 1248.0348 | Val Loss: 18.5332\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.2089\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 1253.1778 | Val Loss: 18.4449\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 18.2089\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 1242.2213 | Val Loss: 17.9773\n",
      "Validation RMSE improved (18.2089 --> 17.9773). Saving model...\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 1209.6545 | Val Loss: 19.0156\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.9773\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 1219.4605 | Val Loss: 17.8879\n",
      "Validation RMSE improved (17.9773 --> 17.8879). Saving model...\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1230.8734 | Val Loss: 16.5968\n",
      "Validation RMSE improved (17.8879 --> 16.5968). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1200.9557 | Val Loss: 16.5159\n",
      "Validation RMSE improved (16.5968 --> 16.5159). Saving model...\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1221.4080 | Val Loss: 18.8570\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 16.5159\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1256.5768 | Val Loss: 16.6470\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 16.5159\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1181.2884 | Val Loss: 15.8765\n",
      "Validation RMSE improved (16.5159 --> 15.8765). Saving model...\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1182.7262 | Val Loss: 15.9785\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 15.8765\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1163.0335 | Val Loss: 15.9341\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 15.8765\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1202.0574 | Val Loss: 15.5457\n",
      "Validation RMSE improved (15.8765 --> 15.5457). Saving model...\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1154.7982 | Val Loss: 15.4122\n",
      "Validation RMSE improved (15.5457 --> 15.4122). Saving model...\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1140.6179 | Val Loss: 15.1446\n",
      "Validation RMSE improved (15.4122 --> 15.1446). Saving model...\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1173.3528 | Val Loss: 16.1308\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 15.1446\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1230.3528 | Val Loss: 15.1295\n",
      "Validation RMSE improved (15.1446 --> 15.1295). Saving model...\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1171.3639 | Val Loss: 14.9472\n",
      "Validation RMSE improved (15.1295 --> 14.9472). Saving model...\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1165.5925 | Val Loss: 14.9802\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 14.9472\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1132.5986 | Val Loss: 14.5514\n",
      "Validation RMSE improved (14.9472 --> 14.5514). Saving model...\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1129.1899 | Val Loss: 13.9371\n",
      "Validation RMSE improved (14.5514 --> 13.9371). Saving model...\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1133.5288 | Val Loss: 13.9581\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 13.9371\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1127.7755 | Val Loss: 13.7234\n",
      "Validation RMSE improved (13.9371 --> 13.7234). Saving model...\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1115.4342 | Val Loss: 14.7230\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 13.7234\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1176.5559 | Val Loss: 13.9818\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 13.7234\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1121.0639 | Val Loss: 13.0114\n",
      "Validation RMSE improved (13.7234 --> 13.0114). Saving model...\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1108.1726 | Val Loss: 13.0434\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 13.0114\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 1116.5237 | Val Loss: 13.2516\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 13.0114\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 1096.8851 | Val Loss: 13.1422\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 13.0114\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 1098.5333 | Val Loss: 12.9612\n",
      "Validation RMSE improved (13.0114 --> 12.9612). Saving model...\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 1114.8109 | Val Loss: 12.8342\n",
      "Validation RMSE improved (12.9612 --> 12.8342). Saving model...\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 1081.6568 | Val Loss: 13.4089\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 12.8342\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 1100.4583 | Val Loss: 13.8436\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 12.8342\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 1130.6138 | Val Loss: 13.6845\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 12.8342\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 1097.7605 | Val Loss: 12.5102\n",
      "Validation RMSE improved (12.8342 --> 12.5102). Saving model...\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1080.6308 | Val Loss: 12.6315\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 12.5102\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 1088.1656 | Val Loss: 12.9627\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 12.5102\n",
      "Epoch  70/250 | Training... 96 | Train Loss (avg/element): 1108.6851 | Val Loss: 12.3604\n",
      "Validation RMSE improved (12.5102 --> 12.3604). Saving model...\n",
      "Epoch  71/250 | Training... 96 | Train Loss (avg/element): 1067.2708 | Val Loss: 12.0760\n",
      "Validation RMSE improved (12.3604 --> 12.0760). Saving model...\n",
      "Epoch  72/250 | Training... 96 | Train Loss (avg/element): 1080.1008 | Val Loss: 12.1385\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 12.0760\n",
      "Epoch  73/250 | Training... 96 | Train Loss (avg/element): 1077.6401 | Val Loss: 13.3395\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 12.0760\n",
      "Epoch  74/250 | Training... 96 | Train Loss (avg/element): 1042.6742 | Val Loss: 11.8403\n",
      "Validation RMSE improved (12.0760 --> 11.8403). Saving model...\n",
      "Epoch  75/250 | Training... 96 | Train Loss (avg/element): 1047.8292 | Val Loss: 12.9585\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 11.8403\n",
      "Epoch  76/250 | Training... 96 | Train Loss (avg/element): 1121.8562 | Val Loss: 13.1101\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 11.8403\n",
      "Epoch  77/250 | Training... 96 | Train Loss (avg/element): 1073.5722 | Val Loss: 12.2810\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 11.8403\n",
      "Epoch  78/250 | Training... 96 | Train Loss (avg/element): 1070.3959 | Val Loss: 12.2280\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 11.8403\n",
      "Epoch  79/250 | Training... 96 | Train Loss (avg/element): 1056.7748 | Val Loss: 12.3770\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 11.8403\n",
      "Epoch  80/250 | Training... 96 | Train Loss (avg/element): 1072.2370 | Val Loss: 12.5717\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 11.8403\n",
      "Epoch  81/250 | Training... 96 | Train Loss (avg/element): 1070.1686 | Val Loss: 12.7774\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 11.8403\n",
      "Epoch  82/250 | Training... 96 | Train Loss (avg/element): 1070.1369 | Val Loss: 11.7455\n",
      "Validation RMSE improved (11.8403 --> 11.7455). Saving model...\n",
      "Epoch  83/250 | Training... 96 | Train Loss (avg/element): 1159.7882 | Val Loss: 13.3585\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 11.7455\n",
      "Epoch  84/250 | Training... 96 | Train Loss (avg/element): 1074.0702 | Val Loss: 12.0771\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 11.7455\n",
      "Epoch  85/250 | Training... 96 | Train Loss (avg/element): 1065.4857 | Val Loss: 11.7763\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 11.7455\n",
      "Epoch  86/250 | Training... 96 | Train Loss (avg/element): 1104.1616 | Val Loss: 12.7945\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 11.7455\n",
      "Epoch  87/250 | Training... 96 | Train Loss (avg/element): 1058.5787 | Val Loss: 11.8693\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 11.7455\n",
      "Epoch  88/250 | Training... 96 | Train Loss (avg/element): 1113.2895 | Val Loss: 12.3321\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 11.7455\n",
      "Epoch  89/250 | Training... 96 | Train Loss (avg/element): 1053.2940 | Val Loss: 12.1997\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 11.7455\n",
      "Epoch  90/250 | Training... 96 | Train Loss (avg/element): 1045.0641 | Val Loss: 12.0565\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 11.7455\n",
      "Epoch  91/250 | Training... 96 | Train Loss (avg/element): 1131.2252 | Val Loss: 13.6173\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 11.7455\n",
      "Epoch  92/250 | Training... 96 | Train Loss (avg/element): 1101.8000 | Val Loss: 12.0727\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 11.7455\n",
      "\n",
      "--- Early stopping triggered after 92 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 92. Best Validation RMSE: 11.7455. Validation RMSE (Last Step): 3.2511.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9765.8090 | Val Loss: 97.4348\n",
      "Validation RMSE improved (inf --> 97.4348). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9481.9906 | Val Loss: 94.7512\n",
      "Validation RMSE improved (97.4348 --> 94.7512). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 8899.1806 | Val Loss: 91.1345\n",
      "Validation RMSE improved (94.7512 --> 91.1345). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 8179.0926 | Val Loss: 86.6965\n",
      "Validation RMSE improved (91.1345 --> 86.6965). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 7368.8829 | Val Loss: 81.8168\n",
      "Validation RMSE improved (86.6965 --> 81.8168). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 6576.0775 | Val Loss: 76.7552\n",
      "Validation RMSE improved (81.8168 --> 76.7552). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 5794.5908 | Val Loss: 71.6104\n",
      "Validation RMSE improved (76.7552 --> 71.6104). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 5084.5203 | Val Loss: 66.6161\n",
      "Validation RMSE improved (71.6104 --> 66.6161). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 4469.1044 | Val Loss: 61.8331\n",
      "Validation RMSE improved (66.6161 --> 61.8331). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 3891.3061 | Val Loss: 56.8925\n",
      "Validation RMSE improved (61.8331 --> 56.8925). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 3316.8684 | Val Loss: 51.5634\n",
      "Validation RMSE improved (56.8925 --> 51.5634). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 2845.1667 | Val Loss: 48.2693\n",
      "Validation RMSE improved (51.5634 --> 48.2693). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 2482.1855 | Val Loss: 43.4531\n",
      "Validation RMSE improved (48.2693 --> 43.4531). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 2143.6863 | Val Loss: 39.8509\n",
      "Validation RMSE improved (43.4531 --> 39.8509). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 1919.5950 | Val Loss: 36.0129\n",
      "Validation RMSE improved (39.8509 --> 36.0129). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 1732.8209 | Val Loss: 33.7119\n",
      "Validation RMSE improved (36.0129 --> 33.7119). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 1560.8922 | Val Loss: 31.8686\n",
      "Validation RMSE improved (33.7119 --> 31.8686). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 1469.2317 | Val Loss: 29.4542\n",
      "Validation RMSE improved (31.8686 --> 29.4542). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 1348.4618 | Val Loss: 26.9869\n",
      "Validation RMSE improved (29.4542 --> 26.9869). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 1304.5516 | Val Loss: 26.9272\n",
      "Validation RMSE improved (26.9869 --> 26.9272). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 1249.9999 | Val Loss: 24.8276\n",
      "Validation RMSE improved (26.9272 --> 24.8276). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 1194.3028 | Val Loss: 25.5465\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 24.8276\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 1229.4677 | Val Loss: 26.5039\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 24.8276\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 1217.2163 | Val Loss: 23.4867\n",
      "Validation RMSE improved (24.8276 --> 23.4867). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 1177.2258 | Val Loss: 23.2871\n",
      "Validation RMSE improved (23.4867 --> 23.2871). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 1173.2008 | Val Loss: 24.3690\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 23.2871\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 1143.9821 | Val Loss: 22.5373\n",
      "Validation RMSE improved (23.2871 --> 22.5373). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 1113.9607 | Val Loss: 21.7161\n",
      "Validation RMSE improved (22.5373 --> 21.7161). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 1124.4608 | Val Loss: 21.4168\n",
      "Validation RMSE improved (21.7161 --> 21.4168). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 1104.9145 | Val Loss: 20.6357\n",
      "Validation RMSE improved (21.4168 --> 20.6357). Saving model...\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 1123.2732 | Val Loss: 19.9514\n",
      "Validation RMSE improved (20.6357 --> 19.9514). Saving model...\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 1087.5049 | Val Loss: 20.4510\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.9514\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 1098.2329 | Val Loss: 21.9591\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.9514\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 1168.1767 | Val Loss: 21.9488\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.9514\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 1058.3800 | Val Loss: 20.5277\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 19.9514\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 1036.0245 | Val Loss: 19.4267\n",
      "Validation RMSE improved (19.9514 --> 19.4267). Saving model...\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 1053.2501 | Val Loss: 21.1247\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.4267\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1040.6011 | Val Loss: 20.1973\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.4267\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1047.5216 | Val Loss: 19.7891\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.4267\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1061.2805 | Val Loss: 17.8504\n",
      "Validation RMSE improved (19.4267 --> 17.8504). Saving model...\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1074.0089 | Val Loss: 19.6205\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.8504\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1078.1954 | Val Loss: 20.6934\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 17.8504\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1067.0261 | Val Loss: 19.6520\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 17.8504\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1052.2041 | Val Loss: 19.6434\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 17.8504\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1120.1549 | Val Loss: 21.3104\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 17.8504\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1077.7921 | Val Loss: 21.3220\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 17.8504\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1071.5482 | Val Loss: 20.4795\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 17.8504\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1059.7449 | Val Loss: 22.9970\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 17.8504\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1092.1623 | Val Loss: 24.1323\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 17.8504\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1160.2566 | Val Loss: 22.7914\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 17.8504\n",
      "\n",
      "--- Early stopping triggered after 50 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 50. Best Validation RMSE: 17.8504. Validation RMSE (Last Step): 5.0658.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9724.9817 | Val Loss: 98.9986\n",
      "Validation RMSE improved (inf --> 98.9986). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9634.2387 | Val Loss: 98.2539\n",
      "Validation RMSE improved (98.9986 --> 98.2539). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 9486.5270 | Val Loss: 97.4845\n",
      "Validation RMSE improved (98.2539 --> 97.4845). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 9334.8705 | Val Loss: 96.6305\n",
      "Validation RMSE improved (97.4845 --> 96.6305). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 9168.7069 | Val Loss: 95.7050\n",
      "Validation RMSE improved (96.6305 --> 95.7050). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 8989.3789 | Val Loss: 94.7550\n",
      "Validation RMSE improved (95.7050 --> 94.7550). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 8810.6275 | Val Loss: 93.7683\n",
      "Validation RMSE improved (94.7550 --> 93.7683). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 8620.5870 | Val Loss: 92.7376\n",
      "Validation RMSE improved (93.7683 --> 92.7376). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 8440.7486 | Val Loss: 91.6740\n",
      "Validation RMSE improved (92.7376 --> 91.6740). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 8243.6532 | Val Loss: 90.5742\n",
      "Validation RMSE improved (91.6740 --> 90.5742). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 8053.3189 | Val Loss: 89.4498\n",
      "Validation RMSE improved (90.5742 --> 89.4498). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 7842.4100 | Val Loss: 88.2892\n",
      "Validation RMSE improved (89.4498 --> 88.2892). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 7656.7188 | Val Loss: 87.1090\n",
      "Validation RMSE improved (88.2892 --> 87.1090). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 7446.2574 | Val Loss: 85.9003\n",
      "Validation RMSE improved (87.1090 --> 85.9003). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 7247.3235 | Val Loss: 84.6736\n",
      "Validation RMSE improved (85.9003 --> 84.6736). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 7070.5561 | Val Loss: 83.4415\n",
      "Validation RMSE improved (84.6736 --> 83.4415). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 6879.3938 | Val Loss: 82.1950\n",
      "Validation RMSE improved (83.4415 --> 82.1950). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 6677.2426 | Val Loss: 80.9345\n",
      "Validation RMSE improved (82.1950 --> 80.9345). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 6492.7014 | Val Loss: 79.6669\n",
      "Validation RMSE improved (80.9345 --> 79.6669). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 6280.3744 | Val Loss: 78.3862\n",
      "Validation RMSE improved (79.6669 --> 78.3862). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 6110.2810 | Val Loss: 77.1111\n",
      "Validation RMSE improved (78.3862 --> 77.1111). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 5933.0975 | Val Loss: 75.8358\n",
      "Validation RMSE improved (77.1111 --> 75.8358). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 5773.8004 | Val Loss: 74.5654\n",
      "Validation RMSE improved (75.8358 --> 74.5654). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 5622.8054 | Val Loss: 73.2987\n",
      "Validation RMSE improved (74.5654 --> 73.2987). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 5439.1016 | Val Loss: 72.0265\n",
      "Validation RMSE improved (73.2987 --> 72.0265). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 5281.0626 | Val Loss: 70.7034\n",
      "Validation RMSE improved (72.0265 --> 70.7034). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 5114.1813 | Val Loss: 69.2885\n",
      "Validation RMSE improved (70.7034 --> 69.2885). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 4916.6187 | Val Loss: 68.0938\n",
      "Validation RMSE improved (69.2885 --> 68.0938). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 4740.7858 | Val Loss: 66.7098\n",
      "Validation RMSE improved (68.0938 --> 66.7098). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 4569.2921 | Val Loss: 65.0173\n",
      "Validation RMSE improved (66.7098 --> 65.0173). Saving model...\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 4385.5957 | Val Loss: 63.6592\n",
      "Validation RMSE improved (65.0173 --> 63.6592). Saving model...\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 4244.6693 | Val Loss: 62.3414\n",
      "Validation RMSE improved (63.6592 --> 62.3414). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 4112.3582 | Val Loss: 60.9854\n",
      "Validation RMSE improved (62.3414 --> 60.9854). Saving model...\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 4004.2274 | Val Loss: 59.6550\n",
      "Validation RMSE improved (60.9854 --> 59.6550). Saving model...\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 3810.1245 | Val Loss: 58.2903\n",
      "Validation RMSE improved (59.6550 --> 58.2903). Saving model...\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 3724.5826 | Val Loss: 57.0148\n",
      "Validation RMSE improved (58.2903 --> 57.0148). Saving model...\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 3630.4290 | Val Loss: 55.7850\n",
      "Validation RMSE improved (57.0148 --> 55.7850). Saving model...\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 3498.4756 | Val Loss: 54.4848\n",
      "Validation RMSE improved (55.7850 --> 54.4848). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 3395.1951 | Val Loss: 53.1862\n",
      "Validation RMSE improved (54.4848 --> 53.1862). Saving model...\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 3254.3224 | Val Loss: 51.9314\n",
      "Validation RMSE improved (53.1862 --> 51.9314). Saving model...\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 3162.0445 | Val Loss: 50.7824\n",
      "Validation RMSE improved (51.9314 --> 50.7824). Saving model...\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 3062.4562 | Val Loss: 49.6216\n",
      "Validation RMSE improved (50.7824 --> 49.6216). Saving model...\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 2997.8241 | Val Loss: 48.4127\n",
      "Validation RMSE improved (49.6216 --> 48.4127). Saving model...\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 2905.6263 | Val Loss: 47.2134\n",
      "Validation RMSE improved (48.4127 --> 47.2134). Saving model...\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 2845.4190 | Val Loss: 46.2449\n",
      "Validation RMSE improved (47.2134 --> 46.2449). Saving model...\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 2712.1071 | Val Loss: 45.1108\n",
      "Validation RMSE improved (46.2449 --> 45.1108). Saving model...\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 2695.9371 | Val Loss: 44.0850\n",
      "Validation RMSE improved (45.1108 --> 44.0850). Saving model...\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 2596.7448 | Val Loss: 42.8923\n",
      "Validation RMSE improved (44.0850 --> 42.8923). Saving model...\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 2518.8372 | Val Loss: 41.9858\n",
      "Validation RMSE improved (42.8923 --> 41.9858). Saving model...\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 2516.2071 | Val Loss: 40.9119\n",
      "Validation RMSE improved (41.9858 --> 40.9119). Saving model...\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 2469.6556 | Val Loss: 40.0578\n",
      "Validation RMSE improved (40.9119 --> 40.0578). Saving model...\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 2440.1394 | Val Loss: 39.3320\n",
      "Validation RMSE improved (40.0578 --> 39.3320). Saving model...\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 2369.9400 | Val Loss: 39.1410\n",
      "Validation RMSE improved (39.3320 --> 39.1410). Saving model...\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 2339.1938 | Val Loss: 37.4839\n",
      "Validation RMSE improved (39.1410 --> 37.4839). Saving model...\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 2299.3801 | Val Loss: 36.4824\n",
      "Validation RMSE improved (37.4839 --> 36.4824). Saving model...\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 2260.3740 | Val Loss: 35.6628\n",
      "Validation RMSE improved (36.4824 --> 35.6628). Saving model...\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 2206.5821 | Val Loss: 34.7448\n",
      "Validation RMSE improved (35.6628 --> 34.7448). Saving model...\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 2135.2829 | Val Loss: 34.5694\n",
      "Validation RMSE improved (34.7448 --> 34.5694). Saving model...\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 2163.5528 | Val Loss: 33.1923\n",
      "Validation RMSE improved (34.5694 --> 33.1923). Saving model...\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 2145.7842 | Val Loss: 32.7595\n",
      "Validation RMSE improved (33.1923 --> 32.7595). Saving model...\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 2112.4609 | Val Loss: 32.1644\n",
      "Validation RMSE improved (32.7595 --> 32.1644). Saving model...\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 2094.1891 | Val Loss: 32.2253\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 32.1644\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 2071.6438 | Val Loss: 31.1565\n",
      "Validation RMSE improved (32.1644 --> 31.1565). Saving model...\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 2030.9347 | Val Loss: 30.3768\n",
      "Validation RMSE improved (31.1565 --> 30.3768). Saving model...\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 2051.1668 | Val Loss: 29.6350\n",
      "Validation RMSE improved (30.3768 --> 29.6350). Saving model...\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 2080.8229 | Val Loss: 28.9251\n",
      "Validation RMSE improved (29.6350 --> 28.9251). Saving model...\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 2011.3765 | Val Loss: 28.3859\n",
      "Validation RMSE improved (28.9251 --> 28.3859). Saving model...\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1980.5163 | Val Loss: 28.1823\n",
      "Validation RMSE improved (28.3859 --> 28.1823). Saving model...\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 2013.6027 | Val Loss: 29.3607\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 28.1823\n",
      "Epoch  70/250 | Training... 96 | Train Loss (avg/element): 1947.1311 | Val Loss: 27.9840\n",
      "Validation RMSE improved (28.1823 --> 27.9840). Saving model...\n",
      "Epoch  71/250 | Training... 96 | Train Loss (avg/element): 1961.5781 | Val Loss: 27.2580\n",
      "Validation RMSE improved (27.9840 --> 27.2580). Saving model...\n",
      "Epoch  72/250 | Training... 96 | Train Loss (avg/element): 1973.1070 | Val Loss: 26.9832\n",
      "Validation RMSE improved (27.2580 --> 26.9832). Saving model...\n",
      "Epoch  73/250 | Training... 96 | Train Loss (avg/element): 1971.3328 | Val Loss: 26.4782\n",
      "Validation RMSE improved (26.9832 --> 26.4782). Saving model...\n",
      "Epoch  74/250 | Training... 96 | Train Loss (avg/element): 1969.2603 | Val Loss: 27.6674\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 26.4782\n",
      "Epoch  75/250 | Training... 96 | Train Loss (avg/element): 1953.1826 | Val Loss: 26.5262\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 26.4782\n",
      "Epoch  76/250 | Training... 96 | Train Loss (avg/element): 1951.1122 | Val Loss: 25.3958\n",
      "Validation RMSE improved (26.4782 --> 25.3958). Saving model...\n",
      "Epoch  77/250 | Training... 96 | Train Loss (avg/element): 1911.9044 | Val Loss: 25.6765\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 25.3958\n",
      "Epoch  78/250 | Training... 96 | Train Loss (avg/element): 1888.3933 | Val Loss: 28.2012\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 25.3958\n",
      "Epoch  79/250 | Training... 96 | Train Loss (avg/element): 1951.3216 | Val Loss: 25.9970\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 25.3958\n",
      "Epoch  80/250 | Training... 96 | Train Loss (avg/element): 1870.4017 | Val Loss: 26.1872\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 25.3958\n",
      "Epoch  81/250 | Training... 96 | Train Loss (avg/element): 1891.9514 | Val Loss: 25.5838\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 25.3958\n",
      "Epoch  82/250 | Training... 96 | Train Loss (avg/element): 1889.4697 | Val Loss: 25.7543\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 25.3958\n",
      "Epoch  83/250 | Training... 96 | Train Loss (avg/element): 1855.2783 | Val Loss: 24.9300\n",
      "Validation RMSE improved (25.3958 --> 24.9300). Saving model...\n",
      "Epoch  84/250 | Training... 96 | Train Loss (avg/element): 1880.8694 | Val Loss: 24.1932\n",
      "Validation RMSE improved (24.9300 --> 24.1932). Saving model...\n",
      "Epoch  85/250 | Training... 96 | Train Loss (avg/element): 1889.8439 | Val Loss: 24.5346\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 24.1932\n",
      "Epoch  86/250 | Training... 96 | Train Loss (avg/element): 1834.4199 | Val Loss: 24.5270\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 24.1932\n",
      "Epoch  87/250 | Training... 96 | Train Loss (avg/element): 1800.3294 | Val Loss: 23.6488\n",
      "Validation RMSE improved (24.1932 --> 23.6488). Saving model...\n",
      "Epoch  88/250 | Training... 96 | Train Loss (avg/element): 1877.7086 | Val Loss: 23.4913\n",
      "Validation RMSE improved (23.6488 --> 23.4913). Saving model...\n",
      "Epoch  89/250 | Training... 96 | Train Loss (avg/element): 1832.3234 | Val Loss: 22.2968\n",
      "Validation RMSE improved (23.4913 --> 22.2968). Saving model...\n",
      "Epoch  90/250 | Training... 96 | Train Loss (avg/element): 1857.2082 | Val Loss: 22.1043\n",
      "Validation RMSE improved (22.2968 --> 22.1043). Saving model...\n",
      "Epoch  91/250 | Training... 96 | Train Loss (avg/element): 1846.0860 | Val Loss: 23.8296\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 22.1043\n",
      "Epoch  92/250 | Training... 96 | Train Loss (avg/element): 1872.1487 | Val Loss: 24.0843\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 22.1043\n",
      "Epoch  93/250 | Training... 96 | Train Loss (avg/element): 1848.6889 | Val Loss: 23.9288\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 22.1043\n",
      "Epoch  94/250 | Training... 96 | Train Loss (avg/element): 1808.9326 | Val Loss: 23.4496\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 22.1043\n",
      "Epoch  95/250 | Training... 96 | Train Loss (avg/element): 1863.0365 | Val Loss: 24.0303\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 22.1043\n",
      "Epoch  96/250 | Training... 96 | Train Loss (avg/element): 1844.6933 | Val Loss: 22.0989\n",
      "Validation RMSE improved (22.1043 --> 22.0989). Saving model...\n",
      "Epoch  97/250 | Training... 96 | Train Loss (avg/element): 1826.6347 | Val Loss: 22.8315\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 22.0989\n",
      "Epoch  98/250 | Training... 96 | Train Loss (avg/element): 1863.0722 | Val Loss: 23.3075\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 22.0989\n",
      "Epoch  99/250 | Training... 96 | Train Loss (avg/element): 1858.5149 | Val Loss: 22.2875\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 22.0989\n",
      "Epoch  100/250 | Training... 96 | Train Loss (avg/element): 1873.0192 | Val Loss: 21.9094\n",
      "Validation RMSE improved (22.0989 --> 21.9094). Saving model...\n",
      "Epoch  101/250 | Training... 96 | Train Loss (avg/element): 1854.3834 | Val Loss: 22.1179\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.9094\n",
      "Epoch  102/250 | Training... 96 | Train Loss (avg/element): 1846.4819 | Val Loss: 22.8347\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.9094\n",
      "Epoch  103/250 | Training... 96 | Train Loss (avg/element): 1841.7648 | Val Loss: 24.3515\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 21.9094\n",
      "Epoch  104/250 | Training... 96 | Train Loss (avg/element): 1860.7496 | Val Loss: 21.5035\n",
      "Validation RMSE improved (21.9094 --> 21.5035). Saving model...\n",
      "Epoch  105/250 | Training... 96 | Train Loss (avg/element): 1886.3663 | Val Loss: 22.5383\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.5035\n",
      "Epoch  106/250 | Training... 96 | Train Loss (avg/element): 1845.9572 | Val Loss: 21.2349\n",
      "Validation RMSE improved (21.5035 --> 21.2349). Saving model...\n",
      "Epoch  107/250 | Training... 96 | Train Loss (avg/element): 1812.6658 | Val Loss: 23.0533\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.2349\n",
      "Epoch  108/250 | Training... 96 | Train Loss (avg/element): 1844.1461 | Val Loss: 22.5477\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.2349\n",
      "Epoch  109/250 | Training... 96 | Train Loss (avg/element): 1834.9177 | Val Loss: 21.4544\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 21.2349\n",
      "Epoch  110/250 | Training... 96 | Train Loss (avg/element): 1871.4612 | Val Loss: 22.4351\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 21.2349\n",
      "Epoch  111/250 | Training... 96 | Train Loss (avg/element): 1811.8854 | Val Loss: 22.3286\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 21.2349\n",
      "Epoch  112/250 | Training... 96 | Train Loss (avg/element): 1825.0042 | Val Loss: 22.0942\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 21.2349\n",
      "Epoch  113/250 | Training... 96 | Train Loss (avg/element): 1832.5818 | Val Loss: 21.7579\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 21.2349\n",
      "Epoch  114/250 | Training... 96 | Train Loss (avg/element): 1875.3459 | Val Loss: 22.0363\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 21.2349\n",
      "Epoch  115/250 | Training... 96 | Train Loss (avg/element): 1817.1453 | Val Loss: 20.3164\n",
      "Validation RMSE improved (21.2349 --> 20.3164). Saving model...\n",
      "Epoch  116/250 | Training... 96 | Train Loss (avg/element): 1834.3978 | Val Loss: 22.9457\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.3164\n",
      "Epoch  117/250 | Training... 96 | Train Loss (avg/element): 1797.2344 | Val Loss: 21.6335\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.3164\n",
      "Epoch  118/250 | Training... 96 | Train Loss (avg/element): 1826.2210 | Val Loss: 22.3443\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 20.3164\n",
      "Epoch  119/250 | Training... 96 | Train Loss (avg/element): 1867.9319 | Val Loss: 20.7934\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 20.3164\n",
      "Epoch  120/250 | Training... 96 | Train Loss (avg/element): 1880.2047 | Val Loss: 21.5712\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 20.3164\n",
      "Epoch  121/250 | Training... 96 | Train Loss (avg/element): 1827.6497 | Val Loss: 22.8294\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 20.3164\n",
      "Epoch  122/250 | Training... 96 | Train Loss (avg/element): 1812.4769 | Val Loss: 22.0057\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 20.3164\n",
      "Epoch  123/250 | Training... 96 | Train Loss (avg/element): 1816.9040 | Val Loss: 22.0201\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 20.3164\n",
      "Epoch  124/250 | Training... 96 | Train Loss (avg/element): 1858.2718 | Val Loss: 22.5739\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 20.3164\n",
      "Epoch  125/250 | Training... 96 | Train Loss (avg/element): 1804.2087 | Val Loss: 21.4708\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 20.3164\n",
      "\n",
      "--- Early stopping triggered after 125 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 125. Best Validation RMSE: 20.3164. Validation RMSE (Last Step): 7.5350.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9726.6557 | Val Loss: 99.2541\n",
      "Validation RMSE improved (inf --> 99.2541). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9428.6518 | Val Loss: 96.0480\n",
      "Validation RMSE improved (99.2541 --> 96.0480). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 8758.2026 | Val Loss: 91.9074\n",
      "Validation RMSE improved (96.0480 --> 91.9074). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 7973.5069 | Val Loss: 87.3800\n",
      "Validation RMSE improved (91.9074 --> 87.3800). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 7202.4384 | Val Loss: 82.6231\n",
      "Validation RMSE improved (87.3800 --> 82.6231). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 6447.9588 | Val Loss: 77.7719\n",
      "Validation RMSE improved (82.6231 --> 77.7719). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 5734.9163 | Val Loss: 72.9756\n",
      "Validation RMSE improved (77.7719 --> 72.9756). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 5100.4284 | Val Loss: 68.3710\n",
      "Validation RMSE improved (72.9756 --> 68.3710). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 4530.5344 | Val Loss: 63.9367\n",
      "Validation RMSE improved (68.3710 --> 63.9367). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 4021.6109 | Val Loss: 59.4904\n",
      "Validation RMSE improved (63.9367 --> 59.4904). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 3463.8068 | Val Loss: 54.4935\n",
      "Validation RMSE improved (59.4904 --> 54.4935). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 3009.3441 | Val Loss: 50.1514\n",
      "Validation RMSE improved (54.4935 --> 50.1514). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 2669.1834 | Val Loss: 46.1733\n",
      "Validation RMSE improved (50.1514 --> 46.1733). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 2321.8450 | Val Loss: 42.6081\n",
      "Validation RMSE improved (46.1733 --> 42.6081). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 2081.9331 | Val Loss: 40.0209\n",
      "Validation RMSE improved (42.6081 --> 40.0209). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 1933.5116 | Val Loss: 36.0688\n",
      "Validation RMSE improved (40.0209 --> 36.0688). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 1804.9090 | Val Loss: 34.0436\n",
      "Validation RMSE improved (36.0688 --> 34.0436). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 1662.5745 | Val Loss: 31.6932\n",
      "Validation RMSE improved (34.0436 --> 31.6932). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 1530.2899 | Val Loss: 29.3629\n",
      "Validation RMSE improved (31.6932 --> 29.3629). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 1477.8439 | Val Loss: 29.9834\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 29.3629\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 1408.2604 | Val Loss: 28.2687\n",
      "Validation RMSE improved (29.3629 --> 28.2687). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 1395.8038 | Val Loss: 25.8679\n",
      "Validation RMSE improved (28.2687 --> 25.8679). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 1318.2442 | Val Loss: 25.6146\n",
      "Validation RMSE improved (25.8679 --> 25.6146). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 1287.4420 | Val Loss: 23.5024\n",
      "Validation RMSE improved (25.6146 --> 23.5024). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 1323.0644 | Val Loss: 23.2395\n",
      "Validation RMSE improved (23.5024 --> 23.2395). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 1262.2845 | Val Loss: 21.8545\n",
      "Validation RMSE improved (23.2395 --> 21.8545). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 1249.9912 | Val Loss: 23.4575\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.8545\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 1195.9452 | Val Loss: 22.8124\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.8545\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 1261.0368 | Val Loss: 19.1241\n",
      "Validation RMSE improved (21.8545 --> 19.1241). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 1240.8815 | Val Loss: 20.5265\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.1241\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 1191.1260 | Val Loss: 20.8465\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.1241\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 1183.5721 | Val Loss: 19.2214\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.1241\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 1198.2073 | Val Loss: 20.0955\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 19.1241\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 1154.9026 | Val Loss: 22.4060\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 19.1241\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 1172.3060 | Val Loss: 20.4061\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 19.1241\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 1174.5176 | Val Loss: 22.1697\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 19.1241\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 1206.7826 | Val Loss: 19.3984\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 19.1241\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1171.0614 | Val Loss: 18.4735\n",
      "Validation RMSE improved (19.1241 --> 18.4735). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1127.5745 | Val Loss: 21.1888\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.4735\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1162.5453 | Val Loss: 18.0276\n",
      "Validation RMSE improved (18.4735 --> 18.0276). Saving model...\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1129.0351 | Val Loss: 20.4850\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.0276\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1135.8694 | Val Loss: 19.0428\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 18.0276\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1157.1825 | Val Loss: 19.8216\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 18.0276\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1122.0714 | Val Loss: 19.6043\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 18.0276\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1137.4332 | Val Loss: 18.6166\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 18.0276\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1092.7187 | Val Loss: 18.4227\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 18.0276\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1124.9146 | Val Loss: 17.7377\n",
      "Validation RMSE improved (18.0276 --> 17.7377). Saving model...\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1106.9273 | Val Loss: 19.2992\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.7377\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1118.7261 | Val Loss: 16.6797\n",
      "Validation RMSE improved (17.7377 --> 16.6797). Saving model...\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1130.7974 | Val Loss: 17.5852\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 16.6797\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1123.1690 | Val Loss: 19.8173\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 16.6797\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1128.0241 | Val Loss: 18.0746\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 16.6797\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1108.5425 | Val Loss: 17.8924\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 16.6797\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1083.8299 | Val Loss: 19.7979\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 16.6797\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1091.8761 | Val Loss: 19.2122\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 16.6797\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1104.1893 | Val Loss: 18.2634\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 16.6797\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1099.4117 | Val Loss: 19.3939\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 16.6797\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1099.3393 | Val Loss: 19.1387\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 16.6797\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1089.8524 | Val Loss: 23.1443\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 16.6797\n",
      "\n",
      "--- Early stopping triggered after 59 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 59. Best Validation RMSE: 16.6797. Validation RMSE (Last Step): 5.1731.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9753.6415 | Val Loss: 97.3229\n",
      "Validation RMSE improved (inf --> 97.3229). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9511.2611 | Val Loss: 95.4994\n",
      "Validation RMSE improved (97.3229 --> 95.4994). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 9122.4522 | Val Loss: 93.0026\n",
      "Validation RMSE improved (95.4994 --> 93.0026). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 8619.2800 | Val Loss: 90.0092\n",
      "Validation RMSE improved (93.0026 --> 90.0092). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 8072.4087 | Val Loss: 86.7715\n",
      "Validation RMSE improved (90.0092 --> 86.7715). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 7502.6053 | Val Loss: 83.3493\n",
      "Validation RMSE improved (86.7715 --> 83.3493). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 6940.8596 | Val Loss: 79.8775\n",
      "Validation RMSE improved (83.3493 --> 79.8775). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 6421.7408 | Val Loss: 76.3957\n",
      "Validation RMSE improved (79.8775 --> 76.3957). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 5904.7652 | Val Loss: 72.9407\n",
      "Validation RMSE improved (76.3957 --> 72.9407). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 5417.9856 | Val Loss: 69.5735\n",
      "Validation RMSE improved (72.9407 --> 69.5735). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 5045.9943 | Val Loss: 66.3480\n",
      "Validation RMSE improved (69.5735 --> 66.3480). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 4696.7657 | Val Loss: 63.1074\n",
      "Validation RMSE improved (66.3480 --> 63.1074). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 4296.8666 | Val Loss: 59.4953\n",
      "Validation RMSE improved (63.1074 --> 59.4953). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 3904.6769 | Val Loss: 56.1282\n",
      "Validation RMSE improved (59.4953 --> 56.1282). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 3620.9443 | Val Loss: 52.9599\n",
      "Validation RMSE improved (56.1282 --> 52.9599). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 3333.0247 | Val Loss: 49.8634\n",
      "Validation RMSE improved (52.9599 --> 49.8634). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 3129.5160 | Val Loss: 47.1345\n",
      "Validation RMSE improved (49.8634 --> 47.1345). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 2910.3143 | Val Loss: 44.3408\n",
      "Validation RMSE improved (47.1345 --> 44.3408). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 2664.0112 | Val Loss: 41.8255\n",
      "Validation RMSE improved (44.3408 --> 41.8255). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 2603.2945 | Val Loss: 39.7564\n",
      "Validation RMSE improved (41.8255 --> 39.7564). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 2489.1911 | Val Loss: 37.7403\n",
      "Validation RMSE improved (39.7564 --> 37.7403). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 2377.0388 | Val Loss: 35.9221\n",
      "Validation RMSE improved (37.7403 --> 35.9221). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 2336.5581 | Val Loss: 34.2994\n",
      "Validation RMSE improved (35.9221 --> 34.2994). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 2219.1326 | Val Loss: 32.7179\n",
      "Validation RMSE improved (34.2994 --> 32.7179). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 2172.9619 | Val Loss: 31.4224\n",
      "Validation RMSE improved (32.7179 --> 31.4224). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 2117.5027 | Val Loss: 30.7730\n",
      "Validation RMSE improved (31.4224 --> 30.7730). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 2098.1966 | Val Loss: 28.9974\n",
      "Validation RMSE improved (30.7730 --> 28.9974). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 2027.8655 | Val Loss: 28.2265\n",
      "Validation RMSE improved (28.9974 --> 28.2265). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 2053.0226 | Val Loss: 27.6647\n",
      "Validation RMSE improved (28.2265 --> 27.6647). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 2000.3430 | Val Loss: 26.7100\n",
      "Validation RMSE improved (27.6647 --> 26.7100). Saving model...\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 2050.7402 | Val Loss: 25.5253\n",
      "Validation RMSE improved (26.7100 --> 25.5253). Saving model...\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 1980.0175 | Val Loss: 25.0337\n",
      "Validation RMSE improved (25.5253 --> 25.0337). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 2017.5450 | Val Loss: 24.8484\n",
      "Validation RMSE improved (25.0337 --> 24.8484). Saving model...\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 1952.0851 | Val Loss: 24.6196\n",
      "Validation RMSE improved (24.8484 --> 24.6196). Saving model...\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 1908.5066 | Val Loss: 23.8979\n",
      "Validation RMSE improved (24.6196 --> 23.8979). Saving model...\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 1992.7669 | Val Loss: 24.1852\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 23.8979\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 1937.8028 | Val Loss: 23.3074\n",
      "Validation RMSE improved (23.8979 --> 23.3074). Saving model...\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 1913.4587 | Val Loss: 23.2932\n",
      "Validation RMSE improved (23.3074 --> 23.2932). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 1873.1490 | Val Loss: 22.3505\n",
      "Validation RMSE improved (23.2932 --> 22.3505). Saving model...\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 1890.8505 | Val Loss: 23.1525\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 22.3505\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 1915.3128 | Val Loss: 22.3489\n",
      "Validation RMSE improved (22.3505 --> 22.3489). Saving model...\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 1863.1157 | Val Loss: 22.5999\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 22.3489\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 1900.9041 | Val Loss: 23.5852\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 22.3489\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1884.7179 | Val Loss: 25.9652\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 22.3489\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1932.4481 | Val Loss: 22.6228\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 22.3489\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1930.6931 | Val Loss: 22.6966\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 22.3489\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1860.1907 | Val Loss: 21.0964\n",
      "Validation RMSE improved (22.3489 --> 21.0964). Saving model...\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1934.8547 | Val Loss: 21.3810\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.0964\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1909.5530 | Val Loss: 21.4693\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.0964\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1839.4369 | Val Loss: 21.5999\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 21.0964\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1828.5234 | Val Loss: 22.3748\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 21.0964\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1866.7708 | Val Loss: 21.3177\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 21.0964\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1919.0128 | Val Loss: 22.1477\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 21.0964\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1894.8979 | Val Loss: 21.7580\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 21.0964\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1883.6477 | Val Loss: 21.1825\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 21.0964\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1869.4189 | Val Loss: 20.1752\n",
      "Validation RMSE improved (21.0964 --> 20.1752). Saving model...\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1839.7323 | Val Loss: 22.1070\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.1752\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1849.2271 | Val Loss: 21.9710\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.1752\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1892.8603 | Val Loss: 22.8012\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 20.1752\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 1851.3839 | Val Loss: 21.1992\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 20.1752\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 1860.4574 | Val Loss: 21.6016\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 20.1752\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 1928.8406 | Val Loss: 20.9104\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 20.1752\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 1891.4832 | Val Loss: 20.0963\n",
      "Validation RMSE improved (20.1752 --> 20.0963). Saving model...\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 1868.4896 | Val Loss: 20.5286\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.0963\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 1878.2952 | Val Loss: 21.1945\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.0963\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 1841.0503 | Val Loss: 21.7529\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 20.0963\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 1843.9322 | Val Loss: 21.7479\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 20.0963\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1821.8157 | Val Loss: 22.2572\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 20.0963\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 1834.0672 | Val Loss: 21.6938\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 20.0963\n",
      "Epoch  70/250 | Training... 96 | Train Loss (avg/element): 1939.0852 | Val Loss: 20.2476\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 20.0963\n",
      "Epoch  71/250 | Training... 96 | Train Loss (avg/element): 1822.7777 | Val Loss: 20.8417\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 20.0963\n",
      "Epoch  72/250 | Training... 96 | Train Loss (avg/element): 1790.2806 | Val Loss: 21.5156\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 20.0963\n",
      "Epoch  73/250 | Training... 96 | Train Loss (avg/element): 1796.4096 | Val Loss: 21.7691\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 20.0963\n",
      "\n",
      "--- Early stopping triggered after 73 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished after epoch 73. Best Validation RMSE: 20.0963. Validation RMSE (Last Step): 4.3183.\n",
      "Using device: mps\n",
      "Epoch   1/250 | Training... 96 | Train Loss (avg/element): 9791.5385 | Val Loss: 98.6718\n",
      "Validation RMSE improved (inf --> 98.6718). Saving model...\n",
      "Epoch   2/250 | Training... 96 | Train Loss (avg/element): 9726.5586 | Val Loss: 98.0763\n",
      "Validation RMSE improved (98.6718 --> 98.0763). Saving model...\n",
      "Epoch   3/250 | Training... 96 | Train Loss (avg/element): 9587.0262 | Val Loss: 97.1381\n",
      "Validation RMSE improved (98.0763 --> 97.1381). Saving model...\n",
      "Epoch   4/250 | Training... 96 | Train Loss (avg/element): 9393.9444 | Val Loss: 96.0732\n",
      "Validation RMSE improved (97.1381 --> 96.0732). Saving model...\n",
      "Epoch   5/250 | Training... 96 | Train Loss (avg/element): 9182.9633 | Val Loss: 94.9105\n",
      "Validation RMSE improved (96.0732 --> 94.9105). Saving model...\n",
      "Epoch   6/250 | Training... 96 | Train Loss (avg/element): 8954.7906 | Val Loss: 93.6524\n",
      "Validation RMSE improved (94.9105 --> 93.6524). Saving model...\n",
      "Epoch   7/250 | Training... 96 | Train Loss (avg/element): 8716.5276 | Val Loss: 92.3163\n",
      "Validation RMSE improved (93.6524 --> 92.3163). Saving model...\n",
      "Epoch   8/250 | Training... 96 | Train Loss (avg/element): 8468.2253 | Val Loss: 90.9245\n",
      "Validation RMSE improved (92.3163 --> 90.9245). Saving model...\n",
      "Epoch   9/250 | Training... 96 | Train Loss (avg/element): 8211.4536 | Val Loss: 89.4808\n",
      "Validation RMSE improved (90.9245 --> 89.4808). Saving model...\n",
      "Epoch  10/250 | Training... 96 | Train Loss (avg/element): 7971.5859 | Val Loss: 88.0170\n",
      "Validation RMSE improved (89.4808 --> 88.0170). Saving model...\n",
      "Epoch  11/250 | Training... 96 | Train Loss (avg/element): 7698.4316 | Val Loss: 86.4622\n",
      "Validation RMSE improved (88.0170 --> 86.4622). Saving model...\n",
      "Epoch  12/250 | Training... 96 | Train Loss (avg/element): 7430.7117 | Val Loss: 84.8839\n",
      "Validation RMSE improved (86.4622 --> 84.8839). Saving model...\n",
      "Epoch  13/250 | Training... 96 | Train Loss (avg/element): 7171.3625 | Val Loss: 83.2843\n",
      "Validation RMSE improved (84.8839 --> 83.2843). Saving model...\n",
      "Epoch  14/250 | Training... 96 | Train Loss (avg/element): 6907.3903 | Val Loss: 81.6597\n",
      "Validation RMSE improved (83.2843 --> 81.6597). Saving model...\n",
      "Epoch  15/250 | Training... 96 | Train Loss (avg/element): 6677.2404 | Val Loss: 80.0306\n",
      "Validation RMSE improved (81.6597 --> 80.0306). Saving model...\n",
      "Epoch  16/250 | Training... 96 | Train Loss (avg/element): 6409.7480 | Val Loss: 78.3844\n",
      "Validation RMSE improved (80.0306 --> 78.3844). Saving model...\n",
      "Epoch  17/250 | Training... 96 | Train Loss (avg/element): 6152.0892 | Val Loss: 76.7252\n",
      "Validation RMSE improved (78.3844 --> 76.7252). Saving model...\n",
      "Epoch  18/250 | Training... 96 | Train Loss (avg/element): 5937.4028 | Val Loss: 75.0826\n",
      "Validation RMSE improved (76.7252 --> 75.0826). Saving model...\n",
      "Epoch  19/250 | Training... 96 | Train Loss (avg/element): 5684.9056 | Val Loss: 73.4428\n",
      "Validation RMSE improved (75.0826 --> 73.4428). Saving model...\n",
      "Epoch  20/250 | Training... 96 | Train Loss (avg/element): 5443.1352 | Val Loss: 71.8047\n",
      "Validation RMSE improved (73.4428 --> 71.8047). Saving model...\n",
      "Epoch  21/250 | Training... 96 | Train Loss (avg/element): 5237.3407 | Val Loss: 70.1369\n",
      "Validation RMSE improved (71.8047 --> 70.1369). Saving model...\n",
      "Epoch  22/250 | Training... 96 | Train Loss (avg/element): 5015.6198 | Val Loss: 68.4040\n",
      "Validation RMSE improved (70.1369 --> 68.4040). Saving model...\n",
      "Epoch  23/250 | Training... 96 | Train Loss (avg/element): 4786.1808 | Val Loss: 66.7506\n",
      "Validation RMSE improved (68.4040 --> 66.7506). Saving model...\n",
      "Epoch  24/250 | Training... 96 | Train Loss (avg/element): 4573.0921 | Val Loss: 64.9923\n",
      "Validation RMSE improved (66.7506 --> 64.9923). Saving model...\n",
      "Epoch  25/250 | Training... 96 | Train Loss (avg/element): 4348.4741 | Val Loss: 63.0118\n",
      "Validation RMSE improved (64.9923 --> 63.0118). Saving model...\n",
      "Epoch  26/250 | Training... 96 | Train Loss (avg/element): 4157.0940 | Val Loss: 61.2444\n",
      "Validation RMSE improved (63.0118 --> 61.2444). Saving model...\n",
      "Epoch  27/250 | Training... 96 | Train Loss (avg/element): 3936.1077 | Val Loss: 59.5752\n",
      "Validation RMSE improved (61.2444 --> 59.5752). Saving model...\n",
      "Epoch  28/250 | Training... 96 | Train Loss (avg/element): 3786.0362 | Val Loss: 57.9176\n",
      "Validation RMSE improved (59.5752 --> 57.9176). Saving model...\n",
      "Epoch  29/250 | Training... 96 | Train Loss (avg/element): 3603.4835 | Val Loss: 56.2346\n",
      "Validation RMSE improved (57.9176 --> 56.2346). Saving model...\n",
      "Epoch  30/250 | Training... 96 | Train Loss (avg/element): 3442.3048 | Val Loss: 54.7391\n",
      "Validation RMSE improved (56.2346 --> 54.7391). Saving model...\n",
      "Epoch  31/250 | Training... 96 | Train Loss (avg/element): 3296.5246 | Val Loss: 53.1238\n",
      "Validation RMSE improved (54.7391 --> 53.1238). Saving model...\n",
      "Epoch  32/250 | Training... 96 | Train Loss (avg/element): 3162.4511 | Val Loss: 51.5235\n",
      "Validation RMSE improved (53.1238 --> 51.5235). Saving model...\n",
      "Epoch  33/250 | Training... 96 | Train Loss (avg/element): 2989.5988 | Val Loss: 49.8446\n",
      "Validation RMSE improved (51.5235 --> 49.8446). Saving model...\n",
      "Epoch  34/250 | Training... 96 | Train Loss (avg/element): 2854.6662 | Val Loss: 48.4236\n",
      "Validation RMSE improved (49.8446 --> 48.4236). Saving model...\n",
      "Epoch  35/250 | Training... 96 | Train Loss (avg/element): 2746.9177 | Val Loss: 46.8078\n",
      "Validation RMSE improved (48.4236 --> 46.8078). Saving model...\n",
      "Epoch  36/250 | Training... 96 | Train Loss (avg/element): 2648.3413 | Val Loss: 45.8398\n",
      "Validation RMSE improved (46.8078 --> 45.8398). Saving model...\n",
      "Epoch  37/250 | Training... 96 | Train Loss (avg/element): 2523.8102 | Val Loss: 44.0357\n",
      "Validation RMSE improved (45.8398 --> 44.0357). Saving model...\n",
      "Epoch  38/250 | Training... 96 | Train Loss (avg/element): 2401.4881 | Val Loss: 42.6095\n",
      "Validation RMSE improved (44.0357 --> 42.6095). Saving model...\n",
      "Epoch  39/250 | Training... 96 | Train Loss (avg/element): 2348.1058 | Val Loss: 41.7701\n",
      "Validation RMSE improved (42.6095 --> 41.7701). Saving model...\n",
      "Epoch  40/250 | Training... 96 | Train Loss (avg/element): 2236.9759 | Val Loss: 40.1180\n",
      "Validation RMSE improved (41.7701 --> 40.1180). Saving model...\n",
      "Epoch  41/250 | Training... 96 | Train Loss (avg/element): 2173.7710 | Val Loss: 38.6395\n",
      "Validation RMSE improved (40.1180 --> 38.6395). Saving model...\n",
      "Epoch  42/250 | Training... 96 | Train Loss (avg/element): 2098.5626 | Val Loss: 40.3265\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 38.6395\n",
      "Epoch  43/250 | Training... 96 | Train Loss (avg/element): 2084.7008 | Val Loss: 36.7478\n",
      "Validation RMSE improved (38.6395 --> 36.7478). Saving model...\n",
      "Epoch  44/250 | Training... 96 | Train Loss (avg/element): 1980.5523 | Val Loss: 35.6883\n",
      "Validation RMSE improved (36.7478 --> 35.6883). Saving model...\n",
      "Epoch  45/250 | Training... 96 | Train Loss (avg/element): 1932.5607 | Val Loss: 34.5340\n",
      "Validation RMSE improved (35.6883 --> 34.5340). Saving model...\n",
      "Epoch  46/250 | Training... 96 | Train Loss (avg/element): 1853.8516 | Val Loss: 33.6886\n",
      "Validation RMSE improved (34.5340 --> 33.6886). Saving model...\n",
      "Epoch  47/250 | Training... 96 | Train Loss (avg/element): 1789.8448 | Val Loss: 32.7893\n",
      "Validation RMSE improved (33.6886 --> 32.7893). Saving model...\n",
      "Epoch  48/250 | Training... 96 | Train Loss (avg/element): 1785.9738 | Val Loss: 31.5991\n",
      "Validation RMSE improved (32.7893 --> 31.5991). Saving model...\n",
      "Epoch  49/250 | Training... 96 | Train Loss (avg/element): 1769.3202 | Val Loss: 30.6632\n",
      "Validation RMSE improved (31.5991 --> 30.6632). Saving model...\n",
      "Epoch  50/250 | Training... 96 | Train Loss (avg/element): 1715.2147 | Val Loss: 30.0787\n",
      "Validation RMSE improved (30.6632 --> 30.0787). Saving model...\n",
      "Epoch  51/250 | Training... 96 | Train Loss (avg/element): 1646.4657 | Val Loss: 29.3406\n",
      "Validation RMSE improved (30.0787 --> 29.3406). Saving model...\n",
      "Epoch  52/250 | Training... 96 | Train Loss (avg/element): 1645.7709 | Val Loss: 28.9456\n",
      "Validation RMSE improved (29.3406 --> 28.9456). Saving model...\n",
      "Epoch  53/250 | Training... 96 | Train Loss (avg/element): 1647.1818 | Val Loss: 28.3692\n",
      "Validation RMSE improved (28.9456 --> 28.3692). Saving model...\n",
      "Epoch  54/250 | Training... 96 | Train Loss (avg/element): 1605.2438 | Val Loss: 27.7242\n",
      "Validation RMSE improved (28.3692 --> 27.7242). Saving model...\n",
      "Epoch  55/250 | Training... 96 | Train Loss (avg/element): 1599.0014 | Val Loss: 27.3294\n",
      "Validation RMSE improved (27.7242 --> 27.3294). Saving model...\n",
      "Epoch  56/250 | Training... 96 | Train Loss (avg/element): 1577.6021 | Val Loss: 26.5955\n",
      "Validation RMSE improved (27.3294 --> 26.5955). Saving model...\n",
      "Epoch  57/250 | Training... 96 | Train Loss (avg/element): 1544.0871 | Val Loss: 25.7580\n",
      "Validation RMSE improved (26.5955 --> 25.7580). Saving model...\n",
      "Epoch  58/250 | Training... 96 | Train Loss (avg/element): 1553.9079 | Val Loss: 24.9352\n",
      "Validation RMSE improved (25.7580 --> 24.9352). Saving model...\n",
      "Epoch  59/250 | Training... 96 | Train Loss (avg/element): 1500.9278 | Val Loss: 24.8752\n",
      "Validation RMSE improved (24.9352 --> 24.8752). Saving model...\n",
      "Epoch  60/250 | Training... 96 | Train Loss (avg/element): 1527.4956 | Val Loss: 24.8694\n",
      "Validation RMSE improved (24.8752 --> 24.8694). Saving model...\n",
      "Epoch  61/250 | Training... 96 | Train Loss (avg/element): 1516.4043 | Val Loss: 23.8784\n",
      "Validation RMSE improved (24.8694 --> 23.8784). Saving model...\n",
      "Epoch  62/250 | Training... 96 | Train Loss (avg/element): 1484.3663 | Val Loss: 24.4569\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 23.8784\n",
      "Epoch  63/250 | Training... 96 | Train Loss (avg/element): 1528.6135 | Val Loss: 22.9110\n",
      "Validation RMSE improved (23.8784 --> 22.9110). Saving model...\n",
      "Epoch  64/250 | Training... 96 | Train Loss (avg/element): 1486.4575 | Val Loss: 22.7850\n",
      "Validation RMSE improved (22.9110 --> 22.7850). Saving model...\n",
      "Epoch  65/250 | Training... 96 | Train Loss (avg/element): 1510.2702 | Val Loss: 21.9302\n",
      "Validation RMSE improved (22.7850 --> 21.9302). Saving model...\n",
      "Epoch  66/250 | Training... 96 | Train Loss (avg/element): 1485.6377 | Val Loss: 21.8688\n",
      "Validation RMSE improved (21.9302 --> 21.8688). Saving model...\n",
      "Epoch  67/250 | Training... 96 | Train Loss (avg/element): 1467.0137 | Val Loss: 21.9673\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 21.8688\n",
      "Epoch  68/250 | Training... 96 | Train Loss (avg/element): 1447.6159 | Val Loss: 28.9529\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 21.8688\n",
      "Epoch  69/250 | Training... 96 | Train Loss (avg/element): 1465.0309 | Val Loss: 20.8992\n",
      "Validation RMSE improved (21.8688 --> 20.8992). Saving model...\n",
      "Epoch  70/250 | Training... 96 | Train Loss (avg/element): 1514.2751 | Val Loss: 20.7219\n",
      "Validation RMSE improved (20.8992 --> 20.7219). Saving model...\n",
      "Epoch  71/250 | Training... 96 | Train Loss (avg/element): 1472.6088 | Val Loss: 20.6149\n",
      "Validation RMSE improved (20.7219 --> 20.6149). Saving model...\n",
      "Epoch  72/250 | Training... 96 | Train Loss (avg/element): 1426.6055 | Val Loss: 20.7121\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.6149\n",
      "Epoch  73/250 | Training... 96 | Train Loss (avg/element): 1421.4462 | Val Loss: 21.1008\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 20.6149\n",
      "Epoch  74/250 | Training... 96 | Train Loss (avg/element): 1450.0317 | Val Loss: 20.6281\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 20.6149\n",
      "Epoch  75/250 | Training... 96 | Train Loss (avg/element): 1450.7359 | Val Loss: 20.0843\n",
      "Validation RMSE improved (20.6149 --> 20.0843). Saving model...\n",
      "Epoch  76/250 | Training... 96 | Train Loss (avg/element): 1472.4132 | Val Loss: 20.2512\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 20.0843\n",
      "Epoch  77/250 | Training... 96 | Train Loss (avg/element): 1410.8300 | Val Loss: 19.7985\n",
      "Validation RMSE improved (20.0843 --> 19.7985). Saving model...\n",
      "Epoch  78/250 | Training... 96 | Train Loss (avg/element): 1433.3407 | Val Loss: 20.3502\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.7985\n",
      "Epoch  79/250 | Training... 96 | Train Loss (avg/element): 1395.6351 | Val Loss: 19.9075\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.7985\n",
      "Epoch  80/250 | Training... 96 | Train Loss (avg/element): 1411.4396 | Val Loss: 19.7991\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.7985\n",
      "Epoch  81/250 | Training... 96 | Train Loss (avg/element): 1410.5545 | Val Loss: 19.3200\n",
      "Validation RMSE improved (19.7985 --> 19.3200). Saving model...\n",
      "Epoch  82/250 | Training... 96 | Train Loss (avg/element): 1422.6870 | Val Loss: 19.1643\n",
      "Validation RMSE improved (19.3200 --> 19.1643). Saving model...\n",
      "Epoch  83/250 | Training... 96 | Train Loss (avg/element): 1419.9646 | Val Loss: 19.3594\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 19.1643\n",
      "Epoch  84/250 | Training... 96 | Train Loss (avg/element): 1409.0317 | Val Loss: 19.2562\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 19.1643\n",
      "Epoch  85/250 | Training... 96 | Train Loss (avg/element): 1429.5407 | Val Loss: 19.2344\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 19.1643\n",
      "Epoch  86/250 | Training... 96 | Train Loss (avg/element): 1392.3037 | Val Loss: 18.8787\n",
      "Validation RMSE improved (19.1643 --> 18.8787). Saving model...\n",
      "Epoch  87/250 | Training... 96 | Train Loss (avg/element): 1382.7424 | Val Loss: 19.3362\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.8787\n",
      "Epoch  88/250 | Training... 96 | Train Loss (avg/element): 1392.5165 | Val Loss: 18.2532\n",
      "Validation RMSE improved (18.8787 --> 18.2532). Saving model...\n",
      "Epoch  89/250 | Training... 96 | Train Loss (avg/element): 1382.8852 | Val Loss: 18.8005\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.2532\n",
      "Epoch  90/250 | Training... 96 | Train Loss (avg/element): 1399.9808 | Val Loss: 18.2676\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 18.2532\n",
      "Epoch  91/250 | Training... 96 | Train Loss (avg/element): 1362.5545 | Val Loss: 18.2299\n",
      "Validation RMSE improved (18.2532 --> 18.2299). Saving model...\n",
      "Epoch  92/250 | Training... 96 | Train Loss (avg/element): 1363.9360 | Val Loss: 19.2945\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 18.2299\n",
      "Epoch  93/250 | Training... 96 | Train Loss (avg/element): 1368.8270 | Val Loss: 18.6646\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 18.2299\n",
      "Epoch  94/250 | Training... 96 | Train Loss (avg/element): 1379.5328 | Val Loss: 18.9946\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 18.2299\n",
      "Epoch  95/250 | Training... 96 | Train Loss (avg/element): 1395.9308 | Val Loss: 18.9568\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 18.2299\n",
      "Epoch  96/250 | Training... 96 | Train Loss (avg/element): 1385.2256 | Val Loss: 18.8482\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 18.2299\n",
      "Epoch  97/250 | Training... 96 | Train Loss (avg/element): 1397.9345 | Val Loss: 17.7798\n",
      "Validation RMSE improved (18.2299 --> 17.7798). Saving model...\n",
      "Epoch  98/250 | Training... 96 | Train Loss (avg/element): 1389.7970 | Val Loss: 19.0329\n",
      "Validation RMSE did not improve for 1 epoch(s). Best: 17.7798\n",
      "Epoch  99/250 | Training... 96 | Train Loss (avg/element): 1372.4289 | Val Loss: 18.5063\n",
      "Validation RMSE did not improve for 2 epoch(s). Best: 17.7798\n",
      "Epoch  100/250 | Training... 96 | Train Loss (avg/element): 1373.9097 | Val Loss: 17.8655\n",
      "Validation RMSE did not improve for 3 epoch(s). Best: 17.7798\n",
      "Epoch  101/250 | Training... 96 | Train Loss (avg/element): 1407.6612 | Val Loss: 17.9398\n",
      "Validation RMSE did not improve for 4 epoch(s). Best: 17.7798\n",
      "Epoch  102/250 | Training... 96 | Train Loss (avg/element): 1381.4327 | Val Loss: 19.7639\n",
      "Validation RMSE did not improve for 5 epoch(s). Best: 17.7798\n",
      "Epoch  103/250 | Training... 96 | Train Loss (avg/element): 1362.9935 | Val Loss: 18.5758\n",
      "Validation RMSE did not improve for 6 epoch(s). Best: 17.7798\n",
      "Epoch  104/250 | Training... 96 | Train Loss (avg/element): 1356.8192 | Val Loss: 19.0923\n",
      "Validation RMSE did not improve for 7 epoch(s). Best: 17.7798\n",
      "Epoch  105/250 | Training... 96 | Train Loss (avg/element): 1375.8510 | Val Loss: 18.2587\n",
      "Validation RMSE did not improve for 8 epoch(s). Best: 17.7798\n",
      "Epoch  106/250 | Training... 96 | Train Loss (avg/element): 1377.4364 | Val Loss: 18.1107\n",
      "Validation RMSE did not improve for 9 epoch(s). Best: 17.7798\n",
      "Epoch  107/250 | Training... 96 | Train Loss (avg/element): 1349.3650 | Val Loss: 18.7161\n",
      "Validation RMSE did not improve for 10 epoch(s). Best: 17.7798\n",
      "\n",
      "--- Early stopping triggered after 107 epochs ---\n",
      "\n",
      "Training finished after epoch 107. Best Validation RMSE: 17.7798. Validation RMSE (Last Step): 5.0618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neerajsohani/Documents/python/PyTorch-LSTM-for-RUL-Prediction/nrj/train_model.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "batch_size = hparams['BatchSize']\n",
    "learning_rate = hparams['LearningRate']\n",
    "learn_states = hparams['LearnableStates']\n",
    "device = hparams['device']\n",
    "hidden_sizes = hparams['HiddenSizes']\n",
    "layer_sizes = hparams['LayerSizes']\n",
    "dropout_rate = hparams['DropoutRate']\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_rmses_post_training = []\n",
    "last_epochs = []\n",
    "\n",
    "kfold = hparams['CrossVal']\n",
    "kfold_frac = (1 / kfold)\n",
    "dataset_split = random_split(train_dataset, [kfold_frac for _ in range(kfold)])\n",
    "\n",
    "# ---- Fold Iteration for Cross Validation ----\n",
    "for i in range(kfold):\n",
    "    fold_str = hparams_str + f'__fold_{(i+1):02}'\n",
    "    # ---- Create Validation and Train DataLoaders for current fold\n",
    "    val_split = dataset_split[i]\n",
    "    train_split = ConcatDataset([dataset_split[j] for j in range(kfold) if j != i])\n",
    "    train_dataLoader = DataLoader(train_split, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn_time_step_level)\n",
    "    val_dataLoader = DataLoader(val_split, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn_time_step_level) \n",
    "    \n",
    "    # Just to use the original visualisation module\n",
    "    # ----- HARD CODED -----\n",
    "    y_test = pd.DataFrame({\"RUL\": [val_split[i][1][-1].item() for i in range(10)]})\n",
    "    \n",
    "    \n",
    "    # ---- Create Model and send it to GPU if available ----\n",
    "    if learn_states:\n",
    "        model = RUL_Model_LearnableStates(input_size=train_dataLoader.dataset[0][0].shape[1], \n",
    "                                        lstm_hidden_sizes=hidden_sizes, \n",
    "                                        lstm_layer_sizes=layer_sizes, \n",
    "                                        lstm_dropout_rate=dropout_rate, \n",
    "                                        output_dropout_rate=dropout_rate)\n",
    "    else:\n",
    "        model = RUL_Model(input_size=train_dataLoader.dataset[0][0].shape[1],\n",
    "                          lstm_hidden_sizes=hidden_sizes,\n",
    "                          lstm_layer_sizes=layer_sizes,\n",
    "                          lstm_dropout_rate=dropout_rate,\n",
    "                          output_dropout_rate=dropout_rate)\n",
    "    model.to(device)\n",
    "    \n",
    "    # ---- Create Loss Function and Optimizer Instances ----\n",
    "    loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # ---- Create SummaryWriter for TensorBoard Visualisation ----\n",
    "    writer = SummaryWriter(log_dir=f'runs/{fold_str}')\n",
    "    writer.add_graph(model, train_dataset[0][0].to(device))\n",
    "\n",
    "    # ---- Train Model for this fold and save the final metrics ----\n",
    "    final_train_loss, best_val_loss, val_rmse_post_training, last_epoch = train_model(\n",
    "        model=model,\n",
    "        train_dataLoader=train_dataLoader, # Your training DataLoader\n",
    "        test_dataLoader=val_dataLoader,   # Your validation DataLoader\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        writer=writer, # Pass your writer if using TensorBoard\n",
    "        num_epochs=250,\n",
    "        y_test=y_test,\n",
    "        patience=10,\n",
    "        best_model_path=f'./models/best_model_{fold_str}.pth', # Choose save path\n",
    "        compare_entire_seq_in_val=True\n",
    "    )\n",
    "    train_losses.append(final_train_loss)\n",
    "    val_losses.append(best_val_loss)\n",
    "    val_rmses_post_training.append(val_rmse_post_training)\n",
    "    last_epochs.append(last_epoch)\n",
    "    \n",
    "    # ---- Log Hyperparameters and final metrics for current fold's run ----\n",
    "    # TODO: need to create valid hparams dict to pass to writer\n",
    "    # writer.add_hparams(hparam_dict=hparams,\n",
    "    #                    metric_dict={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2969dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss = 1334.2834912575292\n",
      "Average Val RMSE (Entire Sequence) = 17.494064212141318\n",
      "Average Val RMSE (Last Step) = 5.061805507919086\n"
     ]
    }
   ],
   "source": [
    "avg_train_loss = np.mean(np.array(train_losses))\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "avg_val_rmse = np.mean(val_rmse_post_training)\n",
    "\n",
    "print(f\"Average Train Loss = {avg_train_loss}\")\n",
    "print(f\"Average Val RMSE (Entire Sequence) = {avg_val_loss}\")\n",
    "print(f\"Average Val RMSE (Last Step) = {avg_val_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bd7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/rhmrwqtn21nbjh5rjcghp6400000gn/T/ipykernel_48150/1368964033.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "batch_size = hparams['BatchSize']\n",
    "learning_rate = hparams['LearningRate']\n",
    "learn_states = hparams['LearnableStates']\n",
    "device = hparams['device']\n",
    "hidden_sizes = hparams['HiddenSizes']\n",
    "layer_sizes = hparams['LayerSizes']\n",
    "dropout_rate = hparams['DropoutRate']\n",
    "\n",
    "# ---- Create Model and send it to GPU if available ----\n",
    "if learn_states:\n",
    "    model = RUL_Model_LearnableStates(input_size=train_dataLoader.dataset[0][0].shape[1], \n",
    "                                    lstm_hidden_sizes=hidden_sizes, \n",
    "                                    lstm_layer_sizes=layer_sizes, \n",
    "                                    lstm_dropout_rate=dropout_rate, \n",
    "                                    output_dropout_rate=dropout_rate)\n",
    "else:\n",
    "    model = RUL_Model(input_size=train_dataLoader.dataset[0][0].shape[1],\n",
    "                        lstm_hidden_sizes=hidden_sizes,\n",
    "                        lstm_layer_sizes=layer_sizes,\n",
    "                        lstm_dropout_rate=dropout_rate,\n",
    "                        output_dropout_rate=dropout_rate)\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "test_rmse_list = []\n",
    "test_pred_list_for_all_folds = []\n",
    "\n",
    "for fold in range(1, hparams['CrossVal'] + 1):\n",
    "    model_path = './models/best_model_' + hparams_str + f'__fold_{(i):02}.pth'\n",
    "    \n",
    "    # ---- Load Best Model for the fold ----\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    test_rmse, test_pred_list = evaluate_data(model=model,\n",
    "                                              dataLoader=test_dataLoader,\n",
    "                                              loss_fn=loss_fn,\n",
    "                                              device=device,\n",
    "                                              compare_entire_seq=False)\n",
    "    test_rmse_list.append(test_rmse)\n",
    "    test_pred_list_for_all_folds.append(test_pred_list)\n",
    "\n",
    "# ---- Calculate Average RMSE and Predictions across all folds ----\n",
    "test_pred_arr = np.array(test_pred_list_for_all_folds).squeeze()\n",
    "test_rmse_arr = np.array(test_rmse_list)\n",
    "\n",
    "avg_test_preds = np.mean(test_pred_arr, axis=0)\n",
    "avg_test_rmse = np.mean(test_rmse_arr)\n",
    "\n",
    "# ---- Visualize RUL vs Sorted Unit Samples ----\n",
    "y_test = pd.DataFrame({\"RUL\": [test_dataset[i][1][-1].item() for i in range(100)]})\n",
    "\n",
    "visualize(avg_test_preds, y_test, 100, avg_test_rmse, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b1712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d29d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2504d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIhCAYAAABE54vcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA8mFJREFUeJzs3Xd4k+X6wPFvku496KDsvTeCokeG4B64EfGI+6hHxXkcP9fxqMc9jnsj7nHwuCdLRGQjILJnobQUuneT3x9P3ow2ad83TZqkvT/XlSsh82mblvfOPR6TzWazIYQQQgghhBBCN3OwFyCEEEIIIYQQ4UYCKSGEEEIIIYQwSAIpIYQQQgghhDBIAikhhBBCCCGEMEgCKSGEEEIIIYQwSAIpIYQQQgghhDBIAikhhBBCCCGEMEgCKSGEEEIIIYQwSAIpIYQQQgghhDBIAikhRLv01ltvYTKZHKeIiAg6duzItGnT2LJlS7CX16wJEyYwYcIEnx573333YTKZ/LsgnWbOnElCQoLX2xMSEpg5c2bAXv+nn35i9OjRxMfHYzKZ+Oyzzww9vnv37rrW1717d0499dQm7zNz5ky6d+/udt2hQ4eYNm0amZmZmEwmpk6damh9nmg/b+0UFRVFjx49uOGGGygqKmrx8+thMpm47777HP/Wfv927txp6Hm+/vprt+dxpfdnI4QQ/hIR7AUIIUQwvfnmm/Tv35+qqip++eUXHnzwQebPn8+ff/5JampqsJfn1QsvvODzYy+//HJOPPFEP64mPNhsNs477zz69u3L559/Tnx8PP369Qvaeu6++25uuOEGt+seeOAB5s6dyxtvvEGvXr1IS0vz2+t9++23JCcnU1paytdff80zzzzDsmXLWLJkSasH1qeccgq//vorHTt2NPS4r7/+mueff95jMDV37lySkpL8tEIhhGieBFJCiHZt8ODBjB49GlBZnvr6eu69914+++wzLrnkkiCvzruBAwf6/NjOnTvTuXNnP64mPOzbt49Dhw5x5plnctxxxwV7OfTq1avRdevXr6dXr15ceOGFfn+9UaNG0aFDBwCmTJlCYWEhc+bMYcmSJRx99NEeH1NRUUFcXJzf15KRkUFGRoZfn3PEiBF+fT4hhGiOlPYJIYQLLag6cOCA2/UrVqzg9NNPJy0tjZiYGEaMGMFHH33kdh+tXGnevHlcccUVpKenk5SUxF//+lfKy8vJy8vjvPPOIyUlhY4dO3LLLbdQW1vr9hz3338/Y8eOJS0tjaSkJEaOHMnrr7+OzWZzu1/D0r6dO3diMpl4/PHHefLJJ+nRowcJCQkcddRRLF261O2xnkr7tFK0b7/9lpEjRxIbG0v//v154403Gn2PFi9ezFFHHUVMTAydOnXi7rvv5rXXXvOpVKs5FRUV3HLLLfTo0YOYmBjS0tIYPXo077//vtv9mvv53HfffY7g8R//+Acmk8lRVuepxE57TCAzNa6vq/38fvzxRzZu3Ogow1uwYAEANTU1/Otf/6J///5ER0eTkZHBJZdcQkFBgc+vf+SRRwKwa9cuQL2nBg8ezKJFixg3bhxxcXFceumlAJSUlDh+DlFRUXTq1IlZs2ZRXl7u9pwlJSWO935CQgInnngimzdvbvTa3kr7vv32W4477jiSk5OJi4tjwIABPPzww47v1/PPPw/gVqqoPYen0r7du3czY8YMMjMziY6OZsCAATzxxBNYrVbHfYz87gghhCvJSAkhhIsdO3YA0LdvX8d18+fP58QTT2Ts2LG89NJLJCcn88EHH3D++edTUVHR6ODt8ssv56yzzuKDDz5g9erV3HnnndTV1bFp0ybOOussrrzySn788UceeeQRcnJyuOmmmxyP3blzJ1dddRVdu3YFYOnSpVx33XXk5uZyzz33NLv+559/nv79+/P0008Dqnzs5JNPZseOHSQnJzf52LVr13LzzTdz++23k5WVxWuvvcZll11G7969OfbYYwH4/fffmTJlCn379mX27NnExcXx0ksv8c477zS7Nl/cdNNNzJkzh3/961+MGDGC8vJy1q9fT2FhoeM+en4+l19+OcOGDeOss87iuuuuY/r06URHRwdkzb7o2LEjv/76K9dccw3FxcW8++67gMo8Wq1WzjjjDH7++Wduu+02xo0bx65du7j33nuZMGECK1asIDY21vBrbt26FcAtM7R//35mzJjBbbfdxkMPPYTZbKaiooLx48ezd+9e7rzzToYOHcqGDRu45557WLduHT/++CMmkwmbzcbUqVNZsmQJ99xzD0cccQS//PILJ510kq71vP7661xxxRWMHz+el156iczMTDZv3sz69esB9V4uLy/nk08+4ddff3X73nlSUFDAuHHjqKmp4YEHHqB79+58+eWX3HLLLWzbtq1ReWxLfneEEO2UTQgh2qE333zTBtiWLl1qq62ttZWWltq+/fZbW3Z2tu3YY4+11dbWOu7bv39/24gRI9yus9lstlNPPdXWsWNHW319vdtzXnfddW73mzp1qg2wPfnkk27XDx8+3DZy5Eiva6yvr7fV1tba/vnPf9rS09NtVqvVcdv48eNt48ePd/x7x44dNsA2ZMgQW11dneP6ZcuW2QDb+++/77ju3nvvtTX889+tWzdbTEyMbdeuXY7rKisrbWlpabarrrrKcd25555ri4+PtxUUFLitc+DAgTbAtmPHDq9fj81ms1188cW2+Ph4r7fHx8fbLr74Yse/Bw8ebJs6dWqTz6n356N9jx577LFGa+rWrVuj5/X2fXJdnzfdunWznXLKKU3ex9Prjh8/3jZo0CC3695//30bYPv000/drl++fLkNsL3wwgtNvo72deTl5dlqa2tthw8ftr3zzju22NhYW5cuXWyVlZWO1wZsP/30k9vjH374YZvZbLYtX77c7fpPPvnEBti+/vprm81ms33zzTc2wPbMM8+43e/BBx+0AbZ7773XcZ32u6K9X0pLS21JSUm2Y445xu193tC1117b6Geiafizuf32222A7bfffnO739VXX20zmUy2TZs22Ww2Y787QgjhSkr7hBDt2pFHHklkZCSJiYmceOKJpKam8r///Y+ICJWw37p1K3/++aejZ6Wurs5xOvnkk9m/fz+bNm1ye86G09oGDBgAqAb7htdrZVWaefPmMXnyZJKTk7FYLERGRnLPPfdQWFhIfn5+s1/PKaecgsVicfx76NChAI1ex5Phw4c7MmEAMTEx9O3b1+2xCxcuZNKkSY5eGwCz2cx5553X7PP7YsyYMXzzzTfcfvvtLFiwgMrKSrfbffn5hJsvv/ySlJQUTjvtNLevb/jw4WRnZzvK/5qTnZ1NZGQkqampzJgxg5EjR/Ltt98SExPjuE9qaiqTJk1q9PqDBw9m+PDhbq9/wgknuJUfzp8/H6BRf9f06dObXduSJUsoKSnhmmuu8Vs55bx58xg4cCBjxoxxu37mzJnYbDbmzZvndn1LfneEEO2TlPYJIdq1t99+mwEDBlBaWsqHH37Iyy+/zAUXXMA333wDOHulbrnlFm655RaPz3Hw4EG3fzectBYVFeX1+qqqKse/ly1bxvHHH8+ECRN49dVX6dy5M1FRUXz22Wc8+OCDjYIIT9LT093+rZWv+fJY7fGujy0sLCQrK6vR/Txd50lERAT19fVeb6+rqyMyMtLx72effZbOnTvz4Ycf8sgjjxATE8MJJ5zAY489Rp8+fXz6+YSbAwcOUFRU5HgfNaT36/vxxx9JTk4mMjKSzp07e/x5eyqTO3DgAFu3bnX7uXh6/cLCQiIiIho9b3Z2drNr03q9/DkEpbCw0GPvW05OjuN2Vy353RFCtE8SSAkh2rUBAwY4BkxMnDiR+vp6XnvtNT755BPOOeccR+bljjvu4KyzzvL4HP4aof3BBx8QGRnJl19+6ZYlMLrXUSClp6c3GsQBkJeXp+vxWVlZVFVVcejQoUaBZWFhIdXV1W5BWXx8PPfffz/3338/Bw4ccGSnTjvtNP7880+//HxiYmKorq5udH2oBGAdOnQgPT2db7/91uPtiYmJup5n2LBhbplETzxlgzp06EBsbKzHwSPa7aDeG3V1dRQWFroFJXreG1qf1t69e5u9r17p6ens37+/0fX79u0DaPZ7IYQQzZHSPiGEcPHoo4+SmprKPffcg9VqpV+/fvTp04e1a9cyevRojye9B7LN0TYGdi0vqqysZM6cOX55fn8YP3488+bNcwsyrFYrH3/8sa7HT548GYAPP/yw0W3alD3tPg1lZWUxc+ZMLrjgAjZt2kRFRYVffj7du3cnPz/fLUCsqanhu+++0/U1Bdqpp55KYWEh9fX1Hr++QO+Fdeqpp7Jt2zbS09M9vr6W9Zk4cSKAY1CG5r333mv2NcaNG0dycjIvvfRSowmVroxkiY477jj++OMPVq1a5Xb922+/jclkcqxXCCF8JRkpIYRwkZqayh133MFtt93Ge++9x4wZM3j55Zc56aSTOOGEE5g5cyadOnXi0KFDbNy4kVWrVukOIppzyimn8OSTTzJ9+nSuvPJKCgsLefzxx0Nqutxdd93FF198wXHHHcddd91FbGwsL730kmMMttnc9OdzEydO5PTTT+eGG25g586djB8/HpvNxqJFi3jqqac4/fTT3ca6jx07llNPPZWhQ4eSmprKxo0bmTNnDkcddZRjf6OW/nzOP/987rnnHqZNm8att95KVVUVzz77bJMliHrk5eXxySefNLq+e/fujiyoHtOmTePdd9/l5JNP5oYbbmDMmDFERkayd+9e5s+fzxlnnMGZZ57ZorU2ZdasWXz66acce+yx3HjjjQwdOhSr1cru3bv5/vvvufnmmxk7dizHH388xx57LLfddhvl5eWMHj2aX375RdcHAQkJCTzxxBNcfvnlTJ48mSuuuIKsrCy2bt3K2rVree655wAYMmQIAI888ggnnXQSFouFoUOHeix7vPHGG3n77bc55ZRT+Oc//0m3bt346quveOGFF7j66qvdJnMKIYQvJJASQogGrrvuOp577jn++c9/csEFFzBx4kSWLVvGgw8+yKxZszh8+DDp6ekMHDjQr0MWJk2axBtvvMEjjzzCaaedRqdOnbjiiivIzMzksssu89vrtMSwYcP44YcfuOWWW/jrX/9KamoqF110EePHj+cf//iHrjHRn3zyCY8//jjvvvsuzzzzDAC9e/fm/vvvb9TnNGnSJD7//HOeeuopKioq6NSpE3/961+56667HPdp6c+nR48e/O9//+POO+/knHPOoWPHjtx0000UFBRw//33G/wOOa1cuZJzzz230fUXX3wxb731lu7nsVgsfP755zzzzDPMmTOHhx9+mIiICDp37sz48eMdwUWgxMfH8/PPP/Pvf/+bV155hR07dhAbG0vXrl2ZPHmyIyNlNpv5/PPPuemmm3j00Uepqanh6KOP5uuvv6Z///7Nvs5ll11GTk4OjzzyCJdffjk2m43u3btz8cUXO+4zffp0fvnlF1544QX++c9/YrPZ2LFjh8deqIyMDJYsWcIdd9zBHXfcQUlJCT179uTRRx9123JACCF8ZbI1lUMXQgghdDj++OPZuXOnx81XhRBCiLZIMlJCCCEMuemmmxgxYgRdunTh0KFDvPvuu/zwww+8/vrrwV6aEEII0WokkBJCCGFIfX0999xzD3l5eZhMJgYOHMicOXOYMWNGsJcmhBBCtBop7RNCCCGEEEIIg2T8uRBCCCGEEEIYJIGUEEIIIYQQQhgkgZQQQgghhBBCGCTDJgCr1cq+fftITEzEZDIFezlCCCGEEEKIILHZbJSWlpKTk9PkRvMSSAH79u2jS5cuwV6GEEIIIYQQIkTs2bOHzp07e71dAikgMTERUN+spKSkIK9GCCGEEEKIdqS2Ft58U12+5BKIjAzqckpKSujSpYsjRvBGxp+jvlnJyckUFxdLICWEEEIIIURrKi+HhAR1uawM4uODuhy9sYEMmxBCCCGEEEIIgySQEkIIIYQQQgiDJJASQgghhBBCCIMkkBJCCCGEEEIIgySQEkIIIYQQQgiDJJASQgghhBBCCINkHykhhBBCCCFE8ERHw5dfOi+HCQmkhBBCCCGEEMETEQGnnBLsVRgmpX1CCCGEEEIIYZBkpIQQQgghhBDBU1sL776rLl94IURGBnc9OkkgJYQQQgghhAiemhq45BJ1+dxzwyaQktI+IYQQQgghhDBIAikhhBBCCCGEMEgCKSGEEEIIIYQwSAIpIYQQQgghhDBIAikhhBBCCCGEMEim9oWYeX8eoKbOGuxltFhiTCRH9kzHYjYFeylCCCGEEEL4nQRSIea2T37nYFlNsJfhF5MHZPH8hSOIjrAEeylCCCGEECJURUfDRx85L4cJCaRCzLDOKRRX1gZ7GS32e24xP248wJVvr+Tli0YREynBlBBCCCGE8CAiQu0fFWZMNpvNFuxFBFtJSQnJyckUFxeTlJQU7OW0Cb9sPcjls1dQWVvPuF7pvHbxaOKiJG4XQgghhBChTW9sIMMmREAc3bsDsy8dQ3yUhSXbCrn4jWWUVoV/pk0IIYQQQvhZXR18/LE61dUFezW6SSAlAmZMjzTmXD6WxJgIlu88zEWvL2sTZYtCCCGEEMKPqqvhvPPUqbo62KvRTQIpEVAju6by3uVHkhIXyZo9RVz42lIOl7eNYRpCCCGEEKL9kh4ppEeqNWzcX8KM136jsLyG/tmJnDyko8f7xUZaOHNkJzokhM/EFiGEEEII0QLl5ZCQoC6XlUF8fFCXozc2kEAKCaRay9b8Uqa/+hv5pU2nbLumxfHeFWPpnBrXSisTQgghhBBBI4FU+JJAqvXsOVTB27/upLym3uPtizYXsPdwJZ1SYnnvirF0Sw/uL5IQQgghhAgwCaTClwRSoSOvuIrpry5l+8FyspKiee+KI+mVkRDsZQkhhBBCiEAJ00BKhk2IkJKdHMMHVx1J36wEDpRUc/7LS9mUVxrsZQkhhBBCCOFGAikRcjITY/jgyqMY2DGJg2XVTHvlV9bnFgd7WUIIIYQQIhCiouDNN9UpKirYq9FNSvuQ0r5QVVRRw8VvLGPt3mKSYiKYc9lYhnVJCfayhBBCCCFEGyalfSLspcRFMefysYzqlkpJVR0XvvYbK3YeCvayhBBCCCGEkEBKhLakmEjevnQMR/ZMo6y6jr++sYwl2w4Ge1lCCCGEEMJf6urgq6/Uqa4u2KvRTQIpEfLioyN4c+YY/tKnAxU19Vzy5nIWbi4I9rKEEEIIIYQ/VFfDqaeqU3XT+42GEgmkRFiIjbLw6l9Hc1z/TKrrrFwxewU/bTwQ7GUJIYQQQoh2SgIpETZiIi28OGMUJw7KpqbeylVzVvLNuv3BXpYQQgghhGiHJJASYSUqwsxz00dw+rAc6qw2/v7+av63JjfYyxJCCCGEEO2MBFIi7ERYzDx1/nDOGdWZequNWR+u4aMVe4K9LCGEEEII0Y5IICXCksVs4tGzh3LBmK7YbHDbJ7/z31V7g70sIYQQQgjRTkggJcKW2WzioTMHM3NcdwDumruenQfLg7soIYQQQgjRLkggJcKayWTinlMHcmTPNCpr67n547XUW23BXpYQQgghhNArKgqee06doqKCvRrdJJASYc9sNvH4ucNIjI5g5a7DvLxoW7CXJIQQQggh9IqMhGuvVafIyGCvRjcJpESb0Dk1jntPHwTAUz9s5o99JUFekRBCCCGEaMskkBJtxtkjO3H8wCxq623c9NEaquvqg70kIYQQQgjRnPp6WLBAnerD5/hNAinRZphMJh46awjp8VH8mVfKUz9sCfaShBBCCCFEc6qqYOJEdaqqCvZqdJNASrQpHRKiefisIQC8vGgby3ceCvKKhBBCCCFEWySBlGhzjh+UzTmjOmOzwc0fraW8ui7YSxJCCCGEEG2MBFKiTbrntIF0Soll96EK/vXVxmAvRwghhBBCtDESSIk2KSkmksfOHQrA+8t28+MfB4K8IiGEEEII0ZYENZBatGgRp512Gjk5OZhMJj777DPHbbW1tfzjH/9gyJAhxMfHk5OTw1//+lf27dvn9hzV1dVcd911dOjQgfj4eE4//XT27t3byl+JCEXjenXg0qN7APD391exeMvBIK9ICCGEEEK0FUENpMrLyxk2bBjPPfdco9sqKipYtWoVd999N6tWreK///0vmzdv5vTTT3e736xZs5g7dy4ffPABixcvpqysjFNPPZX6MBqdKALnthP7Mb5vBlW1Vi6dvZz5f+YHe0lCCCGEEKINMNlsNluwFwFqdPXcuXOZOnWq1/ssX76cMWPGsGvXLrp27UpxcTEZGRnMmTOH888/H4B9+/bRpUsXvv76a0444QRdr11SUkJycjLFxcUkJSX548sRIaS6rp5r313NjxsPEGkx8fz0kRw/KDvYyxJCCCGEEAA1NfDMM+ryDTdAVFRQl6M3NgirHqni4mJMJhMpKSkArFy5ktraWo4//njHfXJychg8eDBLlizx+jzV1dWUlJS4nUTbFR1h4cUZIzllSEdq621c8+4qvvx9X/MPFEIIIYQQgRcVBbfeqk5BDqKMCJtAqqqqittvv53p06c7IsO8vDyioqJITU11u29WVhZ5eXlen+vhhx8mOTnZcerSpUtA1y6CL9Ji5plpwzlzRCfqrDauf381c1dLL50QQgghhPBNWARStbW1TJs2DavVygsvvNDs/W02GyaTyevtd9xxB8XFxY7Tnj17/LlcEaIiLGYeP3cY54/ugtUGN320lg+X7w72soQQQggh2rf6eli+XJ3CaM5BRLAX0Jza2lrOO+88duzYwbx589zqFLOzs6mpqeHw4cNuWan8/HzGjRvn9Tmjo6OJjo4O6LpFaLKYTTx81hCiIszMWbqLf3y6jp2FFVx1bE9S4sInlSyEEEII0WZUVcGYMepyWRnExwd3PTqFdEZKC6K2bNnCjz/+SHp6utvto0aNIjIykh9++MFx3f79+1m/fn2TgZRo38xmE/88YxCXHaNGo7+4YBvHPDKfR779k4Nl1UFenRBCCCGECAdBzUiVlZWxdetWx7937NjBmjVrSEtLIycnh3POOYdVq1bx5ZdfUl9f7+h7SktLIyoqiuTkZC677DJuvvlm0tPTSUtL45ZbbmHIkCFMnjw5WF+WCAMmk4n/O2UAo7ql8uxPW/gzr5QXF2zjzV92MH1MN648tifZyTFuj3EdcNlU6agQQgghhGj7gjr+fMGCBUycOLHR9RdffDH33XcfPXr08Pi4+fPnM2HCBEANobj11lt57733qKys5LjjjuOFF14wNEBCxp+3bzabjZ825vOfeVtYu7cYgCiLmXNHd2ZMj7RGQZPWgze8cwpd0+OCsWQhhBBCiLajvBwSEtTlECjt0xsbhMw+UsEkgZQAFSD9vOUg/5m3heU7Dzd7f7MJTh+Ww7UTe9MnK7EVViiEEEII0QZJIBW+JJASDf22vZC3l+7icHmNx9vLa+pZu6fI8e+TBmdz7cTeDO6U3EorFEIIIYRoIySQCl8SSAlfrM8t5vn5W/lmvXPPskn9M7l2Ym9GdUtt4pFCCCGEEMIhTAOpkB9/LkSoGtwpmRdnjGLzgVJemL+Vz9fuY96f+cz7M5+je6fz94l9OLJn4x4rIYQQQgjhIjIS7r3XeTlMSEYKyUgJ/9h5sJwXF2zj01V7qbOqX6vR3VL5+6TejO+bIQGVEEIIIUQYkNI+AySQEv6093AFryzazgfL91BTZwVgSKdk/j6pN1MGZGE2S0AlhBBCCBGqJJAyQAIpEQgHSqp4ddF23v1tN5W19QD0y0rk2km9OWVIRywSUAkhhBBCgNUKGzeqywMGgNkc1OVIIGWABFIikArLqnnjlx3MXrKLsuo6AHp2iOfqCb2YOqITkZbg/rEQQgghhAiqMB02IYEUEkiJ1lFcUcvsX3fyxi87KKqoBaBTSixXT+jFuaM7Ex1hCfIKhRBCCCGCQAKp8CWBlGhNZdV1vLt0F6/+vJ2DZWqfqqykaK48thcnDc72WvKXGhdFVIRkr4QQQgjRxkggFb4kkBLBUFlTzwfLd/Pywu3klVQ1e/+UuEguPboHF4/rTnJs+IwGFUIIIYRokgRS4UsCKRFM1XX1fLoyl1d/3s7uQxUe72Oz2bBPVCcxOoKLx3Xn0mN6kBYf1YorFUIIIYQIAAmkwpcEUiLU1VttfLVuP8/N28LmA2UAxEZamHFkV674S08yk2KCvEIhhBBCCB9JIBW+JJAS4cJqtfHDxgM8N28r63KLAYiKMHNM7w5E+DBOPTs5hovHdadXRoK/lyqEEEIIoU+YBlIRrbgmIUQLmc0mThiUzfEDs1i4uYD/zNvKyl2Hmfdnvs/POWfpLk4Z0pG/T+pN/2z5IEEIIYQQrSwyEm65xXk5TEhGCslIifBls9lYueuwo9zP0GOxMf/PAn7ceMBx3fEDs/j7pN4M7Zzix1UKIYQQQoQPKe0zQAIp0Z79sa+E5xds5et1+9H+Gozvm8H0sV2Ji/K8t1VqXBSDcpIwmYyXEwohhBBChDIJpAyQQEoI2JpfxgsLtvK/Nfuotzb/Z2FUt1T+Pqk3E/pmSEAlhBBCCN9ZrbB7t7rctSuYg7tvpgRSBkggJYTT7sIKXl60jVW7i7zeZ1tBGTV1VgAGd0ri7xP7cPzALMw+DLwQQgghRDsXpsMmJJBCAikhjMovqeLVn7fzztLdVNbWA9AvK5FrJ/XmlCEdsUhAJYQQQgi9JJAKXxJICeGbQ+U1vLF4B7OX7KS0ug6AHh3iuWZCL6aO6ESkJbipeSGEEEKEAQmkwpcEUkK0THFlLbOX7OSNX3ZQVFELQKeUWK6e0ItzR3cmOsLz0AohhBBCCAmkwpgEUkL4R1l1He8u3cWrP2/nYFkNAFlJ0Vx5bC+mj+lKrJcpgEIIIYRoxySQCl8SSAnhX1W19XywbDcvL9rO/uIqANLjo7hgTFdS4jxvtJeeEMXJQzpK9koIIYRobySQCl8SSAkRGNV19fx3VS4vLNjKnkOVzd7/2L4ZvHLRKGIiJZgSQggh2o0wDaQiWnFNQoh2JjrCwgVjunLuqM58vnYfi7ccxOrhsxsb8P2GAyzaXMAlby7n9ZmjiYuSP09CCCFEuxARAddc47wcJiQjhWSkhAgFy3Yc4tK3llNWXccR3VN5Y+YRJMZ4LgMUQgghhAgUvbGBzCYWQoSEMT3SmHPZGBJjIli+8zAzXl9GsX0CoBBCCCFEqJFASggRMkZ0TeX9K44kJS6StXuKmP7aUg6X1wR7WUIIIYQIJJsNCgrUKYyK5SSQEkKElMGdkvngyiNJj49iw74Spr2ylILS6mAvSwghhBCBUlEBmZnqVFER7NXoJoGUECLk9M9O4sOrjiQzMZpNB0qZ9sqvHCipCvayhBBCCCEcJJASQoSk3pmJfHTVUeQkx7CtoJzzXv6V3KLmR6gLIYQQQrQGCaSEECGre4d4PrzqKLqkxbKrsILzX/6VPYfCJ+UvhBBCiLZLAikhREjrkhbHR1cdRY8O8ew9XMl5L//K9oKyYC9LCCGEEO2cBFJCiJDXMTmWD688kj6ZCewvruL8V5ay5UBpsJclhBBCiHZMAikhRFjITIrhgyuPZEDHJApKqzn/laX8sa8k2MsSQgghRDslgZQQImykJ0Tz/hVjGdIpmUPlNVzw6lJ+31sU7GUJIYQQoiUiIuDii9UpIiLYq9HNZLOF0a5XAVJSUkJycjLFxcUkJSUFezlCiGaUVNUy841lrNpdRHyUhT5ZiR7vFxVh5rShHTl3dBdiIi2tvEohhBBChCO9sYEEUkggJUQ4Kquu49K3lrNsx6Fm75uZGM2Vx/Zk+tiuxEWFzyddQgghhGh9EkgZIIGUEOGptt7Ksh2HqKyp93j7rkMVvP7zdvYVq8180+KjuOyYHlx0VDeSYiJbc6lCCCGE8MZmgwr79iZxcWAyBXU5EkgZIIGUEG1XTZ2Vuav38vz8bey270GVGBPBJeO687cJvSRDJYQQQgRbeTkkJKjLZWUQHx/U5eiNDWTYhBCiTYuKMHP+EV2Zd/N4njp/GL0y4imtquPZeVuZ9cEa5LMkIYQQQvhCAikhRLsQYTFz5ojO/HDjeJ69YASRFhPf/3GAT1flBntpQgghhAhDEkgJIdoVs9nE6cNymDW5LwD3f76BvYcrgrwqIYQQQoQbCaSEEO3SVcf2ZGTXFEqr67j149+xWqXETwghhBD6SSAlhGiXIixmnjhvOLGRFn7dXshbS3YGe0lCCCGECCMyrqqtqKuGqhJIyAj2SkKP1arGaAZ5lGbAWOvBZgVLK43zPrwTNn+nXhPt++ry/Y2Kh+yhkNEfLDr+xNhsULIP6qshtYd/fk41FVC0GyoPQ5cxYPa8GW+PDvHcecoA7v5sPY98+yfH9u1A70zPm/sKIYQQQriSQKotsNngnbNhzzK47HvIGe6f592zDFbOhhMehNgU/zxnU1a9DQsegZMfhf6n+Oc5CzbBKxNh3N9h4p3+ec6acvjkUijdD9FJ6hRjP49OVJdNFrDV24OcehXMWetU8JE9GPqfpi/IaI7NBq8dB1XFcO2ywAdT+1bD7DOgurj5+0bGqYCq00jIGanOkzurn8mB9ZC3HvJ+hwMboNK+qe7Iv8LJT0BElL71VBbBxi/g0HYo2gWHd6kAqjzfeZ9BZ8HZr4PZcwJ+xtiu/PDHARZtLuDGD9fy32vGEWnxIVl/aDt8PBPGXAkjZuh/XMUh+OBCGHAaHHWN8dcVQgghwp3FAuec47wcJiSQagvWfwo7f1aXFz0G0971z/N+dRPkrVMH/kderf9xpXmw/r8w+GxIzNL3mF+fh+/sgc6Gz/wXSO1aArXl8MszMPZvEJfW8uf88yvY/G3LniO1Oxx9AwybDpExvj9PdYkKbgDKDqhAJVDy1sGcM1UQlTlQZZywqWAOe3+RzaYCg/1roaYU9ixVp+aYzOqxq96Gwu1w3tsQn970Y3Yuhv9eBSV7Pd8enax+9hv+CwmZcOK/PWa7TCYTj549lBOeXsS63GKen7/VMYjCkF+fV1/3r88bC6T+/BJ2L1GZvnAOpOpq1Pe3tTKj7VnhNpgzFcZdD2OuCPZqhBCi5WJi4OOPg70KwySQCne1lfDDvc5///kl5P8Jmf1b9rwHt6oDZ4CDW4w9dtFjsPw1WPwknPUq9Jro/b42m7r//Aed1xV7OTD2RXWJOq+rglWz4ZgbW/6cW39S54PPgX4nqWxQdal6repSVWJpq1dZKXOEyoSYLKq8zFoPGz9XB81f3ggL/g1HXQujL1XZLKPKDzovV5e2/GvzJv9PePsMVSrXaTRcNFdl3ryxWqFwC+Sugn2rVLC3/3dVvheTAtlDIGswZA1SgXpGf9jxs8r07VoMr02CCz70/D6uq4EFD8PipwCbCkr7HA8pXSGlG6R2U5djU2HdJ/DpZfDbS5CQBX+5yeNys5NjeGDqYK5/fzX/mbeVSf0zGdo5Rf/3p65GfaABkL9R/Sz0/jz3rlDnpfvU9zc2Vf/rhgqbDd46BUpy4e/LVXmnCJwdC1XmdePnEkgJIUQQSSAV7n59Tn0in9QZMgfA1h/gl6fhzJda9rwb5jovF2419tgDG9R5eYHKYBx7C4y/vXEpm80GP9wDS55V/x5wujowKN7j+7obcg0ulr0GR13XspI6qxW22QOpURdDj2ONP8cJD8KqOerrLslV34Ofn1AlYWOvbj4T46o1AqmDW+Ht06GiEDoOgxmfNh1EgQoeM/qp0/AL1HX1tSpQiM/w3AfV93i4/Ad473wVaL4+Bc55A/pMcV/Lfy93ZuFGzIATH4HoBM/rGHIOlOXDd3fAT/erzJSXbNHpw3L4fkMeX/6+nxs/XMNX1/+FmEid5QVbf1BfGwA2tT69743clc7L+Ruh2zh9jwslh3fC3mXqcsEmVcYpAqeySJ3XyNh+IYQIJpnaF85K8+Dnp9TlKffDhDvU5XUfq08rW6IlgZSWwep1HGDPOL19uhoooLFaVemgFkSd8BCc9Ii6XLIP6ut8Xrob1+CiZC9s+qplz3dgnQoQI+Ohy5G+PUdUPBz5N7h+DZzxPKT3UVmtRY/B65PtpXI6VbgEUlUlvq2nKYd2wOzTVNlg1mC46DPf++UskSqQaWqYROYAuGI+dB2nMnzvnQe/vuAs+3v5LypIiUmBc2er75+3IEpz1DVw9Cx1+fPrYZP3sswHzhhMZmI02wrKefTbTfq/trXvu/9byzI1p7oM8v9w/tv1cjjZ7VK+WSIbHAdcVZE6r5VASgjRRpSXOweDlZcHezW6SSAVzuY9oHpAOh+h+pE6j4Ie49VQgyX/8f15CzZB/gZVjgbqwKhG55u68rDz4P68t1WTf1QC7PoFXjoGtvyogqTProYVbwAmOO0ZVd6WkA3mSFUWV7rf9/W70gKpmBR1/tsrLXu+rT+q857j9Q9E8CYiSmVHrv0NznlTXXdou/7vNaigTlPt50CqaDfMPl2VnGX0h7/+zz89Zs2JT1evNXyGGs7x3R3wwpHw+XXqwLH7X+DqJTBoqv7nnHyf6kez1auBELt/83i31PgoHjlnKABv/LKDJdsOeryfm4pDaoohwDB79k1vILV/jX36od2BcA2kljgvu35gIgJDy37WlAV3HUII0c5JIBWu9q2B1fahEic87PyUX+sBWfU2lBV4fGiztGxU78kQaz9wPrRd32MP2rNXiTkqUzDkHLhqkeqJqSiEd8+GV8bD7x+oQO3s12DUTPUYsxmSO6nL/irvq7JPlzvicvV6uxaraXG+0vqjek1q+do0ZgsMPktNuQP3iXPNcSvt82MgVbJPZaKKd0N6b/jr5xDfwX/P35yIKDjjOTj+X4AJCv5UQfaUf6q1aO8TvUwmOP1Z1UtVV6kyXfl/erzrxH6ZTB/bFYBbP/6dkqrapp97w1yor4GsIWrqIEDuCn2ZRS3gskSr8/yNer6a0OOakfJnj6PwzFHaFz6f2gohRFskgVSo2fKDM+vhjc0G390F2NTAgy5HOG/rMV6Nmq6rgqUv+LYGLZAadKY6iAb9AycOblbnHXo7r0vvBZf9CEfYm6IPrAdLFJw/RwVarpK7qHN/HYxpGamM/jDwdHV52cu+PVdVCeyxZzJ6T2752hrSApVyHVkQTUWh87I/e6Q+vVz1vaR2h4u/0D990Z9MJhh3Hcz4RGV6Lv9BTTr0Msa8WZZIOPctNSyjqgjeOctr9uSukwfQLT2O3KJK7v+8mSzR7x+q82HnQ8fhKmAvO6DvPZxrD6QGnqHO8/8wVtoZCsoPOn/vQUr7WoMjIyWlfUIIEUwSSIWSysPwv2vVnlBz/6ZKhjz580uVWYmIUSVLrkwm+MvN6vLy15wZGb3yN6pP/y1R0P9kZyBVuE3f4wvtAVeHBuOjI2PglMfhvDkqCLnwY88jzrVAqqU9XhotuIhOVOPPAX7/yPv3tik7FqmyybSekNbDP+tzFZ+pzsuMZKRcS/v8FEhVHFKlmAAz/gtJOf55Xl/1nqyGp+SMaPlzRcWr916HvuqA/zfPQ1nioyN44txhmE3w6aq9fLs+z/PzFW5TwbXJDEPOhag4NYkQnEFSU/baB00Mv0AFYFVF/itrbS2u2SiAYgmkAk7rkaqrVJNAhRBCBIUEUqHEEqV6nTCp5vXnx6j9mFw/oa6rhu/vVpeP+jukdGn8PP1OVhmY6hJY/rqxNaz/rzrvPRlikp2ZJb0DJ7TMVXofz7cPPF1Nfes5wfPt2j5IfstI2cvdohOhy1i1Qaw2Ct0obVpfILJRoKbZgXtw1BzX7JW/hk1oQVRGf5VNbGvi0mDERepy6QGvdxvdPY2rxquv/6656zhYVt34Tr9/pM57ToTEbHW582h13lyfVMk+1X9msqj3pva9DreBE7t/VeeZ9gBSMlKBp5X2gQycEEKIIJJAKpRExcOJD8NlP6iD2PIC+OQS+GC6swRp2StweIfaE8fbnkhms3NK2dIX1F5Teths7mV94JKR0hlIafdzLe0zQgsM/dUj5Rg2kaSydVpWavnrxiYD2mzOkstABVIJLQyk/JWR2rlYnXc/xj/PF4q0PZ6a+Z7NmtyH/tmJFJbXcMd/12Fz/VDDZlO9fuAcMgGqdBDcx5p7ogVamQPV737mQPXvcBs4oWWktDLd0v3tM0sSiKmZ3rgGUtInJYQQQRPUQGrRokWcdtpp5OTkYDKZ+Oyzz9xut9ls3HfffeTk5BAbG8uECRPYsGGD232qq6u57rrr6NChA/Hx8Zx++uns3Rvmzc5djlADGsbfrhrsN30Nz49VY6AXPqbuM+nupsc+DzkHkruqg/LV7+h73QMbVGmeJRr6nqiucwRSW5rv3aivc5YAestINcfvGSmX0j5QGb+4dBWobfpa//MUblXlhpaowAUYvmSkXMefVxss4/SmXQVSTR/8RkdYeOr84URaTPzwxwE+Wenyvtzzm+oji0pwL1PVMlL71qi9s7zRSv86j1LnWiAVTgMnasrV5EFQUxRNFlX+aqQ8tS1Y8x78uwus/TDwr1VfCzUuHwBIICWEaAssFjj5ZHWy6NzDMQQENZAqLy9n2LBhPPfccx5vf/TRR3nyySd57rnnWL58OdnZ2UyZMoXSUud/IrNmzWLu3Ll88MEHLF68mLKyMk499VTq68P8E9GIaJh4hwqoOo1SB3zf3aEOlrOHwPDpTT/eEglHX68u//Js0wd0mg32sr4+U5wbrqb1BEyq18p1sIEnRbvAWqt6t5I9lBzqkdzV/lx7Wt50X1/rLHuJtn89kTHOKYHLDIxC16b1dT1KZQ8CwWiPlM3m/4xUxSE1DASgW1sOpOzvBx3jowd0TOKmKf0AuP+LP9h72P6e0vaOGniG6o3SpPeB6GTVv9JUmZ7WH6VlsLK0QGqD5/uHotyVKnBKzIHUHpDYUV3f3sr7dvyszvX0xbVUw75XCaSEEG1BTAx89ZU6xcQEezW6BTWQOumkk/jXv/7FWWed1eg2m83G008/zV133cVZZ53F4MGDmT17NhUVFbz33nsAFBcX8/rrr/PEE08wefJkRowYwTvvvMO6dev48cdmJt+Fi6yBqtTvhIfVeGyTWV0264jWR8xQWY7i3bD+06bv66msDyAy1hkUNVfep92e3tv3yWraWOvacudkKl+5BhZaBgJg9GXqk/OdP+sfhe4o6zuuZWtqitGpfVXFKnDV+COQ0rJRGQOcpYZtkc7SPs2Vx/ZkVLdUyqrruOXjtVhrKp2/L0PPd7+z2Qyd7IMxvPVJ1depjYXBmcHSMlIFm8KnNE4r6+t2lCqd1QaTtLcR6FopcmuU97mW9YEEUkIIEUQh2yO1Y8cO8vLyOP744x3XRUdHM378eJYsUZs/rly5ktraWrf75OTkMHjwYMd9PKmurqakpMTtFNLMFjjqGrh+jdqItMdf9D0uMhaOvFpdXvwUWK3e75v3u9orKiLGWdan0ZrgmxuB7hg04WN/FKg1ayVuLe2T0g6SI2JVhk6T3AkGnKYu6xmFXlvlDDAC1R8FkGDPSOndR6phhtAfB3HtoawPDAdSFrOJJ88bRlyUhaXbD/Htf99SgWxSZ7VBcEPN9UkVbFQfFkQlOidcpnZX79W6Kji0w9CXEzS77H9nux6lzrUPQtpbRkoLHP29KbYnDT9gqpVASgghgiVkA6m8PDVuOCvLff+arKwsx215eXlERUWRmprq9T6ePPzwwyQnJztOXbr4WIbW2hKzIHOAsccccbkqYyr4E77/P+/BlDatr8/xjXuv9A6ccOwh5WN/lMYxAr2lgZTLxL6Gxl6lzn//uPlR6LuXqDKtxI7OrEEgGO2Rang/f2akJJBqpFt6PP93ivr5R274WF059FzP2dfmJvdp13ca4cwumy2QoUoIw2JyX30d7F2uLnc9Up0naYGU5/252iSr1Rk4+nMvN2+00ecayUgJIdqC8nKIj1en8vD5uxaygZTGZDK5/dtmszW6rqHm7nPHHXdQXFzsOO3Z46cJcaEoJhmm/FNdXvo8/O+axv1S3sr6NHoDKcfEvr5N3685/ho40XDQhKuuR6les7rK5keha/1RvY5T5UuBovVIVR7W19OmlQBqAVhLPw0vL3T253Q7umXPFeq090RthaHpjdPHduWu8RlMMK8B4M2yse6T/DRaRurgZs97uWm9NNr9NOE0cOLAetVjFp3sXLe/h8WEg/ICqK9Rl43u2+eLRqV9Mv5cCNFGVFSoUxgJ2UAqO1vtydIws5Sfn+/IUmVnZ1NTU8Phw4e93seT6OhokpKS3E5t2uhLYOpLqi9o7fvw4Qz3kej7VqtBEZFx0PeExo937CXVzKa8/ijtA0ixD5zwV2lfjIefr+so9EWPq74Ub7RAKpD9UQCxqepnBPr6pLSMVFpPdW4wKGjEsX9UG++PAjVpT1NjLItwRdoaIk31/G7twf1Lrfz7mz8bB1MJGfb3sQ1yVzV+Em3QROcGgVQ4DZzQ9o/qMsaZVUtqh6V9JS5BYzBK+3QMTBFCCBEYIRtI9ejRg+zsbH744QfHdTU1NSxcuJBx48YBMGrUKCIjI93us3//ftavX++4j7AbfgFMe1f1QG3+Fuac5fxkU5vW1/cEzxPptMDo0HbvTfCVRc7enpYGUo5PtVsYSFU1UdoHMHSa6m+pKYMPLvTcY1S8V/WzmMzeNxH2F7PZZeCEjj4pbfR5ag/ndQaDAjdaWZ/eHrxwFhGlfhfAeDmWfVpfxYBzAXh50Xbu/+KPxsGUo0+qQXlfVYkqtXW9j0Yr3Q2HjJQWSGllfeAMpIrbUSDlmn1rjWETUtonhBAhI6iBVFlZGWvWrGHNmjWAGjCxZs0adu/ejclkYtasWTz00EPMnTuX9evXM3PmTOLi4pg+XY3+Tk5O5rLLLuPmm2/mp59+YvXq1cyYMYMhQ4YweXIAhwKEq34nwUVzVc/U7iXw1qlQmgcbPlO3eyrrA9WzZImC+mrvwY1W1peQ7TkDZITfe6S8rMcSAee8qUY3F25RZY8ND4a3zVPnnUZBXFrL1qOHkT4pLWuVmK2GFEDLDuR22kc4t/X+KI0PfVIc3AL7VoHJwpGnXcmDZw4G4K0lO7lz7nqsVpf3j6NPqsHAiX2rAZsa9Z/YIHOeOUidF25TQ05Clc3mMrHP5UMrbdhEWV7LsqPhpLhBRqql2zY0p9GwifAqgxFCiLYkqIHUihUrGDFiBCNGqFHBN910EyNGjOCee+4B4LbbbmPWrFlcc801jB49mtzcXL7//nsSE50ZhqeeeoqpU6dy3nnncfTRRxMXF8cXX3yBJYw282pV3cbBJV+rfpwD6+DFo1VwFBkPvad4fozZ4iwf89YnpZX1tXTQBECKPZDyV2mft0AKVAnWeW+rjY83fgG/PON+u2PseSsF5logVWYgkIrP8C0oaPhc2oCDtt4fpfHle7Z9gTrvcSwkZHDh2G48ds5QTCZ4f9lubv3kd+q1YKrzEeo8d4X7wXXDjXhdJWZDTArY6p3DW0LR4R1QdkB9wJIz0nl9fKb6XbJZVTDVHrgGUtY697LpQNAqCUz2/74lIyWEEEET1EBqwoQJ2Gy2Rqe33noLUIMm7rvvPvbv309VVRULFy5k8ODBbs8RExPDf/7zHwoLC6moqOCLL74Inyl8wZI9BC77To1b1srD+p3kvqloQ+nN9EkV+jGQ0jJS5QUtOyhpamqfqy5HwEmPqMs/3e88WK6vg232y70C3B+lMZSRst8nvoNLUOBjRkrrj8oc6CwvbOt8CaS0g9gU59+Yc0d34enzh2Mxm/h01V7OeWkJ8zflY8seooKK8gLVg6hpuBGvK5MJsuxZqVCe3LfLXtaXM0Jtcq0xmyHJvilveynva/iBT6D7pLTSPm3zYwmkhBAiaEK2R0oEWFpPuPQ7yLIHpiMubPr+WiDlbS8px6AJPwRSsakqQwYtOxhrampfQ6MvheEXqk/SP7lUlRXmroTqYpUh6DSy2afwCyN7SWn7SMV3cJZT+pqRcow9bwf9URotU2nkwLe62P2xdmcM78Tz00cQHWFm9e4iLnlzOae9tILiZPs4c23cuc3mkpHyEEiBS59UCAdSjv6ooxrf5hg40U4m9zWcUBjoPimttE/7PksgJYRoC8xmGD9enTxtKxKiwmelwv8Ss+GKeXDtcug1qen7NjcC3Z+lfSaTfwZONDW1z9NrnvIEZA9VAcpHf4VNX6nbek10TiULNMewCT1T++z3ievQ8tK+9rJ/lCtfvmfaQXJMSqObThzckZ9vm8gVf+lBbKSF9bklfFagsgbb1ixUJX/Fe1VJnDkCOg7z/BraKPEDoRxI2fujmgqk2k1GqkEgFeiMlJYV1frRpEdKCNEWxMbCggXqFBsb7NXoFhHsBYggi4iGDB37PjVV2metVxP9wD+BFKjSqYOb/BNI6clIAUTGwvlz4OXxaqDAvtXq+tbqjwLnXlJlzWSkbDZnWWZ8hjND4ss+Nu2xPwp8DKTs318vwXlmUgx3nTKQqyf05o3FO/hzST/gB4q2/MrkJxdycfJqZgJ7onry5H89j9zvURHJ9cDhnWv554drMAEjuqVy7qjOxESGQO9nWYGzlLfLmMa3awf47WFT3tpKZ4ltUic19j3ggVTDjJSMPxdCiGCRQEroowVIxXvUwUOky6cFRbvVRD9LtLO/qaX8sbFnlecyrCaldoezX4d3zwHsAwJaqz8K9PdIVRWpxnaw90i1oLRPy0ZlDoL4dOOPD1e+BFLNTYK0S4uP4pYT+lE6eAa8+hyDzTvZe7CYmqLlEAELyroyd7XnjE0SMVwfA6l1+fy0ejMlxPPf1bk8+9MWrvxLT6aP7Up8dBD/dO+xZ6MyB3qeZJlk/91tD6V9WrAYGa/+9pXkBr60T+uR0v5GSmmfEEIEjQRSQp+4dIhJVsHJoe3Ohnhwlvul9/JfCZw/RqDrmdrnSZ/JMPFOmP8gdBzubJ5vDQk6AymtrC86SWUVWzJsoj2W9UELM1LJuu6emNMfYlKIririhclRDFm3D4qh69C/8H8dB3h9XNniLBKqD/Dg0RFsj+3LRyv2kFtUyYNfb+SFBVu57Jge/HVcd5JiIvWv3V8cZX1Her49uR2V9mkZ8+TOLn2KAQykaiuhzj4W35GRktI+IUQbUF4O3buryzt3QryHfU1DkARSQh+TSZX35a5UgZNrIKWNaW7pRryuUrqq89Ys7XP1l1vUJ+6uX2drcM1I2Wzq++6Joz/KnkFqybCJ9rQRrytfgk9Hj5TO4NxkUnuQbfuJKQk7oVxtxDt+4smMz+jp/XG7hsDWA5zWsQhG9+Gaib2YuzqXF+ZvZWdhBY9/v5mXF21n5rjuXHJ0D9Lio/R/DS21a4k699QfBZCUo85L2kMgZf8akzu7lNcGMJByjD63QIJ9DzIp7RNCtBUHdfSHhxgZNiH08zZwwjFoQkevlV5+GTahc/y5J2YzDDgV0nr4/vq+0AIpa13jjTdduY4+B9+HTZQVQMFGdbk99UeBb+WQOkv73GjT+VbNgbpKiE5u/kOHBgMnIi1mzhvdhR9vGs8z04bTJzOB0qo6/jNvK8c8Mo+Hvt5IfmkrbOBbUw7716rLXgMp++9uWT7U1QR+TcGklR63VkZKK+uLTYHoBHVZhk0IIUTQSCAl9NNGmx9sEEhpgZW/Bk2As7SvOBesVt+eoyUZqWCJiFYH2tD05D7XQRPg+6fhu+zZqKzBnvtd2rJWKO0DnPtF5W+w/3tk86NdtUAqf6Pb1REWM2cM78R3s47lpRkjGZSTREVNPa8s2s5fHpnPvf9bz76iAG4Iu3eF2iw4qbPbXlpu4juofklsULo/cGsJBY7Svi6tlJGyf7gSkwJR9rIX6ZESQoigkUBK6JfeS517y0j5Yw8pTWJHVb5irVXjoo2qq3H2EugtwwoVjj6pJib3NSzt87VHqr32R4HxQMrX91SnUe7/9rZ/lKssLZDaoEo8GzCbTZw4uCNfXncMb848ghFdU6iuszL7112Mf2w+t3/6O7sKA3CA7dg/ykt/FKhyxvZS3tfaGSmttC82xbnXXm2F7x82CSGEaBHpkRL6eSrtqyqBsjx1uYMfe6QsEepgrHiPOhkd+OB6cBwVRhkpUFmmwq1ND5wo95KRMlra1x434tUYDaRcD5CNlPbFp0NqDzi8Q/27k45AqkNfMJlVBqLsgNrzzQOTycTE/plM6JfBr9sK+c+8rfy6vZAPlu/hoxV76JuViNlLn12/7ESuPLYnAzoa+Fq0QKqbl7I+TVIn9fW29YETroGU1qvkyxYEemkZqdhUZ0YKVDCllfoJIYRoNRJICf20jFTlIag4pErBtP1kErKMlTvpkdzZGUh52q+mKdpBb2ScCsrCiRYclTURSDlK++w9Ur58Gl5WAAV/AiboNs7wMsOe0UBKO0COSjQ+nbLzaGcgpScjFRkLab3U79eBDV4DKY3JZGJc7w6M692BFTsP8dz8rSzYVMCfed6/tj/2lzB3dS5TBmbx94m9GdYlpek12WywZ7m63KWJjBS47CXVhkeg22zugZRWxtgaPVIxKfYtKEyATZX3SSAlhBCtLsyOMEVQRcU7N50s3ApxY5z9Uv4s69MkdwF+9W0Euq+jz0OBnr2kHMMmtIyUD/0+7bk/Coxn8ZrZjLdJnY+AdR+rfcq04Lc5mQNUIJW/EXrr38tsdPc03rpkDFvzS8kt8jyAorbOytw1uXy9bj8//HGAH/44wLF9M7huUm+O6O7lvVBVDLX2csHmhmVoo7nb8qa8FYfU8BBQ2fOW7OWml6O0L1WVUEbFq0xYrfRJCSHCnNkMo0c7L4cJCaSEMem9nIFUlzHOjJQ/y/o0LdmUtyUT+4ItIVOdN9kjVajOG/ZIGWl03/GzOm+P/VHgHnxarc3/4fZl0IRm0FmwYS4Mm6b/MVmDYOPnkP+H8dcDemcm0jvT+/t/8sAstuaX8cKCrfxvzT4WbS5g0eYCxvZI47pJfTi6dzom17LAykPqPDIeImOafvH2sJeUNmgiIUsNiYlpxWETsSnqPDJOBVIycEIIEe5iY2H58mCvwrDwCflEaGjYJ6XtIeXP0ecabSqYLyPQw3Fin0bLWDQ1ta9RRsp+EFdfDXXV+l5n1y/qvL3tH6VxvDds+j7R92X0uSYhAy79FkbN1P+YTPuGvT4GUnr0zkzgyfOGM//mCVwwpiuRFhO/7TjEjNd/48wXlvDTxgPYtGEXFfZASgvem6KNQG/LpX2uZX3Qsk2x9XIt7QOXyX0yAl0IIYJBAilhjBZIaZP6AlraZ9+UtyWlfeE2sQ8g3p6RKvOSkbJaocKekWq4jxRAtc4NOg/vUufaqO32JiIGzPakvJ5yLKOb8bZUpn0z6Pw/Az6VrWt6HA+fNYRFt01k5rjuREeYWbOniMtmr+CUZxfz9br9WB1Z0NTmn1Cb2teWM1LaREJHINWaGSn7zyDK3hcV6E15bTZY8G/44/PAvo4QQoQZCaSEMVrAVLhNHdwd2qb+HWqlfVoZVlhmpJrpkaoqUnv5AMTZAymzxXlQVa1jalhNhbO/Q2/PTltjMrlkEXQciLaktM8XaT3Ufkx1lc5BFQHWMTmW+04fxOJ/TOKq8T2Ji7Lwx/4Srnl3FY/NXQKANVZHP532u1txEGpbYaPgYHDdQwqcAbaRrLBRruPPAaLi1HmgN+XdvwYWPAzf/COwryOEaL8qKqB7d3WqCJ8suwRSwhhtct+hbVC0S+2rY4mClG7+fy3tYKy62PhI4XAeNuHokfISSGklf9HJEBHlvN7IwAkto2WJcgZg7ZGR71lLSvt8YbZARj91ucHGvIGWkRjNHScN4Jd/TOL64/qQGBNBTal6383bVceHy3dTU9dEliw2FSJi1eXSNjpwolFpn8v7IlBZqUYZqVbalPfQdnWu/d0QQgh/s9lg1y518rB/YqiSQEoYk9JNlUPVVcH2Beq6tF7Gx0HrEZ3gPGAwmpUK50BKyxDVlHnufXCMPm/Qq2Jk4ESFy7AKL/sMtQvRBsbGt3ZpH6iBExDQPqmmpMZHcdOUvvxy+yRO6hUNwN7qWP7x6TomPDaft3/dSVVtfeMHmkxtf+BEw0DKLSscoECqYY9UpD0jFejSPq0MOJDZNiGECEMSSAljLBFqc1GAzd+q80CU9Wm0shmjfVLhPLUvOkmVdIHnrFTDQROujwNjGSk9gwPaMiMZqdYu7YNWGTihR1JMJKMzVAZqSJ+eZCRGs6+4inv+t4G/PDqf137eTkVNXYMHaSPQ23ggpX2d4NInFYBNeW02D6V9Wo9UgMtginY5LweyB0wIIcKMBFLCuA72PiktIxWIQROaZB8n94Xz1D6TyaVPysPkPu26uAa9TYZK+7QJbO1w/yhXjgxCCJb2gXMQSP6frfea3tjHn4/q34ufb5vIA2cMolNKLAWl1fzrq40c/e95PD9/KyVVter+WoDhS49jqKurgdI8dVn7GwW+bYytV3WpszfSUdqnZaQCXNp32DWQCkCQKIQQYUoCKWGc1idVZ28i7xDAQMrXEejhPLUP1Lhs8LyXlBZINRwSYeQgTjJSSqhnpBKz1Xko9Ka4BN8xkRYuOqo782+ZwKNnD6V7ehyHK2p57LtNHP3veTz5/SYq4+xrb4ub8pbuA2wqc+z6exjITXm1sj5LNETa+8+0HqlAb8jrmpHSM8xGCCHaCdmQVxiX3qCULxB7SGkcGSlfe6TCMCMFTU/uq/ASSBnZx0YCKSXUAymtF6aqSJV2BbOfzUMWMyrCzHlHdOGskZ348vf9PD9/K1vyy3h23laKo0q53wzVh3YTHaQlB4xrf5TrzySQm/I2HDQBLqV9AQykrPXupdVS2ieEEA4SSAnjGgZSDf/tT1ojt9EeKcf48zDNSDW1l1RzPVJGh020Z0aCz2CU9mm9MPU1KgOsZSKCwV7ah4fx5xEWM1NHdOL0YTl8/0ce/5m3lV15qRAF27dt5sPPN3De6C7ERHougkiNiyI1PqrxDQsfgz8+g4u/CK0y1IaDJjRGhpcY1bA/ClyGTQSwR6p0P1hrnf8O5IbDQoj2y2SCgQOdl8OEBFLCONeeqPgM9//Y/a2lpX1hm5GyZ5sM9UjJsAnDjHzPgpGRikoAk0X1xlQWBS+Qstl0vWfMZhMnDu7ICYOyWfabDb59lGwKeWvJTt5astPr4yxmE2eO6MQ1E3rRM8NlHP+ad9UeWlu+h2HT/PTF+IEjkOrifr2RyZlGaaV9bhkpbfx5AKf2ufZHgfRICSECIy4ONmwI9ioMkx4pYVxCJkTZDxgCOWgCnAcqpXmqwVuvcB5/Di57SRnokfJlH6l2H0jp/J7ZbMEZf24yOQM37UA6GGrKVVYMdGWGTCYTY4cNBSDVVMbEnvEkxUR4PCXGRFBvtfHJyr0c9+RCrnt/NX/m2b/X2vt035oAfFEt4C0jFchhE1ppn1buCa2zj1TRbvd/S2mfEEI4SEZKGGcyqYET+9cEdtAEqIyXJVrtX1KSC2k99D0unMefQyv0SMnUPkB/IFVb4ZyY1trBeUyyKqsLZibAsYFztLOcrDkxySqjVlPGm2d1bnKbhDV7inhu3lZ+3HiAL9bu44u1+zhpQBovau/lfauNrXf7Alj3CZzwYGAyiF5L+7SgNwA/K0+lfY5hEwEs7StqkJGS0j4hhHCQQEr4JmuwCqSyBgf2dUwmdbByaJs6eNETSNVVOz89D/dAqqxBIGW1umSTvE3tk4yUbnoDKe3A2GRxHry2ltgUOIzzQDoYtP4oIxs4m0xqBPrBTVCyt8lAaniXFF67eDR/7Cvh+QVb+XrdflZu3AYx6vaqvau5/NVfsJncN/5OiY3iwiO7clTPdEyu6/rhXvX3qdvRMPwCA1+oTo5AqpP79a2RkXIt7YtshfHnWmmfOQKsdZKREkIERkUFHHGEurx8uSr1CwM+BVJFRUV88sknbNu2jVtvvZW0tDRWrVpFVlYWnTp1av4JRPg77m7oPBqGnh/410rpYg+kdPZJuR4Uh3sg1TAjVXkYbGpj1EZBkN7+DJ39Lu2C7kDKpayvtZtgHZP7gpmR8jGDmWwPpIr1bco7MCeJ56ePZGt+GZ998y3sUNfH2Ko5sH09W2ydGz3mq3X7GdUtlb9P6s2EvhmYaishb526sTQAo9dtNuffokY9UgGc2qeVdrqV9rXC1D4tI5UxAA6skx4pIURg2Gzwxx/Oy2HCcCD1+++/M3nyZJKTk9m5cydXXHEFaWlpzJ07l127dvH2228HYp0i1CRmw+hLWue1jI5A1/6jj0oAs6Xp+4YqrUeqohDq68Bi/1XVyvpikiGiwZQzrayouU/Dq0udU7g8TGBrV/ROWQvGoAlNKPRI+RpIJeWo8xJ9gZSmd2YCt/ylgyOQAnjkqHr2dB3udr+Vuw7zwfI9rNx1mEveXM7gTkncPbiIsVoZZukBY+vVo6rYOdwhqTUzUkXq3OOwiVbISGUPUYGUlPYJIYSD4UDqpptuYubMmTz66KMkJjo/7T/ppJOYPn26XxcnBOAMpBo2PXsT7hP7wB7gmACbKqtyDJ/wMvoc9GdXtGxUZBxEhUfqPGD0fs+CMfpco/XEhEJpn9HAO8meQTIYSAGNJlaOjNjJyOHugcsZwzvx94m9efXn7byzdDfrc0uYn/clYyPV7bayPPyeP9Q+0IlLb/z7E8gNeR2lfSnO67TXD1SPVF2N82eXPQTWIhkpIYRwYXhq3/Lly7nqqqsaXd+pUyfy8vL8sigh3BgdgR7uE/tAZaC0sjvXvaS8jT4H92ETTaXFK1z6Xdo710Cqqe9ZSGSkwrS0D3SX9rm/pj3gN9ujov1rPN4tMymGu04ZyC+3T+LvE3tzRMQ2x227d+3AavVziYi3QRMQ2A15myvts1r9/5rFewAbRMSqAUMgGSkhhHBhOJCKiYmhpKTxH9JNmzaRkeHhU3IhWko7YNFb2hfuE/s0nvqkHBkpD4GUdhBnrVObt3rj6I9q52V94HyPWGvVkBJvghpIpdjXUNT6r63xtadOK33zJSOlvWbXI9X5/t9VmasXafFR3HJ8XyYmuEyZK83jzrnr/BtMeeuPggBvyNvEsAlsUFfp/9fU+qNSuoZGQC+EECHGcCB1xhln8M9//pPaWtVjYTKZ2L17N7fffjtnn3223xcohFuPlJ4GxLZQ2geQ4CGQ0g4uPQVSkfGgFTI19Ym4DJpwinLZ/LWpcqxQKO0L5gGsz6V9LchIadnXrkeqn1NdJRzc3PRjinZjdtl7Lct0mA+W7+aWj9dSV++njI0WFHrMSNmDjdoKqK/1z+tpKu0/f9fSPtdR9DUBKO/T+qNSuwV2kIYQQoQpw4HU448/TkFBAZmZmVRWVjJ+/Hh69+5NYmIiDz74YCDWKNq7pE6ASWVZGvRNeKQdELfmxqmB0GRGykP212zW16MhgZST2ezcXLqpLEIwNuPVaAfnweyR8jWLqZX2VRcb7xty7JeWCR2HqcvN7Se1d7k6zxwIQIyplhRzJf9dncusD9dQ649gqqnSPtcPb/zZJ2WtV99DcM9Imc0uI9DL/Pd6GkdGqpvzfdhc6bAQQvjCZIJu3dSptafjtoDhYRNJSUksXryYefPmsWrVKqxWKyNHjmTy5MmBWJ8QajpdYjaU7ofi3c5MjTfaJ/fhnpGKtw+Y0NsjBeprri5uOiiQQMpddCLUlDZ94NvuS/t87KuLTlTTJKuLoWQfZPTz4TXToONw2PWL6pMacaH3x+xZps67/0VlwaqL+c+pOVz6VTFf/r6fmjor/5k+guiIFkzz1AKphhP7ACyRqp+orlK9Z/xVPuuajWz4HoyKVxmwQAyc0Ab8pHZzLx2urZRBNUII/4qLg507g70Kw3RlpNLS0jh4UB3AXXrppZSWljJp0iRuueUWbrvtNgmiROAZGYHeFoZNgLN8zzUL11RpH7gPnPBGe472Pvpco2dyXzBL+0JhHylHf44P7xnHwAmdPY4a7X0f3wFyRqjL+9Y0/RgtI9VlDCRmAfCX7DpeuWg0URFmvv/jAH+bs5Kq2npja3HlyEh56JGCwIxA177/UQkqWHMVyBHoh10yUlEJYLIfMkiflBBCADoDqZqaGseAidmzZ1NV1UQjuxCBoJXRFOmY3NdWAinHyHPXjFQTwybA5SCuiaCg0scJbG2VnkAqmBmpUBh/3pIBJb4OnKhwyb7mDFeX89Z5HzhRWwl5v6vLnY+ABBVIUXaAif0zeePiI4iJNDN/UwGXz15BZY0PwVR9ncqsgefSPghML5GnPaQ0kVogFcDSvlR7qY2eD2qEEKId0VXad9RRRzF16lRGjRqFzWbj+uuvJzY21uN933jjDb8uUAjA2Aj0tjJswmOPlI7SPmhm2ISMP3ejK5AKZo9UijqvKXXfnLm11FY6y8Z8CqTsm/IaGThhtbq/TxOyVC9bTSkc3ARZgxo/Zt8aVXYWn6mmzCV2VNeXqm05junTgdmXjOHSt5azeOtBLn5zGW/MPIKEaAPfz7I8sNWrkexaoNZQIDJSVfaMlOvoc40jI+Xn0r6acuffnpRu9tdPVh8qyMAJIYS/VVbCsceqy4sWgZc4I9Toyki98847nHzyyZSVlWEymSguLubw4cMeT0IEhJGNPdvM+HOtR8p+MGOtdynt89InpicokB4pd3o+ZQ9qaZ/LawYjE6AFNOYI377+ZB825a0qUgELqPep2dz8wAnXsj6TyVHaR9kBx13G9kzn7cvGkhgdwbIdh/jr679RUmVgup6jPypHrcmTQGzK68hIpTS+TetV8ndpn9YfFZPsfN1obeCElPYJIfzMaoUVK9QpEPviBYiuj+KysrL497//DUCPHj2YM2cO6elyECZaUZL90+WS/c3ft81kpLQeqQI1JavyMGCfluUtMyBT+4zT8z0LZmmfJVL1p9SUqfdAa5dkuo4+92WSki+lfdp7NDpZDZsBVd63a7HKPI2Y0fgxe+2DJjofoc4TstV5qftG8aO6pfLuFWO56PVlrNpdxIWv/sacy8aQEhfV/Lqa64+CwGzK6+hRS2l8mzbCv9bPgZRrf5QmkBsOCyFEGDI8/nzHjh0SRInWl2gvDyrVE0gFsQzLn7SsU321+pq0MpvY1MYN5xpHdsXLJ8YNS6aEwdK+IARSENyBExUt7KlL9mEvKU89WY6BEx4yUjYb7LFnpLRAKtFzIAUwtHMK719xJGnxUazLLWbaK0spLGtiQ2aNYzNeL/1R4BKY+/FnpU1sbLK0z98ZKZf+KI2j/0syUkIIATozUs8++yxXXnklMTExPPvss03e9/rrr/fLwoRwk+TS72CtB3MT44urgliG5U9Rcc5MRPnB5vujwGWvFy9BQXWxS8mUDJsAnIGUt2Z9a73qzYHgvadikqFkb3BGoLc0g+makbLZ9GW1XCf2aToOV+cH1qvNbl0/TCjeq/qXzBHOgMsxbKJxIAUwMCeJD688kumv/cafeaVMe2Up714+lsykGO/rKm5iM16N9jvYasMmtNI+P/dIecxIuewlJYQQQl8g9dRTT3HhhRcSExPDU0895fV+JpNJAikRGPGZavSurV5lZrRPmz1pK6V9oLJSNWVqLynHBqVN7KPV3LAJLbsQlQgR0f5bZzhrLiPletAYrCxnMCf3OUr7PBzE66EFUjVlKpPhqTytoQoPHxqk9VSBbHUJFPwJ2UOct2llfVmDnT1DjoyUs0eqoT5ZiSqYevU3tuSXcf4rS3nvirF0TPbS5Owo7fOwh5QmOgDDJprskQrQ1L4iKe0TQojm6Crtcy3n27Fjh9fT9u3bA7pY0Y5ZIpyfMGvjhz2x2drO+HNwn9zn+JS+icxAc0FBS8ZYt1XR9h4Tb98z7aAxIiZ4wacjyxGGpX1Rcc4grKnfXbfX9JAFcxs4scb9/ntXqHOtrA+cfy9qSpsse+uZkcBHVx1Fp5RYdhws57yXf2Xp9kJsNlvjO+vpkdIzOdMoR4+Uh2A2UKV9h5so7ZOMlBBCAD70SHmzbds2Jk2a5K+nE6IxxzjjJvqk6qrAap/C1RYyUq57SZXryUg1MzhBBk001tzBoRa8BDMwd/RIFbX+a/ujp87R46gzkCrXplM2eE1tP6mGfVJ77BmpLmOc10UnOsvePPRJueqaHsdHfzuKbulx7DlUybRXlnLey7+ycHOBe0Clp0cqIOPPi+zPndL4Ni2QqvVzaZ82tc9jRkp6pIQQAdChgzqFEb8FUmVlZSxcuNBfTydEY9p+NE19qu0IIEzOaVbhzDG576Bz2ERTPVLNjfKWQKoxvaV9wRo0Ac6SrmAcwLpO7fOVkamb4Lm0D5x9UvvXOK+rrYL9a9Vl14yUyeQs7yvzXt6n6ZQSy6dXj+OiI7sRZTGzfOdhLn5jGWc8/wvfb8jDWlniDGiSdJT2tVZGytEj5cfSvsrDzmEZKV2d1wei/0sIIQDi46GgQJ3i44O9Gt38FkgJEXB6MlKu/VHe9nkJJ469pFx7pJoaNiEZKcOaC6SCuRmvRjuADUaPlD/eM3p+d/W8pjZIIs8+cAIg73eVhY7rAKnd3e/vZQS6Nx0Sonlg6mB+/sdELjumBzGRZn7fW8yVc1bytxc+B8AWk9z0e6G530FfNNkjZf/AyJ/DJrSyvvhMZ88ZSGmfEEI00AaONEW7oedTbUcZVhso6wMvPVJNZaSa+TRcAqnGmg2kgriHlCYkSvtakpHSkU125e29ntZT7S1VXw35G9V1rmV9DScCetiUV4+spBjuPnUgv/xjEtdM6EVCdATVharUbXtNKp+u3EttvZcNIwMRbLT2+HNPo89Bhk0IIUQDEkiJ8KGnz6ItTewDSPAQSOkt7fPULC/DJhrTW9oX1B6pYA6bsL9nWlLa56+MlMkEOfaBE1p5X8ONeF0ZzEg1lJ4QzW0n9ueXf0zirwPVlgs7alO5+eO1THpiAe/9tpvqunr3B/m7/K2u2tn/5HHYhD1j5M8NeT2NPgcVxIL0SAkh/K+yEiZMUKfKymCvRjdd488BRowYgamJ/T8qKvzc6CpEQ3oyUm1pYh+4Z6S0zICeYRPY1CfU0Q36xGQz3sa071ltBdTXqQmRrhwZqSC+p4I6/tzen9OS94zRjFRTmdOOw2HHIjVwYuRfPU/s02gZKR8DKU1yXCTHdayBrdCxS2865Eex51Ald85dx7M/beGq8T2ZdkRXYqMszvdTTWnze97p4fiZmzz/XXOU9rVGRkrbR0oCKSGEn1mtoM1asHrJ+Icg3YHU1KlTA7gMIXRwZKSaCqS07EEbyUhpPVKlec4DpaZK+yJjwWRR+21Vl3gIpKS0rxHXoSQ1pY0/9Q+p0r5WPoCtr3X+TrUki2kkI1VT4czAeHqva31S+9aoDXJLctUec51GNr6vlpHysimvIeX5AAzq15efZ07i/WW7eXnRNvJKqrj/iz94fv5WrvhLTy4cnY3jHVVdqm/frKY4Bk2keO77dAybaIWMlGv/l97NlYUQog3THUjde++9gVyHEM3TMlLVJVBd1jhIgLZX2qcdSDomcpmaLrEymdTXXlXkuVRNAqnGIqLUHlF1VfYD3waBlCM4D2YgpZVUFbXu62oZTJO5ZYGklpEqL4C6GvU99/qa9hJWS5TnyZvaCPQD62HXEnU5a5CzV8iVIyNlrEfKIy1QiU4kNsrCpcf04MIju/LJyr28uGAbew9X8vA3f/Liwm2sMEURYatR752WBlJN9UeBS4+UH6tCvGWktIyYzar+JrWVv7NCCOEj6ZES4SM60Xlg5e2T7eoQmLDmT7GpYI5w/3fD0rOGmpoaJoGUZ031SYVaaZ+n3rdA0Uafx6S0rEQtLl0FRtB8dsjxHu3gOeOR2kMFdfU1sGq2uq7zmMb3A/9mpLRAyiVgi46wcOHYbsy/ZQKPnzuMnh3iKaqo5bA1BoC35q2lsKy6Za/b1MQ+1/XUlPnnvWGzed5DClTGW/t7JH1SQgihPyMlREhI7AiFW1SvRYc+jW+vCoHBAP5kMqmeKC1wbKo/SuOY3NfgQKe+znlQJoGUu+hElS3xGEiFwD5S2mvb6j33vgWKv4aTaHs6Fe1WPY6uexM15G0zXtfn6jgcdiyEnT+r67p4CaS0faQqD6uhDRHRPi0f8BhIaSItZs4Z1ZkzR3Ti63X7qflfPFhL+Gr5Zh5ZHcn5R3ShZ4bnfVEizGZOHpJNSpyXLF1Te0i5rcemsqqRsQa+KA/KDqjnMZkbbzxsMqn3YkWh+r0I4q+EEEKEAgmkRHhJsgdSXjNSbay0DxoEUjp2/I72kpGqKgLsn1h7Oyhrr5rKSIXC1L7IODBHqv2SqopaMZDy43CSxBwVSDU1dRNcNuNt4jVzRqhASuNp0ATYM7jRalx62YGmA7jmaOW1TWz0bTGbOG1YDralWbB/P0MyTCzPr+etJTubfOrP1uTywRVHYjZ7yMA1V9oX6bLPU015ywMprT8qqRNYIhvfHp2kAinZS0oIISSQEmEmsZnpX21tah+4Z6F0BVIuI9BdadmFmJTmywPbm6b2/gmFYRMmkyrtKi9Q62mYKQgUrbSvJaPPNXqmboJ7aZ83Wp8UqIArrafn+5lMkJAFxbtVn1SLAinvGalGL2svA717cmfGx4zhs9W5jcek2y3YVMCyHYd4ffEOrjjWw9fRXEbKbIGIWKirVGvU8zeiKd7K+jSyl5QQIlDi4pq/T4iRoykRXpKamf7VFjNSCZnOy00dXGq8ZVekP8q7JnukQqTvLiZZBVKtOQLdn+8ZPfvAgb6Np7XJfaCyUU1Nj0u0B1It7ZMyEEhpgbmpuoTxQzMY39d7Se77y3Zzx3/X8dh3mzi2bwb9shv87WquR0pbkxZItVTRTnXecNCExlvpsBBCtER8PJT7cfpoK9EVSD377LO6n/D666/3eTFCNKvZjFQbG38O7geUej5t9jZsQgIp7/QMmwh2ltMxAr2o9V7TUdrnh1JQ3RkpHaV9Kd3U96OqCDqPbvr5Evyzl5SjtC9SRyBlYFPeaUd04fsNeczfVMBNH61h7jVHExXhMgequdI+UJvyVuCfQMrb6HON7CUlhBAOugKpp556SteTmUwmvwZSdXV13Hfffbz77rvk5eXRsWNHZs6cyf/93/9htu+nYbPZuP/++3nllVc4fPgwY8eO5fnnn2fQoEF+W4cIIe0xIxXvkpHSNWzC/rU3PIiTQMo7b4FUXbXqr4Hglva5vn5rZgIcZWV+KO3Tu5eUnr4skwkGnAq/fwT9Tm7mde0DJ1ocSNnHixvISOnpIzKZTDxy9lBOeHoRG/aV8J95W7j5+H7OOzRX2gfOvq1af2SkvIw+1xgIEoUQoq3TFUjt2LEj0Ovw6JFHHuGll15i9uzZDBo0iBUrVnDJJZeQnJzMDTfcAMCjjz7Kk08+yVtvvUXfvn3517/+xZQpU9i0aROJiW3oYFoojoxUc+PP29A4KdfgSU8Q5O0gzl8T2Noib4GU68FisINz1xHorcWfwXdSM9lkjZ7SPoBTnoIpDzT/fvbHCPS6GjXoA/QFUgb7iDKTYnjwzCFc8+4qnp+/lYn9MxnZ1R446Snt8+emvM1lpAwEiUIIoVtVFZx9trr86acQExPc9egU0vtI/frrr5xxxhmccsopdO/enXPOOYfjjz+eFStWACob9fTTT3PXXXdx1llnMXjwYGbPnk1FRQXvvfee1+etrq6mpKTE7STChJaRKjsAVg/N21VtsLQvwXXYhIHx540CKe2TfgmkGvE2oEPL/kQltmwfJX9wlPa1YkbKn+8Z14xUU/sd6SntA7Wpr551+WNTXseG2Pg9I6U5eUhHpg7PwWqDmz9aS2WN/e+broyUnzblra+D4r3qsteMlPRICSECoL4evv5aneo9D+cJRYaHTVx66aVN3v7GG2/4vJiGjjnmGF566SU2b95M3759Wbt2LYsXL+bpp58GVKYsLy+P448/3vGY6Ohoxo8fz5IlS7jqqqs8Pu/DDz/M/fff77d1ilYUn6n2N7HVQ1m+M7ACdXDWJkv7fJ3a17BHyo+jrNsabyPjq0NgYp/GUVJV1Hqv6ddhE/bf1boqFRx4C4L0TO0zwh8ZKS3TY4n2PBK8IR8n291/+mCWbj/EjoPl/Pubjdx/xmCdPVL20j7XgM8XJbnqb6sl2vl9ayhapvYJIYTGcEbq8OHDbqf8/HzmzZvHf//7X4qKivy6uH/84x9ccMEF9O/fn8jISEaMGMGsWbO44IILAMjLU/8xZmVluT0uKyvLcZsnd9xxB8XFxY7Tnj17/LpuEUCWCJfm8QblfbWV6iAAgj8YwJ+M9kjJsAnjmivtC/bEPghOaZ8/x59Hxjifx1ufVH2dMwPT0jHeGkePVEsyUgYm9oHP5W/JcZE8du5QAGb/uoufN+frzEjZS/tqW5iR0vqjUrqA2cvhgWPYhARSQghhOCM1d+7cRtdZrVauueYaevb0speHjz788EPeeecd3nvvPQYNGsSaNWuYNWsWOTk5XHzxxY77mRqMvrXZbI2ucxUdHU10dAt2uBfBldhRHYg1PBhz/Mdu0n/AEw4SMiFzkDqw0XNAK8MmjPMaSIXIxD5o/WET1npn0OavctCkHBWcleyHLA8DgbTADZP/No3WAqnyAhWo+bKHmiOQ0rkRsrffQR3+0ieDi4/qxuxfd3HPx8uYb61TNzQ3/tx1nb5qrj8KZB8pIYRw4ZceKbPZzI033qh7up9et956K7fffjvTpk1jyJAhXHTRRdx44408/PDDAGRnq/8gG2af8vPzG2WpRBvirWnddTPepvaVCTdmC/ztZ7hyofdPiV3JPlLGefuehdLwktYef15ZBNh7mfwW1Gh9Ul4GTmjv0dhU//WkxXUAkwWwqWDKF1rJnN4PaBxZGw/j9HW4/aQB9OwQT1WpCixt5kjnQAlPtJHsLS3ta25iH8g+UkII4cJvwya2bdtGXV2dv54OgIqKCseYc43FYsFqtQLQo0cPsrOz+eGHHxy319TUsHDhQsaNG+fXtYgQ4m2McnUIlWH5m9mi/8Cy2WETEkg10lxGKhTeU1pGorUOYLXsUHSyvr4gPZrbS0rvxD4jzGbnpta+9kk5MlJNBDOuHL+Dvv2sYqMsPHX+cLIiValeMQlU1DbRfO2vYRNFu9V5Slfv94mRqX1CCKExXONw0003uf3bZrOxf/9+vvrqK7dyO3847bTTePDBB+natSuDBg1i9erVPPnkk46BFyaTiVmzZvHQQw/Rp08f+vTpw0MPPURcXBzTp0/361pECPF2MNYWJ/b5QjuIqylT5VlmC9TXOg/qJJBqzNuwiapQykjZ19BaPVKODKafslHg3L7Aa0ZK58Q+oxKy7OXAPvZJ1RrYQwrc+xRtNp8y5MO6pPDwyV3hOzhYF8sdbyzjjZlHkBjjIaht1dK+FHUupX1CCGE8kFq1apVb/5HZbCYjI4Mnnnii2Yl+Rv3nP//h7rvv5pprriE/P5+cnByuuuoq7rnnHsd9brvtNiorK7nmmmscG/J+//33sodUW+btYKwtTuzzhWv2pKZMHYBr2SiTOTSCglCj9b40PPDVPnUPiR6pFHXeWqV9gchgNpeRClT5aWI27Kf5zYC9cZT26e2Rsr9fbFb1WB//Jg1IVlUepeZElu88zEWvL2P2pWNIjm0QTGmBVEs35DVS2ldT6vygRgghWio+vumtMUKUrkDq888/56STTiIyMpIFCxYEeElOiYmJPP30045x556YTCbuu+8+7rvvvlZblwgybwdjrj1S7VlENFiioL5GfWockxyY3pO2xHGga1Of6kfbD5hDqbRPC4BrK9QGsRFRgX09f07s0zSXkSq3v0/9WdoHzkmfZT5mpIxO7YuMBXMEWOvU76CvH+7Yg+ZeXTqRkhvJmj1FXPjaUuZcOpbUeJefvz8yUrVVzkAzpbv3+7n+LlSXNj0EQwgh2jhdPVJnnnmmY7S5xWIhPz8/kGsSommOgzEvPVLtPSMFjXt+ZNBE0yJj7QMJcC/vC8XSPmidPqlAvGeazUgFqLTPMQK9pT1SOgMpk8nnEehu7GWcSakZvH/FkaTHR7E+t4QLXl1KQWm1837aIIqW9EgV27cBiUpoekpjRLTaZwpk4IQQot3TFUhlZGSwdOlSoPnR4kIEnHYwVl0C1S5TqqS0z6nhQZwEUk0zmTwPnAil0j6zRQ1+gFYKpLTSvgBkpCoOQl1149v9vRmv43W1TXl9zUgZLO0D/4wJd9lDakDHJD686kgyE6P5M6+Uaa/8yoGSKvd1tWRqX0muOk/u3HxPl+wlJYTwt6oqOPdcdaqqCvZqdNMVSP3tb3/jjDPOwGKxYDKZyM7OxmKxeDwJEXDRiRBlP+h1zUq15al9RklGyjhPAye0fqRQyEiBy15SRYF/LUc5qB8Dqbg0ZzbDU3YoEFP7ABJaOSMF/slIOd5/KQD0zkzkw6uOomNyDNsKyrn909/t69IyUi0o7dO+93qCWNlLSgjhb/X18Mkn6lTfxJTSEKOrR+q+++5j2rRpbN26ldNPP50333yTlJSUAC9NiCYkdYSDpWovqQ591HVVIZQ9CLaGnxgHIrvQ1jiCT5eDw1Aq7QOITYZiWmdyn5YN8ed7xmRS2aGiXepDkIZDDRwBv5/fp4mt3CMF/tlA2SUjpenRIZ45l41lylMLmb+pgF2F5XRzDJtoQWmf9jciXseHLf4IEoUQog3QPbWvf//+9O/fn3vvvZdzzz2XuDid+2kIEQiJHeHg5gYZKSntc9C+B1VS2qebp9I+7SA4VILz1pzcF6jgOylHBVINN9SGwJX2JbiU9lmt+ja2dqUFUk1tituQt73JjNAC5gYDHXpnJjC+bwYLNhXw7m+7uXOsVtrXgoxUhS8ZKemREkK0b4Y35L333nuJiorixx9/5OWXX6a0VP0nsW/fPsrKWriruhB6Jdl7LVwPxiSQcpLSPuMafs9sNuflUMlIBaO0z++DH7SBEw0CKZstgKV9mYBJTdHTphEa4chIGeiR8suwCXtGSgugXVx0pMrmfbRiD1Ume7lkTbnv44ONfO8d70PJSAkh2jfD+0jt2rWLE088kd27d1NdXc2UKVNITEzk0UcfpaqqipdeeikQ6xTCnXYw5jEjFSLZg2CSYRPGNQykasrBZq/TDpW+Oy0z0SqlfQEYfw7OD0E8Td201qrL/n6fWiLVc1YcVK9rNFDzqbTPD31EWsAc23hT5An9MumUEktuUSXfbS7jDFDv17pqiIwx/lpGMlKOvy+SkRJCtG+GM1I33HADo0eP5vDhw8TGxjquP/PMM/npp5/8ujghvPKYkZLx5w6SkTKu4fdMK1syWYyVdAWSo7QvwAewNlvgSvu8ZaS0jEhkvBpH72+OEeg+9EkFa9iEl9I+AIvZxPSxXQGYvcJlSxJfy/vKDfSnSUZKCCEAHwKpxYsX83//939ERblvBtmtWzdyc3P9tjAhmiQZqaY1CqS0g2IJpLxqOGyi2mXQRKhs+dBaPVJVxc5snN8zUh5+d8HYsANfODbl9WFyXzDGn1utzoDZQ0YK4PwjuhBpMbFqbylWbRpirY+BVIWB0r5o6ZESQgjwIZCyWq3UexhLuHfvXhITJRMgWomnjT1l/LlTw2bwQE1Da0sajj+vCsH3kz8mwemhlfVFxvtWJtaURA/ZZAjcZryO19UCOF8CqSBkpKqLAXu/k4ceKYAOCdGcPER9XZXYf06+ZqSMDPqQfaSEEP4WFwdlZeoURgPtDAdSU6ZM4emnn3b822QyUVZWxr333svJJ5/sz7UJ4Z12UFR2AKz17oMBpLTPPSiorXR+Si0ZKe+8lfaFUoaztXqkApnBTHIJaFwHIxjZx8gXLRmBHoweKW3QRGQcRER5vdsM+9CJorpIdUWNDyPQrVaXjKDsIyWECAKTCeLj1SlUqkB0MDxs4qmnnmLixIkMHDiQqqoqpk+fzpYtW+jQoQPvv/9+INYoRGPxmWAyq/Kjsnx1EGyzqtskkHL/NFw7QDJHhFZQEGoaBlKupX2horVK+xyBlOeSshbRPgSpr1avo5XyaRkRf0/s0/i6Ka/V6vwgwlBGSsva+Jg9dPRHNf0zGN0tlf7ZiZQdigETzjJEI6qKnKWceoJn2UdKCCEAHwKpnJwc1qxZwwcffMDKlSuxWq1cdtllXHjhhW7DJ4QIKEuE6nko3Q+l+wB7uVAoDQYIJtegwHXQRBh9ytPqGmWkitR5SAVSrVTap71n/N0fBRARbZ+gV6h+dx2BVKBL+3zMSNVVOi+3ZkbK8f5LafJuJpOJC4/sRuXXqkfKVlOO4d9yLRsYnawmHDZH9pESQvhbdTVcdZW6/PLLEB0d3PXoZDiQAoiNjeWSSy7hkksu8fd6hNAvsaMKpEr2O5vAoxMlWADvgZTwrlEgFYIZqdYq7asM8HCSxBz1vizZD9lD1HXlAX6f+pqRcvQcmSDCwIeFDXvujNJK+5rJSAGcOaIT679Ra9uyJ4++/Q2+lmPQhM7vvUztE0L4W10dzJ6tLj//fNgEUoZ7pGbPns1XX33l+Pdtt91GSkoK48aNY9euXX5dnBBNct2PRib2uXP9NDyQ2YW2pGG5kmOcfgi9p1yb/K3WwL1OoEafaxx9Ui4DJwJd2ueakTKyaa1jYl88mA38l+k6BdKXTXK1wNLD6POGEqIjSElW743lW/b48FoG+9OktE8IIQAfAqmHHnrIUcL366+/8txzz/Hoo4/SoUMHbrzxRr8vUAivXPejkYl97rQDnbpKZymTTOxrmrdhE6H0ntLKvGxWqPEx06FHoLOYiR6mbga6tE/LSNVVGesx82XQBDjfN9Y6NfDFqPwN6rxDH113z8lUQdD2fQUcKKky9lpGg1gtoK+tgPpaY68lhBBtiOFAas+ePfTu3RuAzz77jHPOOYcrr7yShx9+mJ9//tnvCxTCK9f9aLQSExk0obh+Hw7bM8VS2tc010DKZgvN0r7IGIiwj7kOZHmfVtoXqCymI5vskpEK9NS+yBjnz9LIpry+BlJRCWogDviWudn/uzrPHqrr7klJ6muLtVXx/rLdxl7LaBDr+vdFyvuEEO2Y4UAqISGBwkL16dX333/P5MmTAYiJiaGy0odP3YTwlet+NDL63J0l0tnPcXinOpdAqmnae8daC3XVoVnaB60zcCLQpX0eM1IGxm/7SstKGdmU17W0zwiTyfmeMhps1NfCAXtGquMwfY+x94nGmap5f9luausNlH6WG8xIWSLVHmPg+1RCIYRoA3zaR+ryyy/n8ssvZ/PmzZxyyikAbNiwge7du/t7fUJ455qRkh6pxrSDOAmk9NEGloB6P4ViaR+0zgj0gPdIufQ3ggpctVLFQJagJmoDJ3zJSCU0fT9Pon3cuPbgZjUePioRUnvoe4w90EuPrOVASTU/bTTwNfpSVil7SQkhhPFA6vnnn+eoo46ioKCATz/9lPR09Yd35cqVXHDBBX5foBBeOTJS+12yB5KRctAOdCSQ0sdsVgeuoN5PoVjaB62TkQp0aZ9rfyM4y/rMEc2O+27Z6/qSkbJvcOvLtgq+jgnfv1addxyqf8CFfX0DO1gAeGHBNur0ZqV8KauUgRNCCGF8/HlKSgrPPfdco+vvv/9+vyxICN20jFRNqfOATAIpJ+17UV+tziWQal50ono/uWakQi3LGegR6DZb4IdNaBmpykNQW9V6e50l2Cf3GcpI+VjaB74HGwb7owBHxqxPqpnEggh+31vMiwu2cd1xOoZVOMafGwikZC8pIYQ/xcVBfr7zcpjQHUgtWrTI4/XJycn07t2b+Hgf/pMRoiWiE1UGoaZUlcJA6B30BlPDoFKm9jUvOhFKUYFUdahmpFLUeaBK+2rKob5GXQ7UeyY2FSzRKsgv3R/4iX0anzJSLSjt87X8zZGR0tkfBRClDjxirJU8cMZgZn24hmd+2sLE/pkM7tTMe9hRymng+x8tpX1CCD8ymSAjI9irMEx3IDVhwgSvt1ksFq6++mqeeOIJIiN17IouhL8kdYSDpVCwSf071PpZgqlhUCkZqeZF2w+Wq4qcmYiQC6QCXNqnlfVZon0rZ9PDZFK/u4d3qkAq0JvxanzKSPk4tQ9825TXaoW8deqyoUDKvr7aCs4YnsP3f+Tx9bo8bvxwDV9cdwwxkRbPj7PZnKV9hjJSPvZ/CSFEG6K7R+rw4cMeTzt27OC9997j888/57HHHgvkWoVoTOu10A7+pLTPSQIp47T3T4nLWO5Qy3IGurSvtcrsXKduBnozXsdrtiQj5UMgFeNDad/hHSrLHhEDHfrqf5w2Ra+mDJPJxL+mDqFDQjRb8st44vtN3h9XU+Zb+a8MmxBC+FN1NVx7rTpVVwd7NbrpDqSSk5M9nrp168a5557LM888w7vvvhvItQrRmNZroZFAysk1O2eJ9u1AsL3R3j/Fe9V5RCxERAVvPZ44SvsClJEK9MQ+jevUzdYq7dPGn5f6Mv7cl6l9Pow/18r6sgaBxUAbs/b7bR+OkRYfxSNnDwHgtcU7WLq90PPjtGxURKyxvxHR0iMlhPCjujp44QV1qqsL9mp0Mzy1z5thw4axa9cufz2dEPpoGSlNqGUPgsk1qAx0dqGt0N4/JbnqPBRLRR2lfUWBef7WCqRc95IK9Ga8jte0l/bVlEF1mb7H+KW0z4dAysigCXAJpModVx03IItpR3TBZoObP1pLaVVt48f5mg10ZNskkBJCtF9+C6T27dtHZmamv55OCH0kI+Vdw0BKNK9hRioUA/NAl/YFevS5xrGXVCuW9kUnOjNLZTr7pByBVCuNP8+zT+wz0h8FLj1S5W5X/9+pA+mSFktuUSUPfPlH48eV+5gNdGRGpbRPCNF++SWQys/P5//+7/+YNGmSP55OCP0kI+Wd6/dCJvbp4wiktIxUiA2agMAPmwhGRsrRl9UK71PHwAmd5X21rbghr83mvoeUEa4ZKZvNcXVCdARPnDsckwk+WrGXH/5oEED6GsTKPlJCCKF/at+IESMweSgNKi4uZu/evQwYMIAPPvjAr4sTollJDQKpUCzFChbJSBmnfc9K7cMmQvH9FOjx54HeQ0rjmpGKiLW/ZoAzUqAGThzapn/ghD+GTejN2miDN0wWyBxk7LW0CYvWOjW+PiLacdOYHmlc+ZeevLxoO3f893dGdj2W9AT77RU+llXKsAkhhNAfSE2dOtXj9UlJSfTv35/jjz8ei8XLeFUhAiVRSvu8cs2mSCClj/b+sVnt/w7BQEor7Qv0+PNAl/ZpGanSPOf3PdClfQDx9n1KtJK25rRmj5SWjcroD5Exxl7LdX015W6BFMCNU/qyYFMBmw6Ucufcdbw0Y5T6cNTX0j4ZNiGEEPoDqXvvvTeQ6xDCNwmZ6tNbWz2YI9TIYKFIRsq4hoFTKJf21VVBbZXxA+7mtFaZnRZI1de0XhYMjPeYtWZGytf+KABLpHOT45ryRj+/mEgLT54/jKnP/8J3Gw4wd3UuZ43s7FLaZ7RHSvaREkIIvw2bECIozBZnz0N0kkymcyWBlHENM5qhWNoXlQgm+5/uQJT3OXqkAvyeiYhqXE7WKoFUqjrX+71r0fhzgxvy+tofpdEGYtRWeLx5UE4ysyarvanu/d8G9hVV+j4xUUr7hBD+FBsLO3aoU2xssFejmwRSIvxpfVJS1udOhk0Y1yiQCsGMlNkc2LKqysPqPNClfeDe4xiTrLIqgab1mGlfZ3P8kZGqr4Y6HRtM7m9BRgqcwV6N99HuVx3bkxFdUyitruPWT9Zi0wIpX4dN1FerzKgQQrSE2Qzdu6uTOXzCk/BZqRDeaCVCodjPEkySkTKuYSAVHYKBFAR2BHprTtBz7XFsrfeoke9dXY0qPQTfAqkol/dTc5mb8kIosY/dzxps/LXAOXCixnNGCiDCYubJ84YTG2nhl62FlB62T/EzmpFy/V2R8j4hRDslgZQIf9r0r1AswwomCaSMC4fSPnCZ3OfnjFRNhbMsrDUCKdeMVGtM7ANjUw9d92SK9CGQMpudwVRzwUaevawvrZfv7zsPm/J60qNDPHee3F8t0df+NLPFJTMqgZQQooVqauDWW9WppibYq9FNAikR/hKltM8jswXSeqrR0ildgr2a8BAOwybAZS+pIv8+b946dR6f2ToZXteMVGtM7ANnj5SejJSW2TFHqp4uX+jdlNfRH+VjWR+4BFLeS/s0M47sxsTeSSSYKgGoi/EhcHb0gLWDyX3W+mCvQIi2rbYWHn9cnWprg70a3XRP7dPcdNNNHq83mUzExMTQu3dvzjjjDNLSpCdDtJLek+G3l6DvicFeSei55Ft1UKUdPIqmNRwoEKrlooEq7ctdoc47j26dwS1uGalW+j/D8b3T0SPVkv4oTXQSkNt8RsrRH+XjoAlwrtPLsAlXJpOJf5+YA69Brc3Ci0sPcv1kgz+DmCQooe1npL7/P1j5Nly1ENJ6BHs1QogQYjiQWr16NatWraK+vp5+/fphs9nYsmULFouF/v3788ILL3DzzTezePFiBg4cGIg1C+Gu41C4eZNM7PMkMQvICvYqwkdElBqhX2dvng/Z0j4tI+XnTMBeeyDVaZR/n9cbtx6pECzta8nEPsfr6Sx/82tGqunSPk2WRX19h0nk2Xlbmdg/iyGdDWRh28teUhs+U1m33UslkBJCuDFc2nfGGWcwefJk9u3bx8qVK1m1ahW5ublMmTKFCy64gNzcXI499lhuvPHGQKxXCM8kiBL+4loiGrKlfSnq3N+lfa4ZqdbgmpFq7dK+uiqorWz6vn7LSNF0RqqqBA5tU5ezWxBIOYZN6AuktMEitdFp1Flt3PjRGqpqDZSwtYe9pKpLoXiPuly6L7hrEUKEHMOB1GOPPcYDDzxAUpLzk9qkpCTuu+8+Hn30UeLi4rjnnntYuXKlXxcqhBCtwjWQak+lfWUFULQbMEHOSP89b1MSXUv7WmkgSnSi2sQbmv/++SOQ0pOROrBenSd1Nr4xrivH+HOdgVS5CqQyszuRkRjN1vwyHvtuk/7Xaw97SRVsdl4u2R+8dQghQpLhQKq4uJj8/PxG1xcUFFBSov6YpqSkUBNGEzeEEMLBEUiZQjeQCsSwCS0bldGv9UoaY1NVKSW0XmmfyaT/++co7fNHRqqJTXn90R8Fzg15dWek1B5SkYkdeOTsIQC88csOft1WqO/xerJt4a5go/NyqQRSQgh3PpX2XXrppcydO5e9e/eSm5vL3Llzueyyy5g6dSoAy5Yto2/fvv5eqxBCBJ52cBidGLqbAgZi/LmjP6qVyvpABTUZagw3aT1b73X1Tu5zZKT80COl7RHliT/6o8Bl2ITejJR9M964Dkzqn8UFY7pis8EtH6+ltErH1Cy9EwnDWcGfzsslUtonhHBn+Cjh5Zdf5rjjjmPatGl069aNrl27Mm3aNI477jheeuklAPr3789rr73m98UKIUTAaRmpUM1GQWB6pBz9Ua00aEJz/jsw82vo0Lv1XlPv5D5t+p2W6fFFl7HqfPU7sH2B5/vk2TNS2S3MSEUaGzahZaS0/rT/O2UAXdPiyC2q5IEv/2j+8Y7MXhvOSOW7BFKSkRIicGJjYf16dYqNDfZqdDMcSCUkJPDqq69SWFjomOBXWFjIK6+8Qny8+iM+fPhwhg8f7u+1CiFE4GmBVKgOmgCXQMBPmQCrFXJXqcutmZECtcdZ96Nb9zX1BqL+KO3rdzIMnwE2K3xyGRTnut9eWwX59vIxf2Wkapoffw64ZKRUX1Z8dARPnDcMkwk+WrGXH/440PTj20Vpn0vPWNkBqK8L3lqEaMvMZhg0SJ1CtRrEA59XmpCQwNChQxk2bBgJCS0oexBCiFDiCKTaUUaqcIs6GI6Mg8x2sG1Fa5b2mUxwyuOQPURlgD6+GOpceojz/wBbvQpmknK8P48eBsefU3FInbtMTDyiexpXHqvKLO/47+8UllV7f3ygxvCHiuoyKN5t/4dJBcPljXvEhRDtl+FAqry8nLvvvptx48bRu3dvevbs6XYSQoiwFhalfS5jp60GxlV7o/VHdRwOFsPbC4YfvaV9/pjaBxAZC+fNUT+3vcvh+7uct7n2R7V0GwejPVIV7hkpzU1T+tIvK5GDZTXcOXcdNpvN8+Pb+j5SB+3ZqPhMZ5Ark/uECIyaGrjvPnUKo4F1hv/HvPzyy1m4cCEXXXQRHTt2xCT79wgh2hItSNGyFqHIteywqhji0lr2fMHqjwoW3aV9fgqkQG3keuYr8P75sOwV6DwGhp7rv/4oMJ6Rchk24So6wsKT5w9j6vO/8N2GA8xdnctZIzs3fnxb30dK64/K6Kf2HCvJte8l1U5+T4RoTbW1cP/96vKtt0JUVHDXo5PhQOqbb77hq6++4uijW7mmXQghWsOgs2Dfajji8mCvxLuIKFWGV1vhn0AqGBP7gkl3aZ/WI+Wn8vV+J8JfboGfH4cvroesQf6b2AfGNuS11jszch42Qx6Uk8ysyX157LtN3Pu/DYztmU6nlAYN4G19HyltYl/mADVoIhcozQvqkoQQocVwaV9qaippaS38T1sIIUJVajc4723ockSwV9I0f/VJ1VTAgQ3qcuf2EkilqPPWKu1zNfFO6DlBBcEfXeT83vsjkDKyIW/FIcBeshfr+f/0q47tyciuKZRW13Hrx2uxWhuU+LkOm/BW/hfOClwyUolaaZ+MQBdCOBkOpB544AHuueceKip0TgUSQgjhf45goKhlz7N/rRp2kJANSZ1auqrwYLS0L7IF488bMlvg7NchqTMUboW6KohKhNQeLX9uIxvyav1Rsale++IiLGaeOG84sZEWlmwrZPavO93voGWkrHXOUfFtiSOQGgBJHdVlGYEuhHBhOJB64okn+O6778jKymLIkCGMHDnS7SSEEKIV+GtimqM/anTLhx2Ei9ac2udJfAc4bzaYI9W/Ow71z7hfLXNmrXWfDOhJRaE6bzBooqEeHeK585QBADz+3SYOlbs8b1QCmOzrbmvlfdVlUGSf2Jc5QDJSQgiPDPdITZ06NQDLEEIIYYi/Svsc/VHtqIE+mKV9ms6j1Vj0L2+C/qf45zkjXdZZW6566bzxMmjCkxlju/Lh8t2szy3hpYXbuPNkFVhhMqnyvqoi+8CJjsbWW7gNDu9UpXNJnUIrkD+4WZ3HZ6gexMRs9W/JSAkhXBgOpO69995ArEMIIYQRfstIrVTn7aU/CtyDUJvN+wF8IAMpgFEzYch5zpK8loqIUlkua63qfWtq8qRW2udh0ERDJpOJm6f045K3ljN7yU4uP6YHmUkx6sYYeyBlNCNVVw2vHecMZqOTVECV0U+V0mX2V8NPtKC3tTnK+vqrcxl/LoTwoB1sGCKEEG2QP3qkSg9A8R7ABDkj/LCoMKF976x1KliK9lK6F+hACvwXRDmeL14FNs31SZXrK+3TTOiXwciuKazaXcTz87dy/xmD1Q3R2gh0gwF92QFnEGWyqIzW3uXqpInPgBvWBvb7703DQCrRnm2rKYXqUud+c0II/4iJgWXLnJfDhK6i7LS0NA4eVJ9eaVP7vJ2EEEK0An+U9mn9UZkD2teBYWQcWOxlb97K+2w2/48/bw2OvaTKmr6fgYwUqKzULcf3A+C9ZbvZe9g+XCLGx015tdLCpE5wVx5c/Suc8waM/wcMOF0N4CgvgB2LjD2vv2h7SGXaA6noBOeUQslKCeF/FgsccYQ6WSzBXo1uujJSTz31FImJ6j/Zp59+OpDrEUIIoYc/SvvaY38UqFK+mBQoz7cHol0a36e2Esd48GBkRHylrbW5KXoGeqQ043p3YFyvdJZsK+S5eVv599lDXd6HBkv7HK+frkoSswaqk+arm2H5a7DlB+h3krHn9oeGGSlQWanqErUpb0bf1l+TECLk6AqkLr74Yo+XhRBCBIk/SvtcJ/a1N7EpKpDy9v1zLY3z5/jzQHNkpJop7dM5ta+hm4/vy5IXf+XjlXv52/hedHfdS8oIR0Ysw/PtfY53BlJN9bEFQk05FO1SlzMGOK9P6ggHN0lGSohAqKmBZ55Rl2+4AaKaGJYTQnyat2q1Wtm8eTOLFy9m0aJFbid/y83NZcaMGaSnpxMXF8fw4cNZuXKl43abzcZ9991HTk4OsbGxTJgwgQ0bNvh9HUIIEVJampGy1kPuanW5U3sMpOyDGLyVRta67CHlj9HkrSXSYCAVbyyQGtUtjYn9Mqi32njmpy0upX1GM1IF9tf3Ekh1/wtYoqF4t3OCXmvRXi+ug/v3RxuBXioj0IXwu9pauO02daqtDfZqdDM8bGLp0qVMnz6dXbt2YWuwk7nJZKK+vt5vizt8+DBHH300EydO5JtvviEzM5Nt27aRkpLiuM+jjz7Kk08+yVtvvUXfvn3517/+xZQpU9i0aZOjHFEIIdqclvZIHdysGucj41WPVHujff+89Ui1xqCJQNCbkfKhtE9z05R+zN9UwGdrcrn7qBjSwIceKS2Q8vL6UXHQ/RjY9hNs+V5N82stjv6oBr8X2qa8kpESQtgZ/pjtb3/7G6NHj2b9+vUcOnSIw4cPO06HDh3y6+IeeeQRunTpwptvvsmYMWPo3r07xx13HL169QJUNurpp5/mrrvu4qyzzmLw4MHMnj2biooK3nvvPb+uRQghQkpyZ3VeuA1yVxl/vNYflTMCzOHT2Os3zZVGhm0gZS9DbKpHymZzyUgZD6SGdE7mxEHZ2GywYHe1utJoaV+5jtfvM0Wdb/nB8BpbxNEf1SB40yb3yV5SQgg7w4HUli1beOihhxgwYAApKSkkJye7nfzp888/Z/To0Zx77rlkZmYyYsQIXn31VcftO3bsIC8vj+OPP95xXXR0NOPHj2fJkiVen7e6upqSkhK3kxBChJXUbjD0fMAGX98KVquxxzv6o9rZoAlNc6V94TixD/RN7asqVntNgeEeKc2NU/piMsGy/fYqFF9L+5rKiPWx/9++a4kaOd5aPA2aAJe9pKS0TwihGA6kxo4dy9atWwOxlka2b9/Oiy++SJ8+ffjuu+/429/+xvXXX8/bb78NQF5eHgBZWVluj8vKynLc5snDDz/sFvx16eJhYpMQQoS6Kf9UB/q5K2Dt+8Yeu9fea9oe+6Og7Zb26emR0rJRkfEQGevTy/TLTuT0YTmU2uwZMH8PmwBI7wWpPVTQ15pj0L0FUpKREkI0YDiQuu6667j55pt56623WLlyJb///rvbyZ+sVisjR47koYceYsSIEVx11VVcccUVvPjii273MzWY5mOz2Rpd5+qOO+6guLjYcdqzZ49f1y2EEK0iMVvtuwPw4736+1RqyiHfPpSnPU7sgzZc2qcFUk2U9vk4aKKhG47rQ7lJBVIVJYXGHlyucx8rR3nf9wZX56OaCjhsn9jXqEfKnpEqOwD1da2zHiFESDM8bOLss88G4NJLL3VcZzKZHMGLP4dNdOzYkYEDB7pdN2DAAD799FMAsrOzAZWZ6tixo+M++fn5jbJUrqKjo4mOjvbbOoUQImjG/g1Wz1HDIxb8G058uPnH7FsDNquaQqYdHLY3ukv7wjWQaqK0rwWDJlz1zEhgVL/usB0OHypkyr/nebxfn6wE/ja+F2N7pKkPOW02A4HU8bDsFdjyY+uMQT+4GbCpkseGa4vPAJMFbPVqdH57/d0RQjgYDqR27NgRiHV4dPTRR7Np0ya36zZv3ky3bt0A6NGjB9nZ2fzwww+MGDECgJqaGhYuXMgjjzzSausUQoigiYiCkx6BOWfCby/DiIvcNzb1pL33R4GB0r4w7ZFqathEhc4gRodzxg2E7ZBIOblFlR7vk1tUyYJNBRzRPZW/T+rDsd1iMNXZ79tcMNf9GIiIgZK9kL+x+fd2SznK+jxMsjRbVBa4JFdN7pNASgj/iYmB+fOdl8OE4UBKC2Jaw4033si4ceN46KGHOO+881i2bBmvvPIKr7zyCqAyYbNmzeKhhx6iT58+9OnTh4ceeoi4uDimT5/eausUQoig6jUJ+p8Kf34J39wGF3/R9Cf32sS+9tofBTpK++yBSDhtxgv6xp/7KSMF0DFH9RgnmSr44m+jsEW4HwDV1luZuzqXj5bvZfnOw1z8xjKmdKzkVcAWEYupuYxfZKzaU2rrD+rUaoGUl3HriR1VIFW6D2jHH0QI4W8WC0yYEOxVGKYrkPr888856aSTiIyM5PPPP2/yvqeffrpfFgZwxBFHMHfuXO644w7++c9/0qNHD55++mkuvPBCx31uu+02Kisrueaaazh8+DBjx47l+++/lz2khBDtywkPwdYfYefPsGEuDD7L8/2s9ZBrHzTRXvujoO2W9mmBX1NT9PzUIwWo76MlGuqrGZJcDanZje4yqlsa103qwyuLtvPub7soyMuFaMivT2DZ7/s5eUhHLOYmAv8+U1QQteUHOPqGlq+5Kd72kNIkdYRcZC8pIQSgM5CaOnUqeXl5ZGZmMnXqVK/383ePFMCpp57Kqaee2uRr3nfffdx3331+fV0hhAgrqd3gmBthwcPw/f9B3xPcgwCrFTZ+rm4vyQVzJHQcHrTlBp2jtK9IfW/MDWYvhWtpX6Y9Y5O7QgVTMUmN7+PISPkhkDKZICELinerIQypnqtWspJiuPvUgVw9oRcLv8iFTZBXn8h176/mqR83c+2E3pw+PIdIi4cZWL0nq/Pdv3r/mvyl2YyUvZyvVEagC+FXtbVgrzjjyishMjK469FJ19Q+q9VKZmam47K3k7+DKCGEEAYcfQOkdFWB0s9PqOtsNvjzK3j5WPj4YnWgGJMMp/8HosMsSPAnrbQPm+fR3eE6tS9zAHToB/U1sOkbz/fRMlJ+KO0DVN8Q6BoL3iEhmrP7qWFPSenZJMdGsr2gnJs/XsukJxbw3m+7qa5rcCyR3gvSeoG1DrYv8M+aPampgMM71WVPPVKgMlIgGSkh/K2mBv7+d3WqqQn2anQzPP5cCCFEiIqMhRPsU/uW/AdWzYFXJ8IH0+HAOohKhPG3w6x1MPyC4K412CKiXcrgihrfHq6BlMkEg85UlzfM9XwfPw6bACDRPiW39IC++9tfv0fX7iz+x0T+cWJ/0uOj2HOokjvnrmP8owt485cdVNa4BFTa5rxbf/DPmj0p3ALYIDbN+/fGkZGSQEoI4cOwCYDy8nIWLlzI7t27qWkQNV5//fV+WZgQQggf9D9FDZ/YNg8+/7u6LjIOxl4F466HuLTgri+UxKSo6XaVhyG1u/ttjh6pMMzaDZoKC/8N235SpYuO7JtduZ8zUgn2jFRZnr77O0afp5MYE8nVE3oxc1x33l+2m5cXbSOvpIr7v/iD137ewZzLxtAzIwH6TIbfXgzsGHTX/ihvz58km/IKIZwMB1KrV6/m5JNPpqKigvLyctLS0jh48CBxcXFkZmZKICWEEMFkMsFJj8LL49V+N0dcDkfPgoSMYK8s9MSmqF4XT5P7wjUjBSoQyBgABRth09cwvMEU2wpnIOMXRjNSjkDK+Z6MjbJw6TE9uPDIrnyyci/Pz9tKblEl57+ylPcuH0ufbsdARKz6eR3YANmD/bN2V831R4Ga2gdS2ieEAHwo7bvxxhs57bTTOHToELGxsSxdupRdu3YxatQoHn/88UCsUQghhBEd+sD1q+GmjXDCgxJEeaMNnPBU2qftwxQVZuPPNd7K+2oqnF+bP4ZNgA8ZqQL76zfOiEVHWLhwbDc+v+4Y+mcnUlBazfmvLOWPghrocay605bv/bBoD5raQ0qjBVI1pVBdGph1CCHChuFAas2aNdx8881YLBYsFgvV1dV06dKFRx99lDvvvDMQaxRCCGFUYpaU8TVHG4HuaVPecC7tA2cgtW2e+9enZaPMkRDtp+l3jmETxnqkXDNSDXVIiOb9K45kcKckDpXXcMGrS8nNOEbduPXHFiy2CXoyUtEJzu+bZKWEaPcMB1KRkZGY7LXDWVlZ7N69G4Dk5GTHZSGEECLkNbUpbziX9gFk9IWswWrS3Z9fOa937CHVwX99Rgn20j4feqSakhofxbuXH8nwLikUV9Zy2S/2wHf3Uu8bKfuqthIO7VCXve0hpdGyUjICXYh2z3AgNWLECFasWAHAxIkTueeee3j33XeZNWsWQ4YM8fsChRBCiIBoqrQv3AMpUEMnwL28z9+DJsCZkSo/CPV1Td/XZvPYI+VNcmwk71w+ljHd0/izOo1tthzV++fvMegHN6Mm9qU2vy4ZgS6E/0VHw5dfqlN0dLBXo5vhQOqhhx6iY0f1R+SBBx4gPT2dq6++mvz8fF7RNtISQgghQp230r76OqirUpfDtbQPYKC9vG/7Aqg4pC77e9AEqKDMZAFsUJ7f9H2rS6G+2vk4HRKiI3jr0iM4unc68+uHAXBg5RctWLAHBZvUeUYTE/s0simvEP4XEQGnnKJOET4NFQ8Kw4HU6NGjmThxIgAZGRl8/fXXlJSUsGrVKoYNG+b3BQohhBAB4a20r7bceTmcM1IdekP2EFXet9EeeGjZIH9mpMxmSMhUl0ubKe/TArnIeEODPOKiInj94iM4lDMBANO2H7nuvVX8medhM2Vf5G9U5031R2kkIyWEsJMNeYUQQrRP3kr7tLI+cwRYolpzRf7XcHqfFsj4a2KfxjFwoplASmd/lCcxkRZuuPSv1JiiyDQV8fu61Zz49M9c+fYKft9bZPj53GgZqeb6o8ClR0oCKSH8prYW3npLnWprg70a3QwHUoWFhVx77bUMHDiQDh06kJaW5nYSQgghwoKjtK/I/XrX/qhAbPzamrRAasciFcQ4Ahk/ZqRA/wh0A/1RnkTHxBGV1hWAs3raMJng+z8OcPpzv3DxG8tYsfOQT8/LgXXqPKN/8/dNspf2lUhpnxB+U1MDl1yiTjU1wV6NboaLEGfMmMG2bdu47LLLyMrKckzwE0IIIcKKt9I+LZCKDOOyPk1aT+g4HPavUeV9Wq+U3zNSOjflbWIPKd2SOkHhVm44Ip5TzjiWF+Zv439r97FwcwELNxcwqX8mL1w4kphIi77nK8uHot2ACXKGN39/yUgJIewMB1KLFy9m8eLF0g8lhBAivDVX2hfO/VGuBp2pAqkNc51DNIKVkdKxh1Szkjur85K99B6eyJPnD+eGyX14aeE2Plm5l3l/5vPub7u57Jge+p5vr5pETEZ/iElu/v5aRqrsgBpMYgmfxnghhH8ZLu3r378/lZWVgViLEEII0Xq00r7qEvex3W0ukJqqznf+DIVb1WV/DpsAAxkpP5QWJnVS58W5jqu6pcfz8FlDeeCMwQC8uGAr5dXNjGLX7F2uzjuP1nf/+Aw1pdBmbX5KoRCiTTMcSL3wwgvcddddLFy4kMLCQkpKStxOQgghRFhwzT5UFTsv15Sp83Aefe4qtTvkjFQH/tqGvP4u7dPdI2Uv7WtJIJVsD6RKchvddPaoznRLj+NgWQ2zf92p7/kcgdQR+u5vtjiHa8jkPiHaNcOBVEpKCsXFxUyaNInMzExSU1NJTU0lJSWF1NTUQKxRCCGE8D9LBEQlqsuu5X1tLSMFzqETGn+X9hnOSLWgtC/JXtpX3DiQirSYmTW5DwAvL9xOSVUz07+s9ZC7Sl3WG0iBS5+UDJwQoj0zXNh74YUXEhUVxXvvvSfDJoQQQoS32FSoKXUfONEmA6mp8MPd9n+YnGWN/qJlpMrzwWpVe0t54o99rBwZqb0ebz59WCeen7+NrfllvP7zDm6c0tf7c+VvVPuGRSXq20NKk9QRcpGMlBDtnOFAav369axevZp+/Qz8wRFCCCFCUWwyFAOVh53XOUr72lAgldIVOo2G3BUQl6bK0/wpIRMwqc1/KwohwUvGqcKPPVJVxVBdBtHuJZgWs4mbpvTlmndX8friHcwc153UeC/7geXaB010GmHse5JoHzghGSkh/CM6Gj76yHk5TBgu7Rs9ejR79uwJxFqEEEKI1uVpcl9bzEgBDD5Lnft70ASAJdLZd+WtT8pm88+wiZgkiE5Slz30SQGcOCibgR2TKKuu4+VF270/l9H+KE2SvbRPMlJC+EdEBJx7rjpFhM8kTMOB1HXXXccNN9zAW2+9xcqVK/n999/dTkIIIUTYcGzK65KRqq1Q520tkBp+IfQ7GY6+PjDP7+gb8hJIVRWD1d6z1NJgzjG5z3N5n9ls4ubjVUnfW0t2kF9a5fl5tNHnRgMpyUgJIfChtO/8888H4NJLL3VcZzKZsNlsmEwm6uvr/bc6IYQQIpA8bcrbFkv7QH2tF7wfuOdPzIID67wHUlo2KioRImNa9lrJnaBgo9eMFMCk/pkM75LCmj1FvLhgG/eeNsj9DpVFUPCnutxJ5+hzjWSkhPCvujqYO1ddPvPMsMlKGV7ljh07ArEOIYQQovU1WdrXRsaft5bmRqD7oz9K42EvqYZMJhO3HN+PGa//xrtLd3PFX3qSkxLrvMM++7S+1O7ee7q8cWSkJJASwi+qq+G889TlsrK2G0h169YtEOsQQgghWp/HjFQb7ZEKtOZGoPtjDylNUtOT+zRH905nbI80fttxiOfmb+WhM4c4b/S1rA+c+0jVlEF1KUQnGn8OIUTYM9wjBTBnzhyOPvpocnJy2LVrFwBPP/00//vf//y6OCGEECKgPPVISSDlm+YyUv7YQ0qT3HxGClRW6ubj1ZThj5bvYXdhhfNGXwdNgJoU6Bh4IVkpIdorw4HUiy++yE033cTJJ59MUVGRoycqJSWFp59+2t/rE0IIIQLHY2mf1iMlpX2GNJuR0vaQSm/5azkyUs0PexjTI41j+2ZQZ7Xx2Peb2F5Qxvb8Uur3qEAqN36guq6JU1FFTeMnlk15hWj3DJf2/ec//+HVV19l6tSp/Pvf/3ZcP3r0aG655Ra/Lk4IIYQIKCnt8x/dPVL+yEh1VucluWqsusnU5N1vntKXRZsL+GLtPr5Yu49upjwWRh+m2hbJhHcOUcvCJh8faTHx8FlDOWdUZ+eVSR3h4CbJSAnRjvk0bGLEiBGNro+Ojqa8vNwvixJCCCFahcfSPnv5V2Rc668nnLlmpDwFN4HokaopU2PVtYDYi2FdUrjk6O78d1UuNpuNcai9pTaaehAbE0NsE4+12qCsuo5bP1lLTZ2V6WO7qhtkBLoQ7Z7hQKpHjx6sWbOm0dCJb775hoEDB/ptYUIIIUTAydQ+/9EyUvXV6vupBakaf/ZIRcWp5688rLJSzQRSAPeeNsg5Av2rH2A5DD9yCr+feEKTj7PZbNz/xR+8tWQnd85dR01dPTOP7iEj0IUQxgOpW2+9lWuvvZaqqipsNhvLli3j/fff5+GHH+a1114LxBqFEEKIwNAOwGsroK4aLFFtdx+pQIuMgZhklSEqPeA9kPJHjxRAUmcVSBXnQtag5u/vyjFoovn9o0wmE/eeNpCoCDOvLNrOfV/8QXWdlascPVISSAnRYlFR8OabzsthwnAgdckll1BXV8dtt91GRUUF06dPp1OnTjzzzDNMmzYtEGsUQgghAiM6GTABNtUnFZMMNvvG8hJIGZfY0R5I7YfM/u63+bNHCtTkvgPrmh2B3khNBRxYry7rnNhnMpm446T+REeY+c+8rTz8zZ90Ggmngq6BF0KIZkRGwsyZwV6FYT6NP7/iiivYtWsX+fn55OXlsWfPHi677DJyc5seQyqEEEKEFLNZBU+gytFqXHp9JZAyLsHeJ1XWYHKf1epS2ueHHinQtSmvR/vXgrVOlSImd27+/nbaKPWbp/QF4OXVVQDYJCMlRLvlUyCl6dChA5mZmeTl5XHdddfRu3dvf61LCCGEaB2uk/u0sr6IWDBbgrWi8KVtVFvaYHJfVZEz0xfnp0BK20uqxGAg5VrW18y0P0+uO64Pd508gDybKl20lR6gvq7W8PMIIVzU1cFXX6lTXV2wV6Ob7kCqqKiICy+8kIyMDHJycnj22WexWq3cc8899OzZk6VLl/LGG28Ecq1CCCGE/7lO7pPR5y3jLSOlZaOikyHCT/0PSfZsUrHB0r7cFepcR3+UN1cc25PrTz+KOpsZM1amPfk5H6/YQ2291efnFKJdq66GU09Vp+rqYK9GN909UnfeeSeLFi3i4osv5ttvv+XGG2/k22+/paqqim+++Ybx48cHcp1CCCFEYLhO7quxD0KQQMo33jJSFX4u64MWZKS0QEpff5Q3F43rRcWiTCKq8qg5vJdbP4nh6R+3cPWEXpwzqjMxkZLRFKKt052R+uqrr3jzzTd5/PHH+fzzz7HZbPTt25d58+ZJECWEECJ8uZb21UpGqkW8ZqT8uIeURuuRKtmn9q3SozhXBV4mM+Q03hPTqLh0lRW7dlQsHRKiyS2q5P8+W8/4x+bz+uIdVNbUt/g1hBChS3cgtW/fPsc+UT179iQmJobLL788YAsTQgghWoVW2uc6bEICKd94y0j5cw8pTZJ9Q9y6Kqgo1PcYrawva5B/fsbpvYD/b+++46Oq0j+Ofya9J0AgIRASepXeBJQiIHZ+gKuCCquLiqig66rIKrgrsrCKqGBBKQpiR9aCCkoXkI40pYVOCCWk99zfHzczSUghk0wyCfm+X6/7mjv3nrn3SfaanYdzznNgYO1LrH+2Ly/d3pq6gV6cjU/j39/t4/r/ruLExeSy30dEKqUSJ1LZ2dm4u7vb3ru6uuLrq/+jERGRKs46tE9zpMrOuihvUXOkHLWGFICbZ25iVtJ5UrZCE2Ub1mdTt535emYXXu6ujOwRyZp/9OU/Q64hLNCLcwlpfLH1hGPuJSKVTonnSBmGwahRo/D09AQgNTWVRx55pEAytWTJEsdGKCIiUp4Kq9rn4eesaKo2/5yhfemJkJYInjm/R9vQPgf2SIE5vC/pnDm8L6z9lds7aH6UjTWROr3TdsjDzYW7uzYgyzCY+PUeth6Ldcy9RKTSKXEiNXLkyHzv7733XocHIyIiUuE0tM9xPP3NJDQ90eyVsiZS5VFsAsx1oM7sLFnBiawMOL3D3K9X+op9+YS2NV/jT5q9bnl+vs4RNQHYeeISmVnZuLmWacUZEamESpxIzZ8/vzzjEBERcQ4N7XMsvxC4mAgJZ2xziMpljhTkWZS3BEP7zu4x51N5BUItB6176RUANRvDxcPmQr9NbrCdalrHD38vNxJSM/kjOoE29QIdc0+Rq5GHB8yalbtfReifR0REpHordGifEqlSK6zgRFJ59UjZUQLdOqyvXmdwceDXH+uQwjM78x12cbHQsYHZ27n16EXH3U/kauTuDmPHmluemgyVnRIpERGp3vKtI5VTYc1diVSpFVYC3TpHysfBiZStR8qORMpR86Os8hScuFznCDOR2nb8kmPvKSKVQomH9omIiFyVrHOkUmLVI+UIl/dIZWdBSk6PjKOH9gWa6zgRX4KhfSc3m6/1HTQ/yqqQghNWnayJlHqkRIqXlQXr1pn7110HrlVjQWslUiIiUr1Zh/ZlpecOQVMiVXqX90ilxIKRbe771HTsvWyL8p6B7Oyih+wlRMPFI4Cl/HqkLh0zf1ZrYg60bxCEq4uF03GpnL6UQliQt2PvLXK1SE2Fvn3N/cREqCJLLGlon4iIVG8efmDJ+ddP61wblT8vvct7pKzJqXcNcHXw3Af/umBxgewMSIoput2xDeZraJvcxNlRvGtAUIS5f+b3fKd8PNxoVTcAgG0qgy5y1VEiJSIi1ZvFktuLYK3+ph6p0ru8R6q85kcBuLrlLgJc3DwpayIV0dPxMUCRBScgz/A+JVIiVx0lUiIiItZeCs2RKrvLe6SSy6n0uZWtcl8x86RsiVSP8omhmIIT1kRq6zHNkxK52iiREhERsVbus9LQvtKz9kilXoKM1Dylz2uVz/2uVLkv+SLE7DX3y6tHqrjKfZFmIrX/TAJJaZnlc38RcQolUiIiInkKBADqkSoL7xrg6mnuJ54tv8V4rWyV+4pIpI5vNF+Dmzt+HSuruu3N1wuHIDU+/6lAb+oFeZOVbbDrxKXyub+IOIUSKRERkcsLEHj4OCWMq4LFAv45vVIJ0eU7Rwry9EgVMbSvvIf1gZmgBeQkdNG7C5zuaBvep3lSIlcTJVIiIiIa2udY1gIQidHlP0cqIMx8jT9d+Pljv5qv5TWsz6qYghOdlUiJFM/dHaZPNzd3B1f3LEdaR0pERERD+xzL1iN1tvznSBU3tC8tIXfeUsS15XN/q7rt4I/vii04seNYLNnZBi4ulvKNRaSq8fCAf/zD2VHYTT1SIiIieYf2WVzAzctpoVwV8vZIlfccKevQvoQzkHVZMYcTv5mLAQdF5CZc5aWYghMtQv3x8XAlIS2TAzEJ5RuHiFQYJVIiIiJ5h/Z5+JnzfKT08vVIlfMcKb864OJmJkyJ0fnPlff6UXlZC06cPwDpSflOubm60KFBEABbj2p4n0gBWVmwZYu5ZWU5O5oSUyIlIiKSd2ifhvWVnbVHKv4UpOQkDuXVI+XiCv4586QuL4FeEYUmrPxDzJ/byIboPQVOd4qoCcB2zZMSKSg1Fbp2NbfUVGdHU2JKpERERPIO7VMiVXbWRXlj9gMGYAGfmuV3v8IW5c1IgVPbzP2KSKSghAvzKpESuVpUqURq6tSpWCwWxo8fbztmGAaTJ08mLCwMb29v+vTpw969e50XpIiIVD35hvYpkSoz66K81qF2PjXNnqPyUtiivKe2QVa62UtUs1H53TuvYir3dWgQhMUCxy8mE5NQdf7FXUSKVmUSqS1btjBnzhzatm2b7/j06dOZMWMGs2bNYsuWLYSGhjJgwAASEjSZU0RESijv0D53JVJlZu2Rsiqv+VFWth6pPIlU3mF9FTXnrZgeqQAvd5qH+AMa3idytagSiVRiYiIjRozg/fffp0aN3P+zMwyDmTNnMnHiRIYMGUKbNm348MMPSU5OZvHixU6MWEREqhQN7XMsn2Cw5OmBKq/5UVbWxXDzLsprWz+qgob1QW7BiZj95tDCy9iG96nghMhVoUokUmPHjuWWW26hf//++Y5HRUURHR3NwIEDbcc8PT3p3bs3GzZsKPJ6aWlpxMfH59tERKQac/cGV09zX4lU2bm45A7vg/JbQ8rq8h6prAw4sdncr4iKfVYBYWYSaWTB2X0FTneO1DwpkatJpU+kPv30U7Zv387UqVMLnIuONsdeh4SE5DseEhJiO1eYqVOnEhgYaNvCw8MdG7SIiFQ91l4pDz+nhnHV8M+bSJV3j9Rlc6TO7IKMZHPIZu0W5XvvvCyWPMP7dhY43Tmnct/e03GkZlSdEs8iUrhKnUidOHGCcePGsWjRIry8il4c0XLZ2GfDMAocy2vChAnExcXZthMnTjgsZhERqaKs86TUI+UYfnnmSZX7HKmcoX1JMZCZljusr0EPs3esIhVTcKJ+DW9q+3uSkWXw+8m4Cg1LpFJzd4dJk8zN3d3Z0ZSYm7MDKM62bduIiYmhU6dOtmNZWVmsXbuWWbNm8eeffwJmz1TdunVtbWJiYgr0UuXl6emJp6dn+QUuIiJVj7VynxIpx8jXI1XOiZRPLXDzgsxUSDhTsetHXa6YghMWi4XOETX4YU80W49dpGvDciwJL1KVeHjA5MnOjsJulbpH6oYbbmD37t3s3LnTtnXu3JkRI0awc+dOGjVqRGhoKCtWrLB9Jj09nTVr1tCjhxP+eIqISNVlG9qnRMoh8vZIlXciZbGY85MALp2AYxvNfackUu3N17P7IDO9wGlrwQlV7hOp+ip1j5S/vz9t2rTJd8zX15datWrZjo8fP55XXnmFpk2b0rRpU1555RV8fHwYPny4M0IWEZGqKrgpHPgRakQ6O5KrQ0XOkQJzntTFI3BwOaTFmXPdQtte+XOOFtTA7N1MvQQx+3KH+uWwJlLbjsVecSqCSLWRnQ3795v7LVtW/JDcUqrUiVRJPPPMM6SkpPDoo48SGxtLt27dWL58Of7+/s4OTUREqpK+E6Hl7VCvs7MjuTrk65GqoEQKYM9X5mt4N3B1wtcca8GJqDXm8L7LEqnWYYF4urkQm5zB4XNJNKmj4iYipKSAtfMkMRF8q8bIgKqR7uWxevVqZs6caXtvsViYPHkyZ86cITU1lTVr1hToxRIREbkid28I71pl/iW00svbI1XexSagYAl0Zwzrsyqm4ISHmwvtwoMAePqLXaw9cA7DMCosNBFxHP2/hYiIiDheYDhgATfv3IqI5cnaI2UV2av871mUYgpOADx0XSM8XF3YeeIS98/bzODZv7Ji31klVCJVjBIpERERcTy/OjD4HRg2r2J6+awl0MGs4BfWofzvWRRrwYnoPebiwJfp3yqEdc/25cFeDfFyd2HXyThGf7SVm95Yx/e/nyErWwmVSFWgREpERETKR/t7oMXNFXOvvD1S9buAmxOXOanREDwDICsNzv1ZaJOQAC9euLUV65/tx5g+jfH1cOWP6ATGLt7OwNfXcOBsQgUHLSL2UiIlIiIiVV9gnkTKmfOjwOyBs1YMjP692KbBfp48O6gFvz7Xj/H9mxLg5cbhc0mM/Xg7qRlZFRCsiJSWEikRERGp+ryCzJLn4PxECqBWY/M19liJmgf5eDC+fzNWPt2HYD9PDsYk8trywnuzRKRyUCIlIiIiVZ/FAv0nQ6e/QoQTC01YBYWbr3En7PpYsJ8n04ZeA8AH66PYdOSCoyMTqXzc3eHpp83N3d3Z0ZRYlV9HSkRERASArqOdHUGuwAbmq52JFMANLUO4u0s4n245wdNf7OKHcdfh71V1vlyK2M3DA/77X2dHYTf1SImIiIg4mrWK4CX7EymAf97aivo1vDkZm8LL3+13YGAi4ihKpEREREQczTq0L/4UZGfb/XE/Tzdeu7MdFgt8tvUEP+876+AARSqR7Gw4etTcSvHfi7MokRIRERFxNP8wsLhAVjokxZTqEt0a1WL0dY0AeG7J71xITHNkhCKVR0oKNGxobikpzo6mxJRIiYiIiDiaq5uZTEGph/cBPDWgGc1C/DifmM7Er/dgGFqsV6SyUCIlIiIiUh5slfuOl/oSXu6uzPhLe9xcLPy4N5qlO085KDgpk4xUSE92dhTiZEqkRERERMpDYE4iVYYeKYA29QIZ378pAC/+by+nL1WdoU9XrcV3wuutISXW2ZGIEymREhERESkPth6pk2W+1CO9G9OhQRAJqZm88fPBMl/PIVLjoDoONYw/DVFrIeUinN3r7GjEiZRIiYiIiJQHawn0UqwldTk3Vxeev7klAP/bdYq45IwyX7NMts6H6Y3g4zshK9O5sVS0o+tz9+PPOC8OcTolUiIiIiLlwboobxmH9ll1jqhBi1B/UjOy+Wr7Zb1cx3+DN9rBgZ8ccq9ibXoHvhsP2ZlwaAWseKH871mZHF2Xu5+gRKo6UyIlIiIiUh4cOLQPwGKxcG/3CAAW/XYsfwW/bQsg9ij8+qZD7lWkdTPgx+fM/WY3ma+b3oadn5TvfSuTvD1SSqQcw80NHn3U3NzcnB1NiSmREhERESkP1qF9aXHmfCIHGNyhHr4erhw5l8TGwxdyT5zaar4e31g+BRAMA1ZOgV9eMt/3fg7u+QR6P2u+/3YcnNru+PtWNnGn4OKR3Pfxp50Xy9XE0xNmzzY3T09nR1NiSqREREREyoOHL3jXNPcdNLzPz9ONIR3NBG3hpmPmwZRYOH/A3Dey4NAvDrmXjWGYw/fWTjff958MfSeAxWImVM1ugqw0+OxeSCzd4sNVxrFf879Xj1S1pkRKREREpLw4eHgfYBvet3zfWaLjUuHUtvwNHDlPKjsblv0DNrxlvr9pOvR6Mve8iwsMmQPBzSD+FHx+P2SmO+7+lY11flR4d/NViZRjGAacO2duVagSpBIpERERkfJiXUvKAZX7rJqH+tM1siZZ2QafbjkOJ3MSqRoNzddDKxxTSS87G759Ara8D1jgtjeg28MF23kFwN2LwTPAHFponUN1NbLOj7pmmPmaEF2lvvhXWsnJUKeOuSVXnYWOlUiJiIiIlBfborzHHXrZe681e6U+2Xyc7JObzYNdHwLvGuZQv5Nbyn6TfV/DjoVgcYH/ew86jSq6bXBTGPoBYIGtc83iF1cb6/woiwu0HmIey0qH5AvFf06uWkqkRERERMpLOQztAxjUOpRgPw/OxqeSeTwnaWrQDZoMMPcP/Fj2m/z5g/l67Vhod9eV2ze7Efr909z//mmzJPvVxDo/qm578K0FPsHmexWcqLaUSImIiIiUFwcuypuXh5sLd3UJJ9ISjUd6HLh6Qsg1ZjIDZZ8nlZ0Nh1ea+9Yy5yVx3d+h1R2QnQEfD4OotWWLozKxzo+K7GW+BtQ1XxOinROPOJ0SKREREZHyYhva59hECuCerg3o6HIIgJTa14CbBzS5ASyucG6/ua5UaZ3ZaQ5Z8/CH8K4l/5zFAne8DRE9IS0eFg2FPV+VPo7KJMqaSF1nvvqHma8J6pGqrpRIiYiIiJSXoAbma2I0ZKY59NL1a/hwa81TAOzMbmwe9K4BDa419w8sL/3FrSXUG/UGV3f7PuvpB/cuMXumstLhywdg4+zSx1IZxJ2E2ChzflSDnIp9/qHma7wq91VXSqREREREyotPLXDzNvfjTzn88l3cDgPwVUxdktNzKvXZhveVYZ7U4ZxEqskNpfu8uxcMmw9dc6r8/fQ8/DTRHDJYFR3NMz/KK8DcD1CPVHWnREpERESkvFgsufOkHD28Lz0Zv7g/AdiQ2ohvd+V8oW82yHw9ug7SEu2/bsolOJFTCbBxKRMpABdXuGkaDPiX+X7jLFjyN4f3zFWIy+dHAfjnzJFSj1TZubnByJHm5ubm7GhKTImUiIiISHkKcvxaUgCc2YUlO5Nkj2BOU4uFm45hGIZZirxGQ3NY3ZHV9l83ag0YWeYiuzUiyhajxQI9x8H/zQEXN3O+1KKhkBpXtutWNOv6Udb5UZCnR0rFJsrM0xMWLDA3T09nR1NiSqREREREylNg+ZRAt64V5dagKx5uruw5Fc/WY7Fm8mLtlSrN8L5DP5uvZemNuly7u2DEF+DhZ/buLL676gzzK2x+FOT2SGloX7WlREpERESkPJVX5b5TWwHwiOzKrW3NL/X3zNnEs1/+TnRob7PNweX2JSyGAYdyyp436e/IaKFxPxj1vZlMHd8AOxc59vrlpbD5UZCbSCVfqJrDFSsTw4CkJHMzDGdHU2JKpERERETKk21o33HHXvekmUhRvwvPDWpBj8a1yMw2+GzrCXp/nk6qizcknjVLmZfUuT8h/iS4eUFkT8fGCxDWHvo8Z+6vmATJFx1/D0crbH4UgE9Nc/0ugATNkyqT5GTw8zO35GRnR1NiSqREREREylN5DO2LP21WAbS4QN321AnwYvHo7nw15lr6Nq9NmuHGyoxrAPjuy/nsORVHWmbWFbfMnJLp2Q2uJc3iUWzb9MxSDs3r9gjUbgkpF+GXlxz1Gyk/hc2PAnMIpbUEuuZJVUtVpyyGiIiISFVkrdoXd9IcZufigH/HtvZG1WltrtuUo1NETeb/tSu7T8ax7X874NxmGlxYx61vrS/RZT9y/5zrXWHKn/WY+8/i51dZLPB/HeoxbWhb3F3t+Jlc3eGW12DBzbDtQ+hwP9TvVPLPVyTb/CjX/POjrALC4NIxM7GVakc9UiIiIiLlKSDM7DnKSoekGMdcM6fQBPU7F3r6mvqBjBr5EAYW2rpEEWqJveIlvUijm8sfAKzObnfF9oYBS7afYsyi7aRlZpU8djCHDba9GzDg+ych287PVxRrb1RY+/zzo6xsPVIa2lcdqUdKREREpDy5uoN/mDn36NKJ3C/fZWGbH1V4IgWAXx0s9TrBqa2s+78MUtsOLD7Mwz/j+UUG2QH1WDpmlNnlVIwtRy8yZtF2ft5/loc+2sZ793XCy9215D/DwH/Dnz/AmV2wdR50HV3yz1aUouZHWfnnlEBXj1S1pB4pERERkfLmyLWksjLh9A5zv36X4tvmlEF3P7wCfy/3Yjef42sAcGnSH39vjyu279cihPmjuuDt7sqaA+f46/wtJKdnlvzn8KsD/f5p7q/8NySes/c3Uf6Kmh9lFWAtga45UtWREikRERGR8mabJ+WARCpmL2SmgGcg1GpafNtmN5qvR1ZBRmrxba3rR9lR9rxHk2A+erArfp5ubDxygZHzNpOQmlHiz9PlQQhtay7Qu+LFkn+uIlw6AbFHzflR4d0Kb2NbS0pD+6ojJVIiIiIi5c2Ra0nZ5kd1unLhitBrzOFnGcm5vSuFiT0GFw6aSUOj3naF0yWyJgsf7Iq/lxtbjsZy79zNxCWXMJlycYVbZpj7uxbDsY123btcHctZP6qo+VFgzn8DDe0rK1dXGDbM3FztGB7qZEqkRERERMpbkANLoJ/cZr7WK2Z+lJXFktsr9etMyEgpvN3hX8zX8K7gFWh3SB0a1OCT0d0J8nFn14lLDP9gE7FJ6SX7cHgX6Hi/uf/9382hi5XBleZHQf5iE1VoIdlKx8sLvvjC3Ly8nB1NiSmREhERESlvgQ6cI2XrkbrC/CirLg+Cu4+ZGHxyT+HJ1KGcRKrJDaUOq029QD59qDu1fD3YezqeJz7dUfIP3zAZvGuYwxY3v1fqGBzqSvOjIHdoX2YqpFy5MqJcXZRIiYiIiJQ3Rw3tS4k1h+BB8RX78gq9BkZ8Ce6+5lypy5OprAw4YhaasGd+VGFahAbwyUPdcXWxsO7geQ6cTSjZB31rQf/J5v7G2WWKwSFij115fhSAu7eZAIIKTlRDSqREREREypu12ERanFlYobRO5Qzrq9kIfGqW/HORPeHevMnU3ZCebJ47sRnSE8AnGEKvvH7UlTQL8ad/yzoALNp0rOQfbD3EfI0/BanxZY6jTKJyEsv6nYueH2VlLYGeoHlSpZaUZA5DtVjM/SpCiZSIiIhIefP0y+25KMs8Kdv6USUc1pdXRI88ydTq3GTKWq2vcb8rF68oofu6RwLmgr1JaSWc8+QVAH4h5r61181ZrD10DUtQeMM6TypelfuqGyVSIiIiIhXBEcP77J0fdbmIHnDvV+DhZ/a6fHIXHPjRPFfGYX159Whci4bBviSmZbJ056mSf9Bazv38IYfFYjfDyO2RKkkFwwCVQK+ulEiJiIiIVISgBuZrcQUnzh+Eb8fD1nkQd1kCYhi5PVL1OpU+johr8yRTayFmn3m8cb/SX/MyLi4WRnQzf96FG49hlLSiXXAT89WZPVIx+yDpHLh5lyxhtQ3tUyJV3bg5OwARERGRaqEki/J+92Ru2W0wC0U0vRGaDTKHvqVeAjcvCGlTtlgadDeTqUVDIT0R6rYDv9plu+ZlhnWqz39/+pM/ohPYfjyWThElmNNl65FyYiJlHdYXcS24eV65vbVHSkP7qh31SImIiIhUhCsN7Tvzu5lEWVxzekIsEL0b1r0Kc/vDe9eb7eq2BzePssfToDvc97VZle66v5f9epcJ8vHg9nZmb83CjSUsOhGck0hdcOLQPtuwvj4la69iE9WWEikRERGRihB0hbWkfnvXfG11B/ztZ/jHIfi/98xqdp6B5lpFYM5zcpTwrvDgcvOe5eC+ayMAWLY7mguJaVf+QC3r0L7DkJ1dLjEVKysjd/2okhSaABWbqMY0tE9ERESkItgW5S2kal9iDOz+wtzv/qj56hsM7e42t6wMOPEbnPsTrrmzYuJ1gLb1g2hbP5DfT8bx+daTjOnTuPgPBEWAiztkpkD8ydx5ZRXl1HZzqKN3DQhtW7LPBOT0SCWdM/93cnUvv/iuVq6ucPPNuftVhHqkRERERCqCNZFKiIbM9PzntsyFrHRzSF94IQUOXN0hshd0efDK6xpVMvd2N3ulFm8+Rlb2FYpOuLqZa2SBc+ZJWYf1RV5X8lLwPsFm8ocBiWfLLbSrmpcXfP+9uXl5OTuaElMiJSIiIlIRfIPNSnAYZm+LVUYqbJ1r7ncf45TQytNtbcMI8HLjxMUU1h44d+UPOHOe1BE750eBmXBpeF+1pERKREREpCJYLHkq9+VJpPZ8ZQ4LC6gHLW93TmzlyNvDlTs7m71xCzeVoOiEdZ5URfdIpSfByc3mvj2JFOQmUio4Ua0okRIRERGpKNZEylq5zzBg0zvmftfRV+38GuuaUqv+jOHExeTiG9t6pCo4kTq+0RxeGVA/d3hhSfmrBHqZJCWBr6+5JSU5O5oSUyIlIiIiUlEur9x3dD2c3W0O+es40nlxlbNGtf3o1SQYw4DFm48X39i2llQFD+2zDevrbfYe2iNAi/KWWXKyuVUhSqREREREKkpgThU6ayJl7Y1qfw/4lGDB2irMWnTi8y0nSMvMKrqhtUcq/qQ53M4RSlJK3VpooqRlz/Oy9kgpkapWVP5cREREpKLkHdp38Qj8ucx83+3qKzJxuf4t6xAa4EV0fCqfbzlBn+Z1imjpRZhXDVxTY4mO2ktG7Ta2MyEBXni42dEPcHAFLHsaPAPgrz+Ap1/h7ZIvmgsig9kjZS9rj1S85khVJ5U6kZo6dSpLlizhjz/+wNvbmx49ejBt2jSaN29ua2MYBi+99BJz5swhNjaWbt26MXv2bFq3bu3EyEVEREQKkXdo32/vAQY0GQC1mzk1rIrg5urCPV0b8PrPB3jhf3uBvUW2/dKjNp1dYnn5w2/4Lju30l8tXw/+dl0j7u3eAH+vYuaTpVyCnybCzkW5x1b+G26aVnj7qLWAAbVb5BaOsIet2IR6pKqTSj20b82aNYwdO5ZNmzaxYsUKMjMzGThwIEl5JqFNnz6dGTNmMGvWLLZs2UJoaCgDBgwgISHBiZGLiIiIFCLvorw7cr7kX4Ulz4syonsDmtTxw8vdpdjtGGYPTzO3aNsxD1cXLiSlM+3HP+g1bRUzfz7ApeT0gjc58BO83T0nibJAqzvM47+9B8d/KzywsgzrA/C3zpGKLt3npUqyGIZxhZXRKo9z585Rp04d1qxZw/XXX49hGISFhTF+/HieffZZANLS0ggJCWHatGk8/PDDJbpufHw8gYGBxMXFERBQtRa5ExERkSokKwNergNGzpyd4OYw9jf7ixtc7da/Dj9PhmvuhKEfAJCRlc03O08ze/Uhjpwz/1Hdz9ONe7tH8LfrGhLskgQ/ToDfPzWvUbMxDH4bGnSHpWPNxCq4GTy8DtwvW/T1zY5w8TDcvRha3GJ/vOlJ8EpOMvXciSq3aLLTJSWBX86wy8REs3qfE5U0N6jUQ/suFxcXB0DNmuZkzKioKKKjoxk4cKCtjaenJ71792bDhg1FJlJpaWmkpaXZ3sfHx5dj1CIiIiI5XN3NwgTxp8z33ccoiSqMrXJfbgl0d1cXhnaqz+AO9fhhzxlmrTzEH9EJvLvmMIc3LOG/7u8TlH2RbFz4KWAoS3xHkvGLC7AZn6whvOK6jKDzB/hm1lMsCRplu+41fgn8/eJhsLhAZK/SxevhC56BkBZnDu9TImUfFxfo3Tt3v4qoMomUYRg89dRT9OrVizZtzEmH0dFm92lISEi+tiEhIRw7VvSCb1OnTuWll14qv2BFREREihIYbiZS3jWg7V3OjqZysq0ldchcaytPsunqYuHWtmHc3KYuv/wRw6Kff2PWhRl4ZmdwKDuMf2Q8zI6YphCT/x/Ks1zu5z2Pmdx06VPeiWnDfsOsIhjsugbc4ZhnCzLiXWhyWWdViQXUhXNxZsGJ2s2v3F5yeXvD6tXOjsJuVSaReuyxx/j9999Zv359gXOWy/4lxzCMAsfymjBhAk899ZTtfXx8POHh4Y4LVkRERKQotZvDiU3Q+QHw8HF2NJVTjUizhyg90Zx3FFC3QBMXFwsDWoXQPz0Jy9IM4vybsqvXpwx39WR4oRdty8mte6kfvYKP6yxkZc/FZFlcCV89F5Lg28SmvPb6Wm5uU5exfZvQKszOXiX/UDj3h+ZJVSNVIpF6/PHH+eabb1i7di3169e3HQ8NNSukREdHU7du7n9gMTExBXqp8vL09MTT07P8AhYREREpSt+JUL8ztL3b2ZFUXm6eEBQBsVFw4WChiZSV5chqAALb3crQbk2Kv27z2TC7CzXj9jEs/X/Qcxys2QNAWvh1GFHw/e4zfL/7DP1b1uGxfk1pHx5UsphtBSdUAr26qNSDEA3D4LHHHmPJkiWsXLmShg0b5jvfsGFDQkNDWbFihe1Yeno6a9asoUePHhUdroiIiMiV+YdAx/vBzcPZkVRuwQXnSRVgGJCTSNGo75Wv6R8CN04191dPNdfxSjwLbl78/YH7+HH8ddzWLgyLBX7eH8Pg2b9y39zf+O3IhStf25rsxasEut2SkqB2bXNLctAizBWgUidSY8eOZdGiRSxevBh/f3+io6OJjo4mJSUFMIf0jR8/nldeeYWvv/6aPXv2MGrUKHx8fBg+vPBOXRERERGpAmrlmSdVlJj9kBgNbt5mdb6SaD/cTLoyU+HLB81jDbqDuxctQgN4654O/PxUb4Z2rI+ri4V1B89z15xN/OXdjaw9cI4iC1775yRSWkuqdM6fN7cqpFInUu+88w5xcXH06dOHunXr2rbPPvvM1uaZZ55h/PjxPProo3Tu3JlTp06xfPly/P39nRi5iIiIiJRJcM4wveJ6pI6sMl8jepjDAUvCYoHb3gB3X8g0/3H+8vWjGtf247W/tGP1030Y0a0BHq4ubD56kfvnbWbw7F9Zse9swYTKmkjFa2hfdVGpEynDMArdRo0aZWtjsViYPHkyZ86cITU1lTVr1tiq+omIiIhIFWXrkSomkTqck0g16mPftWtEwA0v5r5vVPhCvOE1fZjyf9ew9pm+/LVnJF7uLuw6Gcfoj7Zyx+xfiY5LzW1sHdqnYhPVRqVOpERERESkmrLOkbp0HDLTCp7PTINjv5r7jUswP+pyXUdDu+HQZhjUbV9s09BALybd1pp1z/Tjkd6N8fVw5feTcdw1ZyOnLuX0almLTSSehews++ORKkeJlIiIiIhUPn4h4OEPRjZcPFLw/InNkJEMvrWhTmv7r+/iCv/3Dgyba+6XQG1/T567qQU/jr+e8JreHLuQzF/e3cixC0ngVwcsrmBkQWKM/fFIlaNESkREREQqH4ul+HlSR/IM63Op2K+04TV9+Pzha2kY7MupSync9d4mDl9IMZM/KF0J9NR4s/dNqgwlUiIiIiJSORU3T8o2P6oUw/ocoG6gN5891J2mdfyIjk/lrvc2kepd2zxpzzypC4dh2T/gtRbwRns4ua1c4q3UXFygc2dzq+CkuCyqxIK8IiIiIlIN2daSuqwEevJFOL3D3C/N/CgHqRPgxacPdefeuZvZfyaeDZme9IMrV+4zDIhaA5vegQM/AXkqAO75Eup3KseoKyFvb9iyxdlR2E2JlB2ysrLIyMhwdhgiUsm5u7vj6lqy8fYiIlKMWjlD+y7vkYpaCxgQ3BwCwio8rLxq+Xnyyehu3Dd3MyfOBoIbnD0VRUhhjTNSYfcXZgIVszf3eLNBUKclrH/dTKwGTXVMcCmX4NgGaHJDycvDS4kpkSoBwzCIjo7m0qVLzg5FRKqIoKAgQkNDsVgszg5FRKTqsvVIHTR7cax/U63zo5zYG5VXkI8HH4/uxjdvhUESbNi5hwPef/Bgr4YE++UkMGf3wRcj4fwB8727L3QYAV0fNueCpcbDhllw8bA53K9W47IH9uNzsOsTCO8Gdy0yC2KIwyiRKgFrElWnTh18fHz0xUhEimQYBsnJycTEmBWb6tat6+SIRESqsJo5yUTqJUi+AL7B5vsjq81XJ82PKkyAlzvD+nSB7z+kVvZF3ll9mPm/RjG8awTjam8l8OdnzAWA/ULg2seg433gXSP3Al4BEHGt2dt24Ce49tGyBZSeBPv+Z+6f+A3m9IV7FkPddmW7bnlIToZWrcz9ffvAx8e58ZSQEqkryMrKsiVRtWrVcnY4IlIFeHt7AxATE0OdOnU0zE9EpLQ8fCAwHOJOmL1SvsFwMQpij4KLG0T2dHaE+XjVrA9AxxoptPUI5M+T52j22wQC3VYDkBp+PV53z89NCC/X9EYzkTrogETqwE9mefiAeuDuDRcOwbxBMPgdaD24bNd2NMOAY8dy96uIqlMWw0msc6J8qkhmLCKVg/VvhuZVioiU0eXzpKzD+up3BU9/58RUlJz5Wn5p5/jf3aFsC/0Pd7utJtuwMCNjGG0PP8w/lp0i6nxS4Z9vdqP5evRXSEsoWyx7vjJf294Ff/sFGt9gJlZfjIRVUyE7u2zXFyVSJaXhfCJiD/3NEBFxkLzzpCC37HklmR+Vj3/OcO60OCxz+uB36Q/wrc2fAz9iR6OHSM+28MW2k9zw2mqe+GQHf0ZflizVagI1GkJ2Ru7wxdJIjYODK8z9NkPBOwiGfw7dx5rH1vzHTKjSi0jopESUSImIiIhI5WVbS+oQZGflVOyjUs2PsvEKAA8/cz89ARr0gIfX0bLn7Sx8sBtfP9qD/i3rkG3AN7tOc+PMtTy8cCu7T8aZn7FYcnulDvxU+jj+WAZZaWZVw5DW5jFXNxj0Ctw+C1zcYf83MPdG+9a8knyUSIlTWCwWli5d6uwwREREpLILzhnad/4gnN5pFp7wDISwDs6Mqmih15ivPcfDyG8hILfoUIcGNfhgZBe+f6IXN18TisUCP+09y22z1jNq/ma2HYuFpgPNxgdXlH6+kHVYX5uhuZUOrTreB6O+A9/acHa3uRiwlIoSqavchg0bcHV1ZdCgQXZ/NjIykpkzZzo+qBIYNWoUFosFi8WCm5sbDRo0YMyYMcTGxuZrV1RCNn78ePr06ZPveoMHDy7foEVERMTxrD1SsVFwKGe4WsPrzB6Wymj4Z/DYNhjwUpExtg4L5O0RnVg+/nr+r0M9XCyw+s9zDHt3AyuSm5ql0ROj4cwu+++fdCF3HlmbIYW3adAd7lsKFhezZ+ror/bfR5RIXe3mzZvH448/zvr16zl+/Lizw7HLoEGDOHPmDEePHuWDDz7g22+/5dFHy1jBRkRERKqWgHrg5g3ZmbB9oXmsMs6PsvIKzO1Fu4KmIf68fld7Vv69D4Nah2IY8Nz//iQt4jqzwcHl9t9//zfm7yq0be78ssKEtoGOI839nyY4t/iExWKWP2/VqmAPWiWmRMpOhmGQnJ7plM2ws3s3KSmJzz//nDFjxnDrrbeyYMGCAm2++eYbOnfujJeXF8HBwQwZYv7LRZ8+fTh27BhPPvmkrWcIYPLkybRv3z7fNWbOnElkZKTt/ZYtWxgwYADBwcEEBgbSu3dvtm/fblfsAJ6enoSGhlK/fn0GDhzIXXfdxfLlpfiDIiIiIlWXi0tu5b74k+ZrZZwfVQaRwb68cU97WoT6cyEpnc8utTRPlGaeVN5hfVfSdyJ4Bpg9X7s+sf9ejuLjA3v3mlsVqpRdSftEK6+UjCxavViGyX9lsO9fN+LjUfL/yT777DOaN29O8+bNuffee3n88cd54YUXbEnR999/z5AhQ5g4cSILFy4kPT2d77//HoAlS5bQrl07HnroIUaPHm1XnAkJCYwcOZI333wTgNdee42bb76ZgwcP4u9fujKlR44c4ccff8Td3b1UnxcREZEqLLiJOZ8HIKgB1Gzk3HjKgaebKzP+0p47Zq/n7ZONuN8LOLUNks4Xve7U5RKi4eh6c7/1/125vV9tuO7v8PMk+OVf0OoO8PQr9c9Q3SiRuorNnTuXe++9FzCHySUmJvLLL7/Qv39/AKZMmcLdd9/NSy+9ZPtMu3bmatc1a9bE1dUVf39/QkND7bpvv3798r1/7733qFGjBmvWrOHWW28t8XW+++47/Pz8yMrKIjU1FYAZM2bYFYuIiIhcBWrlGaLWqG+VGv5lj1ZhATw5oBnTfzT4w4igheWYWXSi/T0lu8DepYBhrrFVI6Jkn+k+BrbNNxc5/vUN6DexlNFXP0qk7OTt7sq+f93otHuX1J9//snmzZtZsmQJAG5ubtx1113MmzfPlkjt3LnT7t6mkoiJieHFF19k5cqVnD17lqysLJKTk+2eo9W3b1/eeecdkpOT+eCDDzhw4ACPP/64w+MVERGRSi7vXJ9GfZwWRkV4+PrG/LI/hp9PtaeF2zGMAz9hKWkiZc+wPis3TxjwL/j8ftjwJnS8H4LC7Q+8LJKToUsXc3/LliozvE+JlJ0sFotdw+ucZe7cuWRmZlKvXj3bMcMwcHd3JzY2lho1auDt7W33dV1cXArM1crIyMj3ftSoUZw7d46ZM2cSERGBp6cn1157Lenp6Xbdy9fXlyZNzDHRb775Jn379uWll17i3//+t62Nv78/cXFxBT576dIlAgMD7bqfiIiIVFLWOVJYrvpEytXFwmt3tmPCG514jP+RfmAFnlkZ4HqF6Q2xx+DkZsACrQfbd9OWt0NETzj2K/zyEgz9oLThl45hwL59uftVhIpNXIUyMzP56KOPeO2119i5c6dt27VrFxEREXz88ccAtG3bll9++aXI63h4eJCVlZXvWO3atYmOjs6XTO3cuTNfm3Xr1vHEE09w880307p1azw9PTl//nyZf65Jkybx6quvcvr0aduxFi1asGXLlnztDMNg27ZtNG/evMz3FBERkUqgbjvocK9ZHMGnprOjKXeRwb7cevNtXDT88MxM5MTvq6/8ob1f53y4F/jbNy0DiwVunAJYYPcXcHJr8e0z0+y7/lVKidRV6LvvviM2NpYHH3yQNm3a5NuGDRvG3LlzATMx+eSTT5g0aRL79+9n9+7dTJ8+3XadyMhI1q5dy6lTp2yJUJ8+fTh37hzTp0/n8OHDzJ49mx9++CHf/Zs0acLChQvZv38/v/32GyNGjChV79fl+vTpQ+vWrXnllVdsx55++mnmzp3LrFmzOHDgALt27eKxxx7j8OHDjB07Nt/n4+Li8iWWO3furHIl4UVERKolF1e4Yzb0rj6Lxw7v3pD9vt0A+O2nT8jIukJ58tIM68srrAO0yxlC+OOEgj1Dmemw+0uYNwhergO/zSndfa4iSqSuQnPnzqV///6FDm0bOnQoO3fuZPv27fTp04cvvviCb775hvbt29OvXz9+++03W9t//etfHD16lMaNG1O7dm0AWrZsydtvv83s2bNp164dmzdv5umnn853j3nz5hEbG0uHDh247777eOKJJ6hTp45DfrannnqK999/nxMnTgDwl7/8hQULFvDhhx/SpUsXBg4cyOHDh1m3bh0REfknWa5evZoOHTrk21588UWHxCUiIiLiSBaLhda9hwFwTfJvzFp5qOjG5w9C9O/g4mYO0yutG14Edx9ziKA1MYs7BStfhtdbw1cPwvGN5vFVL0NqwekV1YnFsHdxoqtQfHw8gYGBxMXFERAQkO9camoqUVFRNGzYEC8vLydFKCJVjf52iIhImSVfxJjeGAvZXJf+Jrf37sYDPRtSy88zf7vV02D1K9BkANz7ZdnuuWY6rJoCAfWhXgf4YxkYOVM9/EKh81/NYYTn/oA+z0OfZ8t2P4CkJPDLKbuemAi+vmW/ZhkUlxvkpR4pEREREZHKyKcmlgbm8L7elh3MXnWYXtNW8fJ3+4iJN5eGwTBgT07yVNphfXld+xgE1DMXP97/rZlERfSCOxfAk3ugz3PQOyd52jgbUi6V/Z5VlBIpEREREZHKqulAAMaFH6Ft/UBSMrL4YH0Uvaav4oWlezh7cBucPwCuntDi5rLfz8MHbn8TgptBl7/Bo5vgr9+bC/xaKwe2Ggx1WkFaHGx6p+z3tFggIsLcqtAaYZW/jreIiIiISHXV7Eb45SVqn/+N/w25wMGoKH7f9wdZcWcI2RZLxo5TYIH9/t35fMVJ4GS+j3u7uzKkYz2a1PEv+T2b9IfH+hd93sXF7JX6YiRsehu6PwLeNUr384G5btTRo6X/vJMokRIRERERqazqtDLnK8WfxPL5/TQDmkGBb/HTY7qyKvpooZd4Z81hbm5Tl7F9m9AqrOg5P3ZpeTuEtIGze2Dj29BvomOuW4UokRIRERERqawsFuj9DPw60+z18a9rrhPlHwr+dTmY4sfa8/608qxHq0I+/md0Ij/vP8v3u8/w/e4z9G9Zh8f6NaV9eFDZ4rL2Sn1+nzm8r/uYarHGV15KpEREREREKrNOI82tEE1ztuL8ER3P7FWH+f730/y8P4af98dwXdNgHuvbhG6NapU+rha3Qsg1cHY3bJxllk8vjZQUuP56c3/tWnDA+qMVQcUmRERERESuYi1CA3jrng78/FRvhnWqj6uLhXUHz3PXnE385d2NrDt4jlKtiOTiYlbxA/jtPUi6ULoAs7Nh61Zzy77CwsOViBIpEREREZFqoFFtP169sx2rn+7DiG4N8HB1YfPRi9w3dzOD397Ain1n7U+oWtwCoW0hPRE2vlV82wuHzQV+rxJKpEREREREqpHwmj5M+b9rWPtMXx7o2RAvdxd2nbjE6I+2ctMb6/j+9zNkZZcwobJYoM8Ec/+3OZB0Pv/57Gw4sBwWDYO3OsKvbzj2h3EizZGSMps8eTJLly5l586dAIwaNYpLly6xdOnSCo3j6NGjNGzYkB07dtC+ffsKvbc9Lly4QMuWLdm8eTORkZHODkdKYdiwYfTo0YOnnnrK2aGIiIiUWmigFy/e1opH+zZm7vooPtpwlD+iExi7eDsRtXyoX6OEc5WMGvzbvSmNMg7y3bsTuNTznwxr5YfXnk9gywcQG5XT0AJJ58rt56lo6pG6So0aNQqLxYLFYsHd3Z1GjRrx9NNPk5SUVO73fuONN1iwYEGJ2h49ehSLxWJLwspbnz59bL8XDw8PGjduzIQJE0hLSytRTIMHD2bUqFH5rjd+/Hi7Ypg6dSq33XabLYmy3s+6BQYG0r17d7799tt8n1uwYAEWi4WWLVsWuObnn3+OxWLJl5hlZWUxdepUWrRogbe3NzVr1qR79+7Mnz/f1ibvc5J3GzRokF0/kz3OnDnD8OHDad68OS4uLkX+/mbOnEnz5s3x9vYmPDycJ598ktTU1GKvbRgGr776Ks2aNcPT05Pw8HBeeeUV2/mift7WrVvb2uzdu5ehQ4cSGRmJxWJh5syZBe7z4osvMmXKFOLj40v1OxAREalMgv08eXZQC359rh/jbmhKgJcbxy4k8+uhCyXbDl/k5aQ7AOgX/z/cvnsCZrSA5RPNJMorEK59DB7fBnfOv0I0VYd6pK5igwYNYv78+WRkZLBu3Tr+9re/kZSUxDvvFFyBOiMjA3d3d4fcNzAw0CHXKS+jR4/mX//6F+np6WzZsoW//vWvgJnglLeUlBTmzp3LsmXLCpz7+eefad26NZcuXeLtt99m6NChbN++nTZt2tja+Pr6EhMTw8aNG7n22mttx+fNm0eDBg3yXW/y5MnMmTOHWbNm0blzZ+Lj49m6dSuxsbH52lmfk7w8PT0d8eMWKi0tjdq1azNx4kRef/31Qtt8/PHHPPfcc8ybN48ePXpw4MABWwJb1GcAxo0bx/Lly3n11Ve55ppriIuL4/z53CEGb7zxBv/5z39s7zMzM2nXrh133nmn7VhycjKNGjXizjvv5Mknnyz0Pm3btiUyMpKPP/6YMWPG2PPji4iIVFpBPh48OaAZf7uuIb8eOk9aph2FH4x2xK75iRqXdnO322oA9meH85XrzdTpch/39GyBv5djvmtWFkqk7GUYkJHsnHu7+5jjUEvI09OT0NBQAIYPH86qVatYunQp77zzjm043hNPPMHLL7/M0aNHycrKIj4+nn/84x8sXbqU1NRUOnfuzOuvv067du1s1/3Pf/7D66+/TnJyMn/5y1+oXbt2vvtePrQvOzub//73v7z//vucOHGCkJAQHn74YSZOnEjDhg0B6NChAwC9e/dm9erVAMyfP5/p06cTFRVFZGQkTzzxBI8++qjtPps3b+bhhx9m//79tGnThokTS7YQnI+Pj+330qBBAxYvXszy5csrJJH64YcfcHNzy5cEWdWqVYvQ0FBCQ0OZMmUKb731FqtWrcqXSLm5uTF8+HDmzZtnu8bJkydZvXo1Tz75JJ988omt7bfffsujjz6aL0nI+7+jVd7npCJERkbyxhvm+Oh58+YV2mbjxo307NmT4cOH2z5zzz33sHnz5iKvu3//ft555x327NlD8+bNC20TGBiYL9FfunQpsbGxtmQaoEuXLnTp0gWA5557rsj73X777XzyySdKpERE5Krj7+XOoDZ17f9grddgyWiywjqyOuAOXtoVyPHYFFhxnFnrTvPXng35a89Ignw8Cn42OLjsgVcwJVL2ykiGV8Kcc+/nT4OHb6k/7u3tTUZGhu39oUOH+Pzzz/nqq69wdXUF4JZbbqFmzZosW7aMwMBA3nvvPW644QYOHDhAzZo1+fzzz5k0aRKzZ8/muuuuY+HChbz55ps0atSoyPtOmDCB999/n9dff51evXpx5swZ/vjjD8BMhrp27WrrjfHwMP/Dev/995k0aRKzZs2iQ4cO7Nixg9GjR+Pr68vIkSNJSkri1ltvpV+/fixatIioqCjGjRtn9+9k165d/PrrrxU2V2nt2rV07ty52DYZGRm8//77AIX2Ej744INcf/31vPHGG/j4+LBgwQIGDRpESEhIvnahoaGsXLmSRx99tECyWxYff/wxDz/8cLFt3nvvPUaMGFHqe/Tq1YtFixbZno8jR46wbNkyRo4sfA0NMBPHRo0a8d133zFo0CAMw6B///5Mnz6dmjULXyBw7ty59O/fn4iICLtj7Nq1K1OnTiUtLa1ce/BERESqjAbdYPzvuAI3AL0HZPPNrtPMXnWIw+eSeOOXg/y4J5ofx1+HJW/ngK8vnKt6c6eUSFUTmzdvZvHixdxwww22Y+np6SxcuND2JXvlypXs3r2bmJgY2xfDV199laVLl/Lll1/y0EMPMXPmTB544AH+9re/AfDyyy/z888/Fzl3JSEhgTfeeINZs2bZvgQ3btyYXr16Adjube2Nsfr3v//Na6+9xpAhQwBo2LAh+/bt47333mPkyJF8/PHHZGVlMW/ePHx8fGjdujUnT54sUe/A22+/zQcffEBGRgbp6em4uLgwe/Zsu36fpXX06FHCwgpPxHv06IGLiwspKSlkZ2cTGRnJX/7ylwLt2rdvT+PGjfnyyy+57777WLBgATNmzODIkSP52s2YMYNhw4YRGhpK69at6dGjB3fccQc33XRTvnbfffcdfn5++Y49++yzvPDCC4XGefvtt9OtW7dif87Lkzp73X333Zw7d45evXphGAaZmZmMGTOm2B6iI0eOcOzYMb744gs++ugjsrKyePLJJxk2bBgrV64s0P7MmTP88MMPLF68uFQx1qtXj7S0NKKjo0uViImIiFzt3FxdGNKxPne0r8ePe6J5a+VBRnSPyJ9EVWFKpOzl7mP2DDnr3nawfkHOzMwkIyODO+64g7feyq3vHxERka+nYtu2bSQmJlKrVv4VrlNSUjh8+DBgDp965JFH8p2/9tprWbVqVaEx7N+/n7S0tHwJ3JWcO3eOEydO8OCDDzJ69Gjb8czMTNuwrP3799OuXTt8fHJ/J4UNlyvMiBEjmDhxIvHx8UybNo2AgACGDh1a4vjKIiUlBS8vr0LPffbZZ7Ro0YIDBw4wfvx43n333SJ7Uh544AHmz59PgwYNSExM5Oabb2bWrFn52rRq1Yo9e/awbds21q9fz9q1a7ntttsYNWoUH3zwga1d3759C8ybK+q+AP7+/vj7+5f0Ry6V1atXM2XKFN5++226devGoUOHGDduHHXr1i0ywcvOziYtLY2PPvqIZs2aAWaPU6dOnfjzzz8LDPdbsGABQUFBDB48uFQxeuesup6c7KShviIiIlWEq4uFW9rW5eZrQkteVr0KUCJlL4ulTMPrKpL1C7K7uzthYWEFhon5+ub/ObKzs6lbt65tjlJeQUFBpYrB+mXTHtk5K1q///77BXo+rEMQS7X6do7AwECaNGkCwKJFi2jdujVz587lwQcftJ0HiIuLK/DZS5culan3ITg4uECxB6vw8HCaNm1K06ZN8fPzY+jQoezbt486deoUaDtixAieeeYZJk+ezP3334+bW+H/Kbu4uNjm/Dz55JMsWrSI++67L9/8NF9fX9vvoyQqYmjfCy+8wH333Wfr+bzmmmtISkrioYceYuLEibi4FCw4WrduXdzc3GxJFGCrcHj8+PF8iZRhGMybN4/77rvPNpzUXhcvXgRw6LBJERGRq5nFYsHNtZDeqJQUsI6Y+eEHKMX3R2dQInUVs/cLcseOHYmOjsbNza3IOUMtW7Zk06ZN3H///bZjmzZtKvKaTZs2xdvbm19++cX2pTgv65fYrKws27GQkBDq1avHkSNHivwy3qpVKxYuXEhKSootWSsujqK4u7vz/PPPM2HCBO655x58fHyoUaMGtWvXZsuWLfTu3dvWNiUlhb179xY63K6kOnTowKJFi67Yrnfv3rRp04YpU6bYCjPkVbNmTW6//XY+//xz3n333RLfv1WrVgBlKoNfEUP7kpOTCyRLrq6uGIZRZBLds2dPMjMzOXz4MI0bNwbgwIEDAAWS3zVr1nDo0CFb8lwae/bsoX79+gRXwcmxIiIilUp2NqxZk7tfRSiREpv+/ftz7bXXMnjwYKZNm0bz5s05ffo0y5YtY/DgwXTu3Jlx48YxcuRIOnfuTK9evfj444/Zu3dvkcUmvLy8ePbZZ3nmmWfw8PCgZ8+enDt3jr179/Lggw9Sp04dvL29+fHHH6lfvz5eXl4EBgYyefJknnjiCQICArjppptIS0uzle5+6qmnGD58OBMnTuTBBx/kn//8J0ePHuXVV18t1c89fPhwnn/+ed5++22efvppAJ5++mleeeUVQkJC6NGjB7GxsUybNg03NzfuvffefJ8/d+5cgTWnrNX3LnfjjTcyYcIEYmNjqVGjRrFx/f3vf+fOO+/kmWeeoV69egXOL1iwgLfffrvAUEyrYcOG0bNnT3r06EFoaChRUVFMmDCBZs2a0aJFC1s76zyfvNzc3IpMEBwxtM/6+0pMTLT9/jw8PGyJ3m233caMGTPo0KGDbWjfCy+8wO23327rlZw1axZff/01v/zyC2A+vx07duSBBx5g5syZZGdnM3bsWAYMGJCvlwrMIX/dunXLVxHRKj09nX379tn2T506xc6dO/Hz88v3DxPr1q1j4MCBZfo9iIiISBVmiBEXF2cARlxcXIFzKSkpxr59+4yUlBQnRFZ6I0eONO64444iz0+aNMlo165dgePx8fHG448/boSFhRnu7u5GeHi4MWLECOP48eO2NlOmTDGCg4MNPz8/Y+TIkcYzzzyT71qX3zsrK8t4+eWXjYiICMPd3d1o0KCB8corr9jOv//++0Z4eLjh4uJi9O7d23b8448/Ntq3b294eHgYNWrUMK6//npjyZIltvMbN2402rVrZ3h4eBjt27c3vvrqKwMwduzYUeTP3bt3b2PcuHEFjk+ZMsWoXbu2kZCQYIt59uzZRtu2bQ1fX1+jXr16xtChQ42DBw8WuB5QYJs0aVKRMXTv3t149913be+joqIKjTs7O9to3ry5MWbMGMMwDGP+/PlGYGBgkdd9/fXXjYiICNv7OXPmGH379jVq165teHh4GA0aNDBGjRplHD161NZm5MiRhcbfvHnzIu/jCIXdM2/sGRkZxuTJk43GjRsbXl5eRnh4uPHoo48asbGxtjaTJk3K9xnDMIxTp04ZQ4YMMfz8/IyQkBBj1KhRxoULF/K1uXTpkuHt7W3MmTOn0Nis/3tcvuV9NlNSUoyAgABj48aNRf6MVfVvh4iISIVLTDQMc5Ehc9/JissN8rIYRhkmm1wl4uPjCQwMJC4ujoCAgHznUlNTiYqKomHDhkUWCRCxx7Jly3j66afZs2dPoXN9pPKbPXs2//vf/1i+fHmRbfS3Q0REpISSksBaQTgx0SyH7kTF5QZ5aWifSAW7+eabOXjwIKdOnSI8PNzZ4UgpuLu756uAKSIiItWPEikRJyjN4sFSeTz00EPODkFEREScTImUiIiIiIg4l49966VWBkqkRERERETEeXx9zXlSVYxmupeQanKIiD30N0NEROTqpkTqCtzd3QFzgVARkZKy/s2w/g0RERGRq4uG9l2Bq6srQUFBxMTEAODj44PFYnFyVCJSWRmGQXJyMjExMQQFBdkWEBYREZEipKbC0KHm/ldfQRVZNkSJVAmEhoYC2JIpEZErCQoKsv3tEBERkWJkZcGyZbn7VYQSqRKwWCzUrVuXOnXqkJGR4exwRKSSc3d3V0+UiIjIVU6JlB1cXV315UhERERERFRsQkRERERExF5KpEREREREROykREpERERERMROmiNF7sKZ8fHxTo5ERERERKSaSUrK3Y+Pd3rlPmtOYM0RiqJECkhISAAgPDzcyZGIiIiIiFRjYWHOjsAmISGBwMDAIs9bjCulWtVAdnY2p0+fxt/f3+mL7cbHxxMeHs6JEycICAhwaixSNeiZEXvpmRF76ZkRe+mZEXtVpmfGMAwSEhIICwvDxaXomVDqkQJcXFyoX7++s8PIJyAgwOkPkVQtembEXnpmxF56ZsReembEXpXlmSmuJ8pKxSZERERERETspERKRERERETETkqkKhlPT08mTZqEp6ens0ORKkLPjNhLz4zYS8+M2EvPjNirKj4zKjYhIiIiIiJiJ/VIiYiIiIiI2EmJlIiIiIiIiJ2USImIiIiIiNhJiZSIiIiIiIidlEhVMm+//TYNGzbEy8uLTp06sW7dOmeHJJXA1KlT6dKlC/7+/tSpU4fBgwfz559/5mtjGAaTJ08mLCwMb29v+vTpw969e50UsVQ2U6dOxWKxMH78eNsxPTNyuVOnTnHvvfdSq1YtfHx8aN++Pdu2bbOd1zMjeWVmZvLPf/6Thg0b4u3tTaNGjfjXv/5Fdna2rY2emept7dq13HbbbYSFhWGxWFi6dGm+8yV5PtLS0nj88ccJDg7G19eX22+/nZMnT1bgT1E0JVKVyGeffcb48eOZOHEiO3bs4LrrruOmm27i+PHjzg5NnGzNmjWMHTuWTZs2sWLFCjIzMxk4cCBJSUm2NtOnT2fGjBnMmjWLLVu2EBoayoABA0hISHBi5FIZbNmyhTlz5tC2bdt8x/XMSF6xsbH07NkTd3d3fvjhB/bt28drr71GUFCQrY2eGclr2rRpvPvuu8yaNYv9+/czffp0/vvf//LWW2/Z2uiZqd6SkpJo164ds2bNKvR8SZ6P8ePH8/XXX/Ppp5+yfv16EhMTufXWW8nKyqqoH6NohlQaXbt2NR555JF8x1q0aGE899xzTopIKquYmBgDMNasWWMYhmFkZ2cboaGhxn/+8x9bm9TUVCMwMNB49913nRWmVAIJCQlG06ZNjRUrVhi9e/c2xo0bZxiGnhkp6NlnnzV69epV5Hk9M3K5W265xXjggQfyHRsyZIhx7733GoahZ0byA4yvv/7a9r4kz8elS5cMd3d349NPP7W1OXXqlOHi4mL8+OOPFRZ7UdQjVUmkp6ezbds2Bg4cmO/4wIED2bBhg5OiksoqLi4OgJo1awIQFRVFdHR0vufH09OT3r176/mp5saOHcstt9xC//798x3XMyOX++abb+jcuTN33nknderUoUOHDrz//vu283pm5HK9evXil19+4cCBAwDs2rWL9evXc/PNNwN6ZqR4JXk+tm3bRkZGRr42YWFhtGnTplI8Q27ODkBM58+fJysri5CQkHzHQ0JCiI6OdlJUUhkZhsFTTz1Fr169aNOmDYDtGSns+Tl27FiFxyiVw6effsr27dvZsmVLgXN6ZuRyR44c4Z133uGpp57i+eefZ/PmzTzxxBN4enpy//3365mRAp599lni4uJo0aIFrq6uZGVlMWXKFO655x5Af2ekeCV5PqKjo/Hw8KBGjRoF2lSG78dKpCoZi8WS771hGAWOSfX22GOP8fvvv7N+/foC5/T8iNWJEycYN24cy5cvx8vLq8h2embEKjs7m86dO/PKK68A0KFDB/bu3cs777zD/fffb2unZ0asPvvsMxYtWsTixYtp3bo1O3fuZPz48YSFhTFy5EhbOz0zUpzSPB+V5RnS0L5KIjg4GFdX1wLZdUxMTIFMXaqvxx9/nG+++YZVq1ZRv3592/HQ0FAAPT9is23bNmJiYujUqRNubm64ubmxZs0a3nzzTdzc3GzPhZ4Zsapbty6tWrXKd6xly5a2gkf6OyOX+8c//sFzzz3H3XffzTXXXMN9993Hk08+ydSpUwE9M1K8kjwfoaGhpKenExsbW2QbZ1IiVUl4eHjQqVMnVqxYke/4ihUr6NGjh5OiksrCMAwee+wxlixZwsqVK2nYsGG+8w0bNiQ0NDTf85Oens6aNWv0/FRTN9xwA7t372bnzp22rXPnzowYMYKdO3fSqFEjPTOST8+ePQssq3DgwAEiIiIA/Z2RgpKTk3Fxyf9V0tXV1Vb+XM+MFKckz0enTp1wd3fP1+bMmTPs2bOncjxDTitzIQV8+umnhru7uzF37lxj3759xvjx4w1fX1/j6NGjzg5NnGzMmDFGYGCgsXr1auPMmTO2LTk52dbmP//5jxEYGGgsWbLE2L17t3HPPfcYdevWNeLj450YuVQmeav2GYaeGclv8+bNhpubmzFlyhTj4MGDxscff2z4+PgYixYtsrXRMyN5jRw50qhXr57x3XffGVFRUcaSJUuM4OBg45lnnrG10TNTvSUkJBg7duwwduzYYQDGjBkzjB07dhjHjh0zDKNkz8cjjzxi1K9f3/j555+N7du3G/369TPatWtnZGZmOuvHslEiVcnMnj3biIiIMDw8PIyOHTvayltL9QYUus2fP9/WJjs725g0aZIRGhpqeHp6Gtdff72xe/du5wUtlc7liZSeGbnct99+a7Rp08bw9PQ0WrRoYcyZMyffeT0zkld8fLwxbtw4o0GDBoaXl5fRqFEjY+LEiUZaWpqtjZ6Z6m3VqlWFfn8ZOXKkYRglez5SUlKMxx57zKhZs6bh7e1t3Hrrrcbx48ed8NMUZDEMw3BOX5iIiIiIiEjVpDlSIiIiIiIidlIiJSIiIiIiYiclUiIiIiIiInZSIiUiIiIiImInJVIiIiIiIiJ2UiIlIiIiIiJiJyVSIiIiIiIidlIiJSIiIiIiYiclUiIiIqVksVhYunSps8MQEREnUCIlIiKVWkxMDA8//DANGjTA09OT0NBQbrzxRjZu3Ojs0EREpBpzc3YAIiIixRk6dCgZGRl8+OGHNGrUiLNnz/LLL79w8eJFZ4cmIiLVmHqkRESk0rp06RLr169n2rRp9O3bl4iICLp27cqECRO45ZZbAJgxYwbXXHMNvr6+hIeH8+ijj5KYmGi7xoIFCwgKCuK7776jefPm+Pj4MGzYMJKSkvjwww+JjIykRo0aPP7442RlZdk+FxkZyb///W+GDx+On58fYWFhvPXWW8XGe+rUKe666y5q1KhBrVq1uOOOOzh69Kjt/OrVq+natSu+vr4EBQXRs2dPjh075thfmoiIVAglUiIiUmn5+fnh5+fH0qVLSUtLK7SNi4sLb775Jnv27OHDDz9k5cqVPPPMM/naJCcn8+abb/Lpp5/y448/snr1aoYMGcKyZctYtmwZCxcuZM6cOXz55Zf5Pvff//6Xtm3bsn37diZMmMCTTz7JihUrCo0jOTmZvn374ufnx9q1a1m/fj1+fn4MGjSI9PR0MjMzGTx4ML179+b3339n48aNPPTQQ1gsFsf8skREpEJZDMMwnB2EiIhIUb766itGjx5NSkoKHTt2pHfv3tx99920bdu20PZffPEFY8aM4fz584DZI/XXv/6VQ4cO0bhxYwAeeeQRFi5cyNmzZ/Hz8wNg0KBBREZG8u677wJmj1TLli354YcfbNe+++67iY+PZ9myZYBZbOLrr79m8ODBzJs3j+nTp7N//35bcpSenk5QUBBLly6lc+fO1KpVi9WrV9O7d+/y+WWJiEiFUY+UiIhUakOHDuX06dN888033HjjjaxevZqOHTuyYMECAFatWsWAAQOoV68e/v7+3H///Vy4cIGkpCTbNXx8fGxJFEBISAiRkZG2JMp6LCYmJt+9r7322gLv9+/fX2ic27Zt49ChQ/j7+9t60mrWrElqaiqHDx+mZs2ajBo1ihtvvJHbbruNN954gzNnzpT11yMiIk6iREpERCo9Ly8vBgwYwIsvvsiGDRsYNWoUkyZN4tixY9x88820adOGr776im3btjF79mwAMjIybJ93d3fPdz2LxVLosezs7CvGUtRQvOzsbDp16sTOnTvzbQcOHGD48OEAzJ8/n40bN9KjRw8+++wzmjVrxqZNm+z6XYiISOWgREpERKqcVq1akZSUxNatW8nMzOS1116je/fuNGvWjNOnTzvsPpcnOZs2baJFixaFtu3YsSMHDx6kTp06NGnSJN8WGBhoa9ehQwcmTJjAhg0baNOmDYsXL3ZYvCIiUnGUSImISKV14cIF+vXrx6JFi/j999+Jioriiy++YPr06dxxxx00btyYzMxM3nrrLY4cOcLChQttc5wc4ddff2X69OkcOHCA2bNn88UXXzBu3LhC244YMYLg4GDuuOMO1q1bR1RUFGvWrGHcuHGcPHmSqKgoJkyYwMaNGzl27BjLly/nwIEDtGzZ0mHxiohIxdE6UiIiUmn5+fnRrVs3Xn/9dQ4fPkxGRgbh4eGMHj2a559/Hm9vb2bMmMG0adOYMGEC119/PVOnTuX+++93yP3//ve/s23bNl566SX8/f157bXXuPHGGwtt6+Pjw9q1a3n22WcZMmQICQkJ1KtXjxtuuIGAgABSUlL4448/+PDDD7lw4QJ169blscce4+GHH3ZIrCIiUrFUtU9ERKQQkZGRjB8/nvHjxzs7FBERqYQ0tE9ERERERMROSqRERERERETspKF9IiIiIiIidlKPlIiIiIiIiJ2USImIiIiIiNhJiZSIiIiIiIidlEiJiIiIiIjYSYmUiIiIiIiInZRIiYiIiIiI2EmJlIiIiIiIiJ2USImIiIiIiNjp/wEWXG1hdzzONQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_rul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
